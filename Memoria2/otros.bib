
@misc{alzheimers-association-media-line-2021,
	author = {{Alzheimer’s Association International}},
	month = {07},
	title = {{Global Dementia Cases Forecasted to Triple by 2050 | AAIC 2021}},
	url = {https://aaic.alz.org/releases_2021/global-prevalence.asp},
	year = {2021},
}
@book{alberca-serrano-2010,
	author = {Alberca Serrano, R. and López Pousa, S.},
	edition = {4},
	publisher = {Editorial Médica Panamericana},
	title = {{Enfermedad de Alzheimer y otras Demencias}},
	year = {2010},
}
@article{RCSP892,
	author = {Héctor Bayarre Vea},
	title = {Múltiples perspectivas para el análisis del Envejecimiento Demográfico. Una necesidad en el ámbito sanitario contemporáneo},
	journal = {Revista Cubana de Salud Pública},
	volume = {43},
	number = {2},
	year = {2017},
	keywords = {-},
	abstract = {-},
	issn = {1561-3127},		url = {http://www.revsaludpublica.sld.cu/index.php/spu/article/view/892}
}
@InProceedings{gardent2017creating,
	author = 	"Gardent, Claire
	and Shimorina, Anastasia
	and Narayan, Shashi
	and Perez-Beltrachini, Laura",
	title = 	"Creating Training Corpora for NLG Micro-Planners",
	booktitle = 	"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	year = 	"2017",
	publisher = 	"Association for Computational Linguistics",
	pages = 	"179--188",
	location = 	"Vancouver, Canada",
	doi = 	"10.18653/v1/P17-1017",
	url = 	"http://www.aclweb.org/anthology/P17-1017"
}

@article{johnston-etal-2002-match,
	title = "{MATCH}: An Architecture for Multimodal Dialogue Systems",
	author = "Johnston, Michael  and
	Bangalore, Srinivas  and
	Vasireddy, Gunaranjan  and
	Stent, Amanda  and
	Ehlen, Patrick  and
	Walker, Marilyn  and
	Whittaker, Steve  and
	Maloor, Preetam",
	booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2002",
	address = "Philadelphia, Pennsylvania, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/P02-1048",
	doi = "10.3115/1073083.1073146",
	pages = "376--383",
}
@inproceedings{pescado,
	author = {Wanner, Leo and Rospocher, Marco and Vrochidis, Stefanos and Bosch, Harald and Bouayad-Agha, Nadjet and Bügel, Ulrich and Casamayor, Gerard and Ertl, Thomas and Hilbring, Désirée and Karppinen, Ari and Kompatsiaris, Ioannis and Koskentalo, Tarja and Mille, Simon and Moßgraber, Jürgen and Moumtzidou, Anastasia and Myllynen, Maria and Pianta, Emanuele and Saggion, Horacio and Serafini, Luciano and Tonelli, Sara},
	year = {2012},
	month = {05},
	pages = {},
	title = {Personalized Environmental Service Configuration and Delivery Orchestration: The PESCaDO Demonstrator},
	isbn = {978-3-662-46640-7},
	doi = {10.1007/978-3-662-46641-4_41}
}

@article{gatt_2009_babytalk,
	author = {Gatt, Albert and Portet, François and Reiter, Ehud and Hunter, Jim and Mahamood, Saad and Wendy, Moncur and Sripada, Somayajulu},
	year = {2009},
	month = {01},
	pages = {153-186},
	title = {From data to text in the Neonatal Intensive Care Unit: Using NLG technology for decision support and information management},
	volume = {22},
	journal = {AI Commun.},
	doi = {10.3233/AIC-2009-0453}
}
@inproceedings{simplenlg_gatt,
	author = {Gatt, Albert and Reiter, Ehud},
	title = {SimpleNLG: A Realisation Engine for Practical Applications},
	year = {2009},
	publisher = {Association for Computational Linguistics},
	address = {USA},
	abstract = {This paper describes SimpleNLG, a realisation engine for English which aims to provide simple and robust interfaces to generate syntactic structures and linearise them. The library is also flexible in allowing the use of mixed (canned and non-canned) representations.},
	booktitle = {Proceedings of the 12th European Workshop on Natural Language Generation},
	pages = {90–93},
	numpages = {4},
	location = {Athens, Greece},
	series = {ENLG '09}
}


@inproceedings{aramossoto2017adapting,
	title = {Adapting {SimpleNLG} to Spanish},
	journal = {10th International Conference on Natural Language Generation},
	year = {2017},
	pages = {144-148},
	abstract = {We describe SimpleNLG-ES, an adaptation of the SimpleNLG realization library for the Spanish language. Our implementation is based on the bilingual English-French SimpleNLG-EnFr adaptation. The library has been tested using a battery of examples that ensure that the most common syntax, morphology and orthography rules for Spanish are met. The library is currently being used in three different projects for the development of data-to-text systems in the meteorological, statistical data information, and business intelligence application domains.},
	isbn = {978-1-945626-52-4},
	publisher = {Association for Computational Linguistics},
	author = {A. Ramos-Soto and J. Janeiro-Gallardo and Alberto Bugar\'{i}n}
}
@article{Androutsopoulos_2014,
	author = {Androutsopoulos, Ion and Lampouras, Gerasimos and Galanis, Dimitrios},
	year = {2014},
	month = {04},
	pages = {671-715},
	title = {Generating Natural Language Descriptions from OWL Ontologies: the NaturalOWL System},
	volume = {48},
	journal = {The Journal of Artificial Intelligence Research (JAIR)},
	doi = {10.1613/jair.4017}
}

 @report{alzheimers_disease_international_2019, 
 	title={World alzheimer report 2019}, url={https://www.alzint.org/u/WorldAlzheimerReport2019.pdf}, 
 	author={{Alzheimer’s Disease International}}, year={2019},
 	month={Sep},
 	location = {London},
} 

@article{romano2007enfermedad,
	title={Enfermedad de alzheimer},
	author={Romano, M and Nissen, Maria Daniela and Del Huerto, N and Parquet, C},
	journal={Revista de posgrado de la v{\'\i}a c{\'a}tedra de medicina},
	volume={75},
	year={2007}
}

@article{mattson2004pathways,
	title={Pathways towards and away from Alzheimer's disease},
	author={Mattson, Mark P},
	journal={Nature},
	volume={430},
	number={7000},
	year={2004},
	publisher={Nature Publishing Group}
}

@article{BabyTalk,
	author = {Gatt, Albert and Portet, François and Reiter, Ehud and Hunter, Jim and Mahamood, Saad and Wendy, Moncur and Sripada, Somayajulu},
	year = {2009},
	month = {01},
	pages = {153-186},
	title = {From data to text in the Neonatal Intensive Care Unit: Using NLG technology for decision support and information management},
	volume = {22},
	journal = {AI Commun.},
	doi = {10.3233/AIC-2009-0453}
}

@article{ren2021hybrid,
	title={A hybrid deep generative neural model for financial report generation},
	author={Ren, Yunpeng and Hu, Wenxin and Wang, Ziao and Zhang, Xiaofeng and Wang, Yiyuan and Wang, Xuan},
	journal={Knowledge-Based Systems},
	volume={227},
	pages={107093},
	year={2021},
	publisher={Elsevier}
}


@article{gatt2018survey,
	title={Survey of the state of the art in natural language generation: Core tasks, applications and evaluation},
	author={Gatt, Albert and Krahmer, Emiel},
	journal={Journal of Artificial Intelligence Research},
	volume={61},
	year={2018}
}

@misc{louis-2021,
	author = {Louis, Antoine},
	month = {12},
	title = {{A Brief History of Natural Language Processing — Part 1}},
	url = {https://medium.com/@antoine.louis/a-brief-history-of-natural-language-processing-part-1-ffbcb937ebce},
	year = {2021},
}

@inproceedings{LeCun1989HandwrittenDR,
	title={Handwritten Digit Recognition with a Back-Propagation Network},
	author={Yann LeCun and Bernhard E. Boser and John S. Denker and Donnie Henderson and Richard E. Howard and Wayne E. Hubbard and Lawrence D. Jackel},
	booktitle={Advances in neural information processing systems},
	pages={396-404},
	year={1990}
}
@article{sulem2018simple,
	title={Simple and effective text simplification using semantic and neural methods},
	author={Sulem, Elior and Abend, Omri and Rappoport, Ari},
	journal={arXiv preprint arXiv:1810.05104},
	year={2018}
}
@inproceedings{Lai2015RecurrentCN,
	title={Recurrent Convolutional Neural Networks for Text Classification},
	author={Siwei Lai and Liheng Xu and Kang Liu and Jun Zhao},
	booktitle={AAAI},
	year={2015}
}
@inproceedings{Kim2014ConvolutionalNN,
	title={Convolutional Neural Networks for Sentence Classification},
	author={Yoon Kim},
	booktitle={EMNLP},
	year={2014}
}

@article{jacovi2018understanding,
	title={Understanding convolutional neural networks for text classification},
	author={Jacovi, Alon and Shalom, Oren Sar and Goldberg, Yoav},
	journal={arXiv preprint arXiv:1809.08037},
	year={2018}
}
@article{MoyotlHernndez2016MtodoPA,
	title={M{\'e}todo para autocompletar consultas basado en cadenas de Markov y la ley de Zipf},
	author={Edgar Moyotl-Hern{\'a}ndez and M{\'o}nica Mac{\'i}as-P{\'e}rez},
	journal={Res. Comput. Sci.},
	year={2016},
	volume={115},
	pages={157-170}
}
@misc{make-school-no-date,
	author = {{Make School}},
	title = {Generating Sentences | Tweet Generator: Data Structures \& Probability with Python},
	url = "https://makeschool.org/mediabook/oa/tutorials/tweet-generator--data-structures---probability-with-python/generating-sentences/",
	year = {2017}
}
@inproceedings{li-etal-2017-adversarial,
	title = "Adversarial Learning for Neural Dialogue Generation",
	author = "Li, Jiwei  and
	Monroe, Will  and
	Shi, Tianlin  and
	Jean, S{\'e}bastien  and
	Ritter, Alan  and
	Jurafsky, Dan",
	booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
	month = sep,
	year = "2017",
	address = "Copenhagen, Denmark",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D17-1230",
	doi = "10.18653/v1/D17-1230",
	pages = "2157--2169",
	abstract = "We apply adversarial training to open-domain dialogue generation, training a system to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning problem where we jointly train two systems: a generative model to produce response sequences, and a discriminator{---}analagous to the human evaluator in the Turing test{---} to distinguish between the human-generated dialogues and the machine-generated ones. In this generative adversarial network approach, the outputs from the discriminator are used to encourage the system towards more human-like dialogue. Further, we investigate models for adversarial evaluation that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines",
}
@inproceedings{Yu2017SeqGANSG,
	title={Seqgan: Sequence generative adversarial nets with policy gradient},
	author={Yu, Lantao and Zhang, Weinan and Wang, Jun and Yu, Yong},
	booktitle={Proceedings of the AAAI conference on artificial intelligence},
	volume={31},
	number={1},
	year={2017}
}
@inproceedings{li-etal-2018-paraphrase,
	title = "Paraphrase Generation with Deep Reinforcement Learning",
	author = "Li, Zichao  and
	Jiang, Xin  and
	Shang, Lifeng  and
	Li, Hang",
	booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
	month = oct # "-" # nov,
	year = "2018",
	address = "Brussels, Belgium",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D18-1421",
	doi = "10.18653/v1/D18-1421",
	pages = "3865--3878",
	abstract = "Automatic generation of paraphrases from a given sentence is an important yet challenging task in natural language processing (NLP). In this paper, we present a deep reinforcement learning approach to paraphrase generation. Specifically, we propose a new framework for the task, which consists of a generator and an evaluator, both of which are learned from data. The generator, built as a sequence-to-sequence learning model, can produce paraphrases given a sentence. The evaluator, constructed as a deep matching model, can judge whether two sentences are paraphrases of each other. The generator is first trained by deep learning and then further fine-tuned by reinforcement learning in which the reward is given by the evaluator. For the learning of the evaluator, we propose two methods based on supervised learning and inverse reinforcement learning respectively, depending on the type of available training data. Experimental results on two datasets demonstrate the proposed models (the generators) can produce more accurate paraphrases and outperform the state-of-the-art methods in paraphrase generation in both automatic evaluation and human evaluation.",
}
@article{Shi2018TowardDT,
	title={Toward Diverse Text Generation with Inverse Reinforcement Learning},
	author={Zhan Shi and Xinchi Chen and Xipeng Qiu and Xuanjing Huang},
	booktitle={IJCAI},
	journal={arXiv preprint arXiv:1804.11258},
	year={2018},
	month = apr
}
@article{Karras2019ASG,
	title={A Style-Based Generator Architecture for Generative Adversarial Networks},
	author={Tero Karras and Samuli Laine and Timo Aila},
	journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	year={2019},
	pages={4396-4405}
}
@inproceedings{Ge2019AutomaticGE,
	title={Automatic Grammatical Error Correction for Sequence-to-sequence Text Generation: An Empirical Study},
	author={Tao Ge and Xingxing Zhang and Furu Wei and M. Zhou},
	booktitle={ACL},
	year={2019}
}

@article{He2017DeepLF,
	title={Deep Learning for Image-to-Text Generation: A Technical Overview},
	author={Xiaodong He and Li Deng},
	journal={IEEE Signal Processing Magazine},
	year={2017},
	volume={34},
	pages={109-116}
}

@inbook{cnn_lecun,
	author = {LeCun, Yann and Bengio, Yoshua},
	title = {Convolutional Networks for Images, Speech, and Time Series},
	year = {1998},
	isbn = {0262511029},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
	booktitle = {The Handbook of Brain Theory and Neural Networks},
	pages = {255–258},
	numpages = {4}
}

@inproceedings{bengio_2000,
	author = {Bengio, Y. and Ducharme, Réjean and Vincent, Pascal},
	year = {2000},
	month = {01},
	pages = {932-938},
	title = {A Neural Probabilistic Language Model},
	volume = {3},
	journal = {Journal of Machine Learning Research},
	doi = {10.1162/153244303322533223}
}

@inproceedings{collobertWeston,
	author = {Collobert, Ronan and Weston, Jason},
	year = {2008},
	month = {01},
	pages = {160-167},
	title = {A unified architecture for natural language processing: Deep neural networks with multitask learning},
	journal = {Proceedings of the 25th International Conference on Machine Learning},
	doi = {10.1145/1390156.1390177}
}
@article{Bahdanau,
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Y.},
	year = {2014},
	month = {09},
	pages = {},
	title = {Neural Machine Translation by Jointly Learning to Align and Translate},
	volume = {1409},
	journal = {ArXiv}
}

@article{o2013cross,
	title={A cross-national comparison of reminiscence functions between Canadian and Israeli older adults},
	author={O’Rourke, Norm and Carmel, Sara and Chaudhury, Habib and Polchenko, Natalia and Bachner, Yaacov G},
	journal={Journals of Gerontology Series B: Psychological Sciences and Social Sciences},
	volume={68},
	number={2},
	year={2013},
	publisher={Oxford University Press US}
}

@article{karlsson2014stories,
	title={Stories about life narrated by people with Alzheimer's disease},
	author={Karlsson, Eva and S{\"a}venstedt, Stefan and Axelsson, Karin and Zingmark, Karin},
	journal={Journal of Advanced Nursing},
	volume={70},
	number={12},
	year={2014},
	publisher={Wiley Online Library}
}

@book{linde1993life,
	title={Life stories: The creation of coherence},
	author={Linde, Charlotte and others},
	year={1993},
	publisher={Oxford University Press on Demand}
}



@article{thompsonlifestory,
	author = {Thompson, Rachel},
	year = {2011},
	month = {10},
	pages = {16-21},
	title = {Using life story work to enhance care},
	volume = {23},
	journal = {Nursing older people},
	doi = {10.7748/nop2011.10.23.8.16.c8713}
}


@article{sai2020survey,
	title={A survey of evaluation metrics used for NLG systems},
	author={Sai, Ananya B and Mohankumar, Akash Kumar and Khapra, Mitesh M},
	journal={arXiv preprint arXiv:2008.12009},
	year={2020}
} 

@inproceedings{Cho2014LearningPR,
	title={Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation},
	author={Kyunghyun Cho and Bart van Merrienboer and Çaglar G{\"u}lçehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
	booktitle={EMNLP},
	year={2014}
}
@article{goldberg1994using,
	title={Using natural-language processing to produce weather forecasts},
	author={Goldberg, Eli and Driedger, Norbert and Kittredge, Richard I},
	journal={IEEE Expert},
	volume={9},
	number={2},
	pages={45--53},
	year={1994},
	publisher={IEEE}
}
@inproceedings{sripada2014case,
	title={A case study: NLG meeting weather industry demand for quality and quantity of textual weather forecasts},
	author={Sripada, Somayajulu and Burnett, Neil and Turner, Ross and Mastin, John and Evans, Dave},
	booktitle={Proceedings of the 8th International Natural Language Generation Conference (INLG)},
	pages={1--5},
	year={2014}
}
@article{clarke2010discourse,
	title={Discourse constraints for document compression},
	author={Clarke, James and Lapata, Mirella},
	journal={Computational Linguistics},
	volume={36},
	number={3},
	pages={411--441},
	year={2010},
	publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{islam2018bangla,
	title={Bangla sentence correction using deep neural network based sequence to sequence learning},
	author={Islam, Sadidul and Sarkar, Mst Farhana and Hussain, Towhid and Hasan, Md Mehedi and Farid, Dewan Md and Shatabda, Swakkhar},
	booktitle={2018 21st International Conference of Computer and Information Technology (ICCIT)},
	pages={1--6},
	year={2018},
	organization={IEEE}
}

@inproceedings{leppanen2017data,
	title={Data-driven news generation for automated journalism},
	author={Lepp{\"a}nen, Leo and Munezero, Myriam and Granroth-Wilding, Mark and Toivonen, Hannu},
	booktitle={Proceedings of the 10th International Conference on Natural Language Generation},
	pages={188--197},
	year={2017}
}

@inproceedings{wang-cho-2019-bert,
	title = "{BERT} has a Mouth, and It Must Speak: {BERT} as a {M}arkov Random Field Language Model",
	author = "Wang, Alex  and
	Cho, Kyunghyun",
	booktitle = "Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation",
	month = jun,
	year = "2019",
	address = "Minneapolis, Minnesota",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/W19-2304",
	doi = "10.18653/v1/W19-2304",
	pages = "30--36",
}


@misc{fumagalli_2020,
	 title={Conditional story generation}, url={https://pub.towardsai.net/conditional-story-generation-part-1-6a5a16a63744}, 
	 journal={Medium},
	 publisher={Towards AI},
	 author={Fumagalli, Francesco},
	 year={2020},
	 month={Dec}
} 
 
@misc{howell_2022,
	 title={Markov chains simply explained},
	 url={https://towardsdatascience.com/markov-chains-simply-explained-dc77836b47e3}, 
	 journal={Medium},
	 publisher={Towards Data Science},
	 author={Howell, Egor},
	 year={2022},
	 month={Feb}
} 
  
@inproceedings{CaneteCFP2020,
	title={Spanish Pre-Trained BERT Model and Evaluation Data},
	author={Cañete, José and Chaperon, Gabriel and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and Pérez, Jorge},
	booktitle={PML4DC at ICLR 2020},
	year={2020}
}

@article{vicente2015generacion,
	title={La generaci{\'o}n de lenguaje natural: an{\'a}lisis del estado actual},
	author={Vicente, Marta and Barros, Cristina and Peregrino, Fernando S and Agull{\'o}, Francisco and Lloret, Elena},
	journal={Computaci{\'o}n y Sistemas},
	volume={19},
	number={4},
	pages={721--756},
	year={2015},
	publisher={Centro de Investigaci{\'o}n en computaci{\'o}n, IPN}
}

@article{reiter1997building,
	title={Building applied natural language generation systems},
	author={Reiter, Ehud and Dale, Robert},
	journal={Natural Language Engineering},
	volume={3},
	number={1},
	year={1997},
	publisher={Cambridge university press}
}

@article{vaswani2017attention,
	title={Attention is all you need},
	author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	journal={Advances in neural information processing systems},
	volume={30},
	year={2017}
}

@article{neuron,
	author = {Lukic, Bojan},
	year = {2020},
	month = {06},
	pages = {},
	title = {Applied Machine Learning Methods with Long-Short Term Memory Based Recurrent Neural Networks for Multivariate Temperature Prediction}
}

@article{lstm,
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	year = {1997},
	month = {12},
	pages = {1735-80},
	title = {Long Short-term Memory},
	volume = {9},
	journal = {Neural computation},
	doi = {10.1162/neco.1997.9.8.1735}
}

@article{firstrnn,
	author = {J J Hopfield },
	title = {Neural networks and physical systems with emergent collective computational abilities.},
	journal = {Proceedings of the National Academy of Sciences},
	volume = {79},
	number = {8},
	pages = {2554-2558},
	year = {1982},
	doi = {10.1073/pnas.79.8.2554},
	
	URL = {https://www.pnas.org/doi/abs/10.1073/pnas.79.8.2554},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.79.8.2554}
}

@unknown{atencion_capas,
	author = {Zhou, Long and Zhang, Jiajun and Zong, Chengqing},
	year = {2019},
	month = {05},
	pages = {},
	title = {Synchronous Bidirectional Neural Machine Translation}
}

@article{luong2015effective,
	title={Effective approaches to attention-based neural machine translation},
	author={Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D},
	journal={arXiv preprint arXiv:1508.04025},
	year={2015}
}

@inproceedings{aramossoto2017adapting,
	title = {Adapting {SimpleNLG} to Spanish},
	journal = {10th International Conference on Natural Language Generation},
	year = {2017},
	pages = {144-148},
	abstract = {We describe SimpleNLG-ES, an adaptation of the SimpleNLG realization library for the Spanish language. Our implementation is based on the bilingual English-French SimpleNLG-EnFr adaptation. The library has been tested using a battery of examples that ensure that the most common syntax, morphology and orthography rules for Spanish are met. The library is currently being used in three different projects for the development of data-to-text systems in the meteorological, statistical data information, and business intelligence application domains.},
	isbn = {978-1-945626-52-4},
	publisher = {Association for Computational Linguistics},
	author = {A. Ramos-Soto and J. Janeiro-Gallardo and Alberto Bugar\'{i}n}
}

@misc{analytics_vidhya_2020,
 	 title={Natural language generation using pytorch: Model and generate text data}, url={https://www.analyticsvidhya.com/blog/2020/08/build-a-natural-language-generation-nlg-system-using-pytorch/}, 
 	 journal={Analytics Vidhya}, 
 	 author={Prateek Joshi},
 	 year={2020}, 
 	 month={Aug}
} 
 
@misc{rajasekharan_2019, 
	title={A review of Bert based models}, url={https://towardsdatascience.com/a-review-of-bert-based-models-4ffdc0f15d58}, 
	journal={Medium}, 
	publisher={Towards Data Science}, 
	author={Rajasekharan, Ajit}, 
	year={2019}, 
	month={Aug}
} 
@article{damonte2019structural,
	title={Structural neural encoders for AMR-to-text generation},
	author={Damonte, Marco and Cohen, Shay B},
	journal={arXiv preprint arXiv:1903.11410},
	year={2019}
}
 
@article{moryossef2019step,
	title={Step-by-step: Separating planning from realization in neural data-to-text generation},
	author={Moryossef, Amit and Goldberg, Yoav and Dagan, Ido},
	journal={arXiv preprint arXiv:1904.03396},
	year={2019}
}
@inproceedings{yang2020creative,
	title = {Creative storytelling with language models and knowledge graphs},
	author = {Yang, Xinran and Tiddi, Ilaria},
	booktitle = {CIKM (Workshops)},
	year = {2020}
}

@article{MoralesdeJess2020ACM,
	title={A Conversational Model for the Reminiscence Therapy of Patients with Early Stage of Alzheimer},
	author={V{\'i}ctor Manuel Morales-de-Jes{\'u}s and Mar{\'i}a Josefa Somodevilla Garc{\'i}a},
	journal={Res. Comput. Sci.},
	year={2020},
	volume={149},
	pages={57-67}
}

@article{shi2012user,
	title={User-oriented ontology-based clustering of stored memories},
	author={Shi, Lei and Setchi, Rossitza},
	journal={Expert Systems with Applications},
	volume={39},
	number={10},
	pages={9730--9742},
	year={2012},
	publisher={Elsevier}
}







@Book{morozov2015la,
	author = {Morozov, Evgeny},
	title = {La locura del solucionismo tecnologico},
	publisher = {Katz},
	year = {2015},
	address = {Madrid},
	isbn = {9788415917199}
}

@inproceedings{parikh-etal-2020-totto,
	title = "{ToTTo}: A Controlled Table-To-Text Generation Dataset",
	author = "Parikh, Ankur  and
	Wang, Xuezhi  and
	Gehrmann, Sebastian  and
	Faruqui, Manaal  and
	Dhingra, Bhuwan  and
	Yang, Diyi  and
	Das, Dipanjan",
	booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.emnlp-main.89",
	doi = "10.18653/v1/2020.emnlp-main.89",
	pages = "1173--1186",
}

@article{hallucinations_data2text,
	author = {Rebuffel, Clement and Roberti, Marco and Soulier, Laure and Scoutheeten, Geoffrey and Cancelliere, Rossella and Gallinari, Patrick},
	year = {2022},
	month = {01},
	pages = {1-37},
	title = {Controlling hallucinations at word level in data-to-text generation},
	volume = {36},
	journal = {Data Mining and Knowledge Discovery},
	doi = {10.1007/s10618-021-00801-4}
}

@inproceedings{rohrbach-etal-2018-object,
	title = "Object Hallucination in Image Captioning",
	author = "Rohrbach, Anna  and
	Hendricks, Lisa Anne  and
	Burns, Kaylee  and
	Darrell, Trevor  and
	Saenko, Kate",
	booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
	month = oct # "-" # nov,
	year = "2018",
	address = "Brussels, Belgium",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D18-1437",
	doi = "10.18653/v1/D18-1437",
	pages = "4035--4045"
}

@unknown{hallucination_survey,
	author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiehzheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Yejin and Madotto, Andrea and Fung, Pascale},
	year = {2022},
	month = {02},
	pages = {},
	title = {Survey of Hallucination in Natural Language Generation}
}

@article{lee2021deduplicating,
	title={Deduplicating training data makes language models better},
	author={Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
	journal={arXiv preprint arXiv:2107.06499},
	year={2021}
}

@article{radford2019language,
	title={Language Models are Unsupervised Multitask Learners},
	author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year={2019}
}

@article{gage1994new,
	title={A new algorithm for data compression},
	author={Gage, Philip},
	journal={C Users Journal},
	volume={12},
	number={2},
	pages={23--38},
	year={1994},
	publisher={McPherson, KS: R \& D Publications, c1987-1994.}
}

@inproceedings{Zhu_2015_ICCV,
	title = {Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
	author = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
	booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
	month = {December},
	year = {2015}
}
@inproceedings{Devlin2019BERTPO,
	title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
	booktitle={NAACL},
	year={2019}
}

@inproceedings{wordpiece,
	author = {Schuster, Mike and Nakajima, Kaisuke},
	year = {2012},
	month = {03},
	pages = {5149-5152},
	title = {Japanese and Korean voice search},
	isbn = {978-1-4673-0045-2},
	journal = {Acoustics, Speech, and Signal Processing, 1988. ICASSP-88., 1988 International Conference on},
	doi = {10.1109/ICASSP.2012.6289079}
}

@article{2020t5,
	author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
	title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	journal = {Journal of Machine Learning Research},
	year    = {2020},
	volume  = {21},
	number  = {140},
	pages   = {1-67},
	url     = {http://jmlr.org/papers/v21/20-074.html}
}

@inproceedings{bostrom-durrett-2020-byte,
	title = "Byte Pair Encoding is Suboptimal for Language Model Pretraining",
	author = "Bostrom, Kaj  and
	Durrett, Greg",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.findings-emnlp.414",
	doi = "10.18653/v1/2020.findings-emnlp.414",
	pages = "4617--4624",
	abstract = "The success of pretrained transformer language models (LMs) in natural language processing has led to a wide range of pretraining setups. In particular, these models employ a variety of subword tokenization methods, most notably byte-pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994), the WordPiece method (Schuster and Nakajima, 2012), and unigram language modeling (Kudo, 2018), to segment text. However, to the best of our knowledge, the literature does not contain a direct evaluation of the impact of tokenization on language model pretraining. We analyze differences between BPE and unigram LM tokenization, finding that the latter method recovers subword units that align more closely with morphology and avoids problems stemming from BPE{'}s greedy construction procedure. We then compare the fine-tuned task performance of identical transformer masked language models pretrained with these tokenizations. Across downstream tasks and two languages (English and Japanese), we find that the unigram LM tokenization method matches or outperforms BPE. We hope that developers of future pretrained LMs will consider adopting the unigram LM method over the more prevalent BPE.",
}

@article{lebret-etal-2016-neural,
	title = "Neural Text Generation from Structured Data with Application to the Biography Domain",
	author = "Lebret, R{\'e}mi  and
	Grangier, David  and
	Auli, Michael",
	booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
	journal  = CoRR,
	month = nov,
	year = "2016",
	address = "Austin, Texas",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D16-1128",
	doi = "10.18653/v1/D16-1128",
	pages = "1203--1213",
	
}

@article{dhingra2019handling,
	title={Handling divergent reference texts when evaluating table-to-text generation},
	author={Dhingra, Bhuwan and Faruqui, Manaal and Parikh, Ankur and Chang, Ming-Wei and Das, Dipanjan and Cohen, William W},
	journal={arXiv preprint arXiv:1906.01081},
	year={2019}
}

@inproceedings{agarwal-etal-2021-knowledge,
	title = "Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training",
	author = "Agarwal, Oshin  and
	Ge, Heming  and
	Shakeri, Siamak  and
	Al-Rfou, Rami",
	booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
	month = jun,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.naacl-main.278",
	doi = "10.18653/v1/2021.naacl-main.278",
	pages = "3554--3565",
	abstract = "Prior work on Data-To-Text Generation, the task of converting knowledge graph (KG) triples into natural text, focused on domain-specific benchmark datasets. In this paper, however, we verbalize the entire English Wikidata KG, and discuss the unique challenges associated with a broad, open-domain, large-scale verbalization. We further show that verbalizing a comprehensive, encyclopedic KG like Wikidata can be used to integrate structured KGs and natural language corpora. In contrast to the many architectures that have been developed to integrate these two sources, our approach converts the KG into natural text, allowing it to be seamlessly integrated into existing language models. It carries the further advantages of improved factual accuracy and reduced toxicity in the resulting language model. We evaluate this approach by augmenting the retrieval corpus in a retrieval language model and showing significant improvements on the knowledge intensive tasks of open domain QA and the LAMA knowledge probe.",
}

@article{pan2009survey,
	title={A survey on transfer learning},
	author={Pan, Sinno Jialin and Yang, Qiang},
	journal={IEEE Transactions on knowledge and data engineering},
	volume={22},
	number={10},
	pages={1345--1359},
	year={2009},
	publisher={IEEE}
}

@INPROCEEDINGS{xinyuan2018,  author={Tu, Xinyuan and Lai, Kenneth and Yanushkevich, Svetlana},  booktitle={2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS)},   title={Transfer Learning on Convolutional Neural Networks for Dog Identification},   year={2018},  volume={},  number={},  pages={357-360},  doi={10.1109/ICSESS.2018.8663718}}

@article{slovikovskaya2019transfer,
	title={Transfer learning from transformers to fake news challenge stance detection (FNC-1) task},
	author={Slovikovskaya, Valeriya},
	journal={arXiv preprint arXiv:1910.14353},
	year={2019}
}