\chapter{Conjuntos de datos y preparación para el entrenamiento}
\label{cap:ConjuntosDeDatosYPreparacionParaElEntrenamiento}

En este capítulo se presentan soluciones de datos efectivas para cubrir los requisitos de generación del sistema. Entre estos requisitos se encuentra la generación de textos que representen textualmente información bibliográfica de una persona. Otra de las necesidades a cubrir es la generación textual a partir de unos datos precisados como entrada al sistema. 

La aparición de los modelos \textit{transformers} como evolución de los modelos basados en redes neuronales recurrentes para la generación de lenguaje natural, supuso un cambio de paradigma en el modelado de lenguaje debido a la introducción de mecanismos de preentrenaminto y ajuste o \textit{finetune}.
Mediante la utilización de modelos de lenguaje tal cual han sido preentrenados, ninguno de los requisitos de generación mencionados podrían cubrirse. Es por ello que se pretende la construcción de adaptaciones de dichos modelos de lenguaje mediante el ajuste de los mismos utilizando para ello conjunto de datos que satisfagan los requisitos.


Son muchas las bases de datos existentes para realizar tareas de ajuste de modelos. Algunos corpus como \textit{News Aggregator}\footnote{Disponible en Kaggle https://www.kaggle.com/datasets/uciml/news-aggregator-dataset} pueden ser utilizados para la creación de noticias. Otra tarea puede ser la generación de recetas, utilizando para ello \textit{datasets} como \textit{recipe-box}\footnote{Disponible en github https://github.com/rtlee9/recipe-box}.
Es importante realizar una correcta elección del conjunto de datos sobre el que se va a entrenar el modelo. La elección de la base de datos no es trivial, sino que obedece a las necesidades establecidas: en nuestro caso, generación de texto biográfico a partir de unos datos especificados como entrada. 
Así, se realizó una búsqueda de conjunto de datos apropiados a las características necesarias para construir el sistema propuesto en este trabajo, resultando esta búsqueda en tres grandes bases de datos: \textit{Wiki2bio}, \textit{WebNLG} y \textit{KELM}, que serán descritos a continuación.



\section{Wiki2bio}

Wiki2bio es un conjunto de datos propuesto por \citep{lebret-etal-2016-neural}. Fue creado debido a la necesidad de existencia de una gran base de datos que permitiera la redacción notas biográficas. Hasta entonces, las bases de datos con información bibliográfica eran demasiado pequeñas para entrenar un modelo de red neuronal, por lo que se ideó la construcción de un conjunto de una orden de magnitud superior. Compuesto por más de 700.000 ejemplos y un vocabulario de 400.000 palabras, extrajeron todos estos datos mapeando los datos contenidos en las tablas de información de Wikipedia con los textos descriptivos escritos en lenguaje natural.

Antes de realizar cualquier entrenamiento en el modelo bajo un conjunto de datos es necesario realizar un análisis y preprocesamiento previo que nos permita conocer más afondo dicho conjunto. Como primer paso, vamos a importar todas las bibliotecas de Python necesarias que se utilizarán durante el procesamiento.

Existen diferentes métodos para obtener datos de Wiki2bio. La manera más sencilla de descargar el conunto de datos completo es hacer uso de las API de HuggingFace o TensorFlow, ya que devuelven los datos encapsulados en un objeto procesable con un conjunto de herramientas adecuadas para la tarea. En este caso, nos quedaremos con la primera opción ya que posteriormente emplearemos los modelos de lenguaje de esta misma API. 

El conjunto completo de datos se descarga finalmente a través de la biblioteca \textit{datasets} de HuggingFace \footnote{Disponible en https://huggingface.co/datasets/wiki\_bio} que devuelve un objetivo de tipo \textit{DatasetDict}. El archivo final ocupa 738.19 MB y está compuesto por un total de 728.321 muestras.
El conjunto de datos se divide aleatoriamente en tres subconjuntos: conjunto para entrenamiento (80\%), conjunto para validación (10\%) y conjunto para prueba (10\%), constando cada uno de ellos de 582.659, 72.831 y 72.831 ejemplos, respectivamente.

Un ejemplo de los cualquiera presentes en este conjunto de datos es representado en la figura~\ref{fig:ex_wiki2bio}. Como se puede apreciar, la información de cada uno de los ejemplos se divide en dos grupos. Por una parte, el \textit{input\_text} o texto de entrada especifica el contexto del que se están proporcionando los datos y el cuadro de información de Wikipedia en formato tabular. Los datos contenidos en esta tabla establecen asignaciones a características del contexto como son la nacionalidad, fecha de nacimiento o trabajo. El otro grupo corresponde a la información de destino o \textit{target\_text} que representa en lenguaje natural la información contenida en el contexto y la tabla anterior.


\figura{Bitmap/04ConjuntoDeDatos/wiki2bio}{width=1\textwidth}{fig:ex_wiki2bio}%
{Ejemplo de entrada de wiki2bio}


Después de descargar con éxito la base de datos completa, se debe realizar una limpieza de los datos ya que en numerosas ocasiones aparecen saltos de linea o algunos caracteres como paréntesis o corchetes codificados como símbolos en mitad de palabras u oraciones y esto podría llevar a la generación de datos incorrectos e imprecisos después del entrenamiento. El resultado de esta limpieza de los datos llevada a cabo se muestra en la figura~\ref{fig:ex_wiki2bio_clean}


\figura{Bitmap/04ConjuntoDeDatos/wiki2bio_limpieza}{width=1\textwidth}{fig:ex_wiki2bio_clean}%
{Ejemplo de wiki2bio con y sin limpieza de datos}



\section{WebNLG}
Introducido por \citep{gardent2017creating}, el corpus WebNLG se compone de conjunto de tripletas semánticas, que describen hechos en forma de entidades y relaciones entre ellas, y los eventos correspondientes presentados en lenguaje natural. Este conjunto de datos nace de la explosión de desarrollo de bases de datos RDF (Resouce Description Format), cuya entidad atómica de datos es precisamente la tripleta semántica. Una tripleta está formada por un conjunto de tres entidades que codifica una relación entre datos semánticos, en el caso de WebNLG, expresada de la forma <sujeto - predicado - objeto>. Para la creación de esta base de datos, se parte de la base de conocimiento DBPedia construida a partir de datos estructurados de Wikipedia. Según mencionan sus autores, esta base de datos se crea con la finalidad de suplir las tareas de lexicalización, agregación de oraciones y realización de la arquitectura tradicional presentada en la sección~\ref{sec:arquitectura_tradicional}.

Esta base de datos \footnote{Disponible en https://gitlab.com/shimorina/webnlg-dataset/-/tree/master/} está disponible en inglés, aunque la tercera versión del conjunto de datos también incluye una colección de datos en ruso (es necesario precisar que los conjuntos de datos en idiomas diferentes se encuentran separados). Un total de 45.050 oraciones componen WebNLG, dividiendose a su vez en tres subconjuntos de datos: \textit{train}, \textit{dev}  y \textit{test}, conteniendo cada uno 35.426, 4.464 y 5.150 oraciones, respectivamente. 

\figura{Bitmap/04ConjuntoDeDatos/webnlg_ex}{width=1\textwidth}{fig:webnlg_ex}%
{Ejemplo de WebNLG}

El formato de un ejemplo del \textit{dataset} se encuentra en la figura~\ref{fig:webnlg_ex}. Como se puede comprobar, cada ejemplo está formado por diversos campos. En primer lugar, aparecen una serie de información a cerca de los datos que nos vamos a encontrar en el ejemplo como el id, categoría, forma, tipos de estructura de las tripletas y número de las mismas. Por una parte, la categoría representa el tipo de datos de la entidad representada en la DBpedia y el 'eid' un id único en el subconjunto de datos y categoría para el ejemplo. También se muestra el número de tripletas en el conjunto, representado por el campo 'size'. Por otra parte, se tiene en cuenta la forma del árbol formado por el conjunto de tripletas en el campo 'shape', que representa dicho árbol en forma de cadena formado por paréntesis anidados donde X es un nodo, este formato se basa en la representación de árboles de Newick. En el campo 'shape\_type' se indica si el objeto de una tripleta es el sujeto de otra (\textit{chain}); si las tripletas tienen un tema compartido (\textit{siblings}) o si se dan ambos casos (\textit{mixed}). 



Finalmente, se muestra la información más interesante de este conjunto de datos. Junto con unos ids y la calidad  de las lexicalizaciones (\textit{good} o \textit{bad} en el campo 'comment') aparecen diferentes oraciones (campo 'text') que representan en lenguaje natural la información dada por los conjuntos de tripletas ('modified\_triples\_sets' y 'original\_triple\_sets'). Es destacable la existencia de varias oraciones para representar la misma información de un solo conjunto de tripletas y de varios conjuntos de tripletas representantes todas de la misma información.


Dado que el formato en el que descargamos los ficheros que componen esta \textit{dataset} es XML, se cargaron los datos y se pasaron a un objeto que pudiera entrenar el modelo, en esta caso un DataFrame que finalmente incluía 73.119 ejemplos para el entrenamiento. Después de procesar los datos para asignar a cada uno de los conjuntos de tripletas cada una de las soluciones en lenguaje natural, el resultado final se muestra en la figura~\ref{fig:webnlg_ex_own}.


\begin{figure}[!h]
	\centering
	%
	\includegraphics[width=1\textwidth]%
	{Imagenes/Bitmap/04ConjuntoDeDatos/webnlg_ex_own}%
	\caption{Ejemplo procesado de WebNLG%
		\label{fig:webnlg_ex_own}}
\end{figure}


\section{KEML}
KELM es un enorme corpus de datos en inglés construido específicamente para un sistema \textit{data-to-text} llamado TeKGen (\textit{Text from KG Generator}) propuesto por \cite{agarwal-etal-2021-knowledge}. Este sistema se basa en un modelo secuencia a secuencia para generar texto en lenguaje natural a partir de un grafo de conocimiento o \textit{Knowledge Graph} (KG) presentado a través de tripletas semánticas. La motivación de construcción de este conjunto de datos surge de la necesidad de variedad de entidades y relaciones que, según sus autores, WebNLG no tenía. De acuerdo a las cifras que mencionan, los datos de Wikidata (base de conocimiento con datos estructurados que sirve como base de datos secundaria para dar soporte a Wikipedia y como la que se ha construido el corpus KELM) podrían representarse en aproximadamente 6 millones de entidades y 1500 relaciones, frente a las 600 entidades y 20 relaciones de WebNLG.


Para crear este enorme corpus de datos, sus autores utilizaron supervisión a distancia para alinear las tripletas de Wikidata con el texto de Wikipedia. Para cada una de las entidades de Wikidata, se seleccionaron como oraciones candidatas aquellas que aparecen en la primera sección de su página de Wikipedia. A continuación, emparejaron con cada frase de esta sección las tripletas que contenían la entidad como sujeto. De esta manera, una tripleta puede alinearse con varias oraciones de la sección y a su vez, una sección puede alinearse con varias tripletas. Como observación, realizaron una heurística de sustitución de pronombres ya que en numerosas ocasiones las oraciones contenían este tipo de elemento sintáctico.
 
Finalmente, el conjunto de datos resultante está compuesto, según las cifras oficiales proporcionadas por sus creadores, por más de 18 millones de oraciones, 45 millones de tripletas y 1500 relaciones diferentes.




%Datasets such as WebNLG have instances with grouped triples that can be expressed as a fluent sentence. Such groups are not available for a large KG and using one triple at a time for inference would lead to hallucination as training uses multiple triples per example. Therefore, we develop a strategy to create entity subgraphs based on relation co-occurrence counts i.e. frequency of alignment of two relations to the same sentence in the training data. The algorithm is shown in Figure 4. It produces ∼18M entity subgraphs from ∼45M triples so the final corpus will have 18M generated sentences corresponding to each entity subgraph.


Al igual que las bases de datos anteriores, KELM se encuentra disponible para descargar desde la API de HuggingFace \footnote{Disponible en https://huggingface.co/datasets/kelm} y ocupa en total de 3.08 GB de tamaño y 7.964.073 ejemplos en total. Está dividida en 3 subconjuntos de datos: datos de entrenamiento, prueba y validación. El número de ejemplos de cada uno de los subconjuntos mencionados anteriormente son 6.371.131, 796.471 y 796.471, que corresponde respectivamente al 80\%. 10\% y 10\% del número de ejemplos totales del conjunto de datos. 

\begin{figure}[!h]
	\centering
	%
	\begin{SubFloat}
		{\label{fig:kelm_ex1}%
			Ejemplo 1 de KELM}%
		\includegraphics[width=1\textwidth]%
		{Imagenes/Bitmap/04ConjuntoDeDatos/kelm_ex1}%
	\end{SubFloat}
	\qquad
	\begin{SubFloat}
		{\label{fig:kelm_ex2}%
			Ejemplo 2 de KELM}%
		\includegraphics[width=1\textwidth]%
		{Imagenes/Bitmap/04ConjuntoDeDatos/kelm_ex2}%
	\end{SubFloat}
	\caption{Diversos ejemplos del conjunto de datos KELM%
		\label{fig:kelm_ex}}
\end{figure}


En la figura~\ref{fig:kelm_ex}, se muestran dos ejemplos aleatorios del conjunto de datos KELM. Como se puede apreciar, cada ejemplo está formado por dos partes: la oración (\textit{sentence}), escrita en lenguaje natural; y la lista de tripletas (\textit{triple}), formada por las tripletas alineadas al texto de la oración separadas por una coma. Se muestran dos ejemplos diferentes ya que en el proceso de análisis de los datos se encontró que la lista de tripletas puede adoptar diversas estructuras. La primera e supone que todas las relaciones mencionadas se rforma, correspondiente al primer ejemplo mostrado (figura~\ref{fig:kelm_ex1}), es representada de la manera (entidad\_sujeto, nombre\_relacion1, valor\_relacion1), (nombre\_relacion2, valor\_relacion2)... Sefieren a la misma entidad sujeto. Otra estructura se muestra en la figura~\ref{fig:kelm_ex2} en la que debido a la diferencia de la entidad sujeto en las tripletas representadas, cada una de las tripletas es independiente de la otra adquiriendo la forma (entidad\_sujeto1, nombre\_relacion1, valor\_relacion1), (entidad\_sujeto2, nombre\_relacion2, valor\_relacion2)...

Como se puede comprobar en los ejemplos presentados, esta base de datos no solo contiene información  biográfica de personas sino que también consta de conjuntos de ejemplos sobre otros temas. Es por esto que puede resultar adecuada para el sistema que se pretende construir ya que podrá representar cualquier tipo de información que se desee sin limitarse al formato biográfico de wiki2bio.





Con respecto al procesado de los datos, hay que destacar la limpieza de los datos originales. Únicamente hizo falta cambiar la codificación de los datos ya que se encontraban codificados en Windows-1252 por lo que algunos caracteres eran representados en un formato no apropiado para realizar el entrenamiento.