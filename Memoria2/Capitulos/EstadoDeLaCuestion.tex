\chapter{Estado de la Cuestión}
\label{cap:estadoDeLaCuestion}
En este capítulo se realizará una aproximación a las historias de vida como terapia de reminiscencia de la enfermedad de Alzheimer. A continuación, se presentarán las técnicas de generación de lenguaje natural con mayor relevancia como método alternativo de composición de dichas historias de vida. Para ello, navegaremos a lo largo de la historia de la generación, concretamente del modelado de lenguaje como motor del sistema. Se  abordarán las técnicas más antiguas de la generación, como el análisis de la arquitectura tradicional de un sistema de generación de lenguaje genérico, hasta los modelos de lenguaje más novedosos.%TODO

\section{Alzheimer e historias de vida}
La pirámide poblacional modifica su estructura continuamente debido al progresivo envejecimiento generalizado de la población. Según proyecciones de la \cite{alzheimers-association-media-line-2021}, en el año 2050 las personas mayores de 65 años constituirán el 16 por ciento de la población mundial frente al 8 por ciento del año 2010. El aumento de la esperanza de vida en todo el mundo, principalmente en las sociedades más avanzadas, y la disminución de la natalidad, se encuentran entre las causas de la modificación de la distribución demográfica hacia edades más avanzadas. Este fenómeno es conocido como \textit{inversión de la pirámide poblacional} \citep{RCSP892}.

La realidad detrás de estas estadísticas: el incremento del número de personas de edad avanzada, y asociándose al envejecimiento la acumulación a lo largo del tiempo de una gran variedad de daños moleculares y celulares que lleva a un descenso gradual de las capacidades mentales y físicas, deriva en un mayor riesgo de determinadas enfermedades. 

La pérdida de la audición, las cataratas, la artritis y la artrosis son solo algunas de las enfermedades con mayor incidencia. Sin embargo, una de las dolencias más comunes y serias dentro de este rango de población es la enfermedad de Alzheimer, cuya prevalencia a nivel global se espera que supere todo dato conocido hasta ahora, dado que se estima que en el año 2050 se incremente el número de casos a 152,8 millones, sobrepasando considerablemente los 57,4 millones del año 2019 \citep{alzheimers_disease_international_2019}.


\subsection{Descripción general}
La enfermedad de Alzheimer es un trastorno neurológico caracterizado por cambios degenerativos en diferentes sistemas neurotransmisores que abocan finalmente a la muerte de las células nerviosas del cerebro encargadas del almacenamiento y procesamiento de la información. Las regiones del cerebro involucradas con la memoria y los procesos de aprendizaje, asociadas a los lóbulos temporal y frontal, reducen su tamaño como consecuencia de la degeneración de las sinapsis y la muerte de las neuronas \citep{romano2007enfermedad,mattson2004pathways}. En las etapas finales de esta patología, este proceso, también denominado \textit{atrofia cerebral} se extiende y provoca una pérdida significativa del volumen cerebral (figura~\ref{fig:CerebroPersonaAlzheimer} a).


\figura{Vectorial/02EstadoDeLaCuestion/CerebroPersonaAlzheimer}%
{width=.8\textwidth}%
{fig:CerebroPersonaAlzheimer}%
{Reducción del cerebro asociada al Alzheimer \citep{mattson2004pathways}}

En numerosas ocasiones son utilizadas imágenes similares a las mostradas en la figura~\ref{fig:CerebroPersonaAlzheimer} como indicativos de la enfermedad del Alzheimer. La figura~\ref{fig:CerebroPersonaAlzheimer} b  representa unas \textit{tomografías por emisión de positrones} o \textit{PET scans} en inglés. En ellas se reflejan los patrones de distribución espacial de la glucosa en el cerebro. En el cerebro de la persona con Alzheimer, el flujo glucolítico cerebral se minimiza provocando los síntomas de la enfermedad. Esta prueba se utiliza en el diagnóstico de la gravedad de la patología.


El proceso de detección de la enfermedad de Alzheimer es una tarea ardua de realizar dado que, por lo general, los síntomas iniciales de la enfermedad suelen atribuirse a un olvido puntual o la vejez. Nada más lejos de la realidad. Según avanza la enfermedad, sus síntomas lo hacen con ella, agravándose y aumentando cada vez más hasta que el deterioro cognitivo ocasionado llega a afectar significativamente a las actividades de la vida diaria y finalmente a las necesidades fisiológicas básicas.

La evolución del Alzheimer se puede dividir en tres fases o etapas. En una primera instancia, se comienza a observar un deterioro cognitivo leve como puede ser la pérdida paulatina de la memoria episódica, seguido de pérdidas de la memoria reciente asociadas a un deterioro mayor así como otras funciones mentales y de la personalidad. Para terminar, se produce una pérdida progresiva de la memoria referida a los acontecimientos más antiguos, acompañando además un importante deterioro físico.


\subsection{Síntomatología y pérdida de la memoria}
La amnesia o pérdida de la memoria es uno de los síntomas más representativos del Alzheimer. Sin embargo, se trata tan solo de la punta del iceberg debido a todos los desordenes que también se producen y que no son considerados o tenidos en cuenta por el personal no profesional: alteraciones del estado de ánimo y la conducta, dificultad de toma de decisiones, desorientación, problemas del lenguaje, dificultad para comer, movilidad reducida  y un largo etcétera son algunos de los síntomas que acompañan a esta enfermedad durante todo su camino. Todos estos síntomas dependen de la fase evolutiva de la enfermedad.

Podemos distinguir en cuanto a sintomatología dos fases marcadas por las alteraciones neurológicas: en una primera fase, conocida como fase predemencial, los signos de desordenes neurológicos todavía no se encuentran presentes; y la fase demencial, en la que se pueden observar grandes alteraciones motoras, cognitivas, sensoriales y emocionales.

En la etapa predemencial, durante la cual en numerosas ocasiones el paciente no se encuentra diagnosticado de la enfermedad, comienzan a producirse lesiones microscópicas en el cerebro. Sin embargo, no es hasta entre 10 y 20 años después que pueden aparecer las primeras alteraciones cognitivas. El conjunto de síntomas presentes en esta fase comprende principalmente alteraciones en la conducta como trastorno de la personalidad, apatía o cambios en el estado de ánimo; y deterioro gradual de la memoria, comenzando el paciente a olvidar pequeñas cosas hasta llegar a no ser capaz de recordar familia o amigos.

A medida que progresa el daño cerebral aparece progresivamente un deterioro más pronunciado del paciente, comenzando entonce la fase demencial de la enfermedad. En esta etapa comienzan a aparecer alteraciones neurológicas como pérdida del movimiento, temblores, alucinaciones,  trastornos en el lenguaje oral y escrito o alteraciones de la personalidad \citep{alberca-serrano-2010}.


\subsection{Tratamientos: historias de vida}
En la actualidad el Alzheimer es una enfermedad irreversible. Sin embargo, existen diversos tratamientos disponibles para ralentizar el avance de la enfermedad, así como mejorar la calidad de vida de los pacientes. Estos tratamientos se pueden dividir en dos ramas diferenciadas: tratamientos farmacológicos o farmacoterapia, que hacen uso de medicamentos; y tratamientos no farmacológicos o psicosociales, que no hacen uso de sustancias químicas. Ambos tipos de tratamientos resultan eficaces para tratar la enfermedad de Alzheimer. Sin embargo, de la combinación de ambos resulta el procedimiento más recomendado debido a su mayor efectividad. Esto es posible gracias a que ambos tipos de tratamientos no son mutuamente excluyentes \citep{romano2007enfermedad}.

Existen una gran variedad de terapias no farmacológicas. Algunas de las más utilizadas son el entrenamiento y estimulación cognitiva, ejercicio físico o musicoterapia. Además, en cada una de estas terapias podemos encontrar una enorme cantidad de técnicas, siendo la reminiscencia la más utilizada como terapia de estimulación cognitiva.

Según \cite{o2013cross}, la reminiscencia es el acto o proceso de recordar sucesos, eventos o información del pasado. Esto puede implicar el recuerdo de episodios particulares o genéricos que pueden o no haber sido olvidados previamente, y que son acompañados por la sensación de que estos episodios son relatos verídicos de las experiencias originales. Esta técnica es empleada en la estimulación del la memoria episódica autobiográfica mediante el encadenamiento de recuerdos, que se agrupan en categorías y se archivan en el tiempo mediante la elaboración de la \textit{historia de vida}.

La historia de vida es una técnica narrativa que se basa en organizar y estructurar recuerdos de una persona para componer una autobiografía. Según \cite{linde1993life}, una historia de vida debe cumplir dos criterios: primero, debe incluir algunos puntos de evaluación que comuniquen los valores morales de la persona; y segundo, los eventos incluidos en la historia de vida deben tener un significado especial y ser de importancia para ella. Estos eventos deben ser aspectos significativos de la vida pasada de la persona, su presente y su futuro.

Para componer la historia de vida de una persona con Alzheimer se recopilan historias a través de familiares u otras personas cercanas. Posteriormente, se documentan en forma de un libro o cuaderno, incluyendo experiencias y logros junto con fotografías y escritos sobre hechos importantes para la vida de la persona, a través de los cuales se muestra quién es esa persona.

Cada persona tiene su propia historia de vida única. Nuestras experiencias nos modelan y construyen la persona que somos. Las historias de vida ayudan a las personas con Alzheimer a conectar con su identidad recordando épocas felices. El miedo y la frustración provocados por el olvido de las tareas de la vida cotidiana, nombres y rostros, se mitigan recordando quiénes eran a través de estas historias. Les ayuda a ser conscientes de los momentos especiales que han marcado su vida, las personas que han conocido en su infancia o trabajo. También pueden ser utilizados por los cuidadores para comprender más sobre ellos, quiénes son, y ayudarles en la reminiscencia de recuerdos \citep{karlsson2014stories}.


Existen diferentes formatos en los que se pueden registrar estas experiencias de la persona. Ninguno de ellos es mejor o peor que otro, sino que lo ideal es utilizar aquel que mejor se adapte a la persona y a los hechos que se quieran transmitir.

Por una parte encontramos historias de vida más visuales, compuestas enteramente de imágenes (\textit{collages}) o videos, dirigidas especialmente a las personas con Alzheimer que se encuentran en una etapa tardía de la enfermedad. Otro formato se centra especialmente en textos. Los \textit{libros de vida}, destinados a los cuidadores y visitantes tanto como a la propia persona, combina las \textit{historias de vida}, en forma de texto claro y fácil de leer, con algunas imágenes. También nos encontramos los documentos de perfil personal que se centran en pequeñas versiones cortas de las historias de vida excluyendo las imágenes. Estos documentos son utilizados a menudo en hospitales y están diseñados para ayudar al personal a comprender las necesidades de la persona.

El contenido de una historia de vida es variable, aunque existen algunos temas básicos en los que se debe centrar: el perfil de la persona, incluyendo datos e información básica como es el nombre, edad, lugar de nacimiento o de residencia son esenciales para aproximarse de manera inicial a la persona. Otros temas como las relaciones significativas familiares y de amistad, infancia, lugares y eventos significativos y gustos o preferencias y aficiones son incluidos dentro de esta lista de posibles temas a tratar el la historia de vida \citep{thompsonlifestory}.

\section{Generación de lenguaje natural}

La Generación de Lenguaje Natural (GLN) se define como el ``subcampo de la inteligencia artificial y la lingüística computacional que se ocupa de la construcción de sistemas informáticos que pueden producir textos comprensibles en inglés u otros lenguajes humanos a partir de alguna representación no lingüística subyacente de la información''  \citep{reiter1997building}. Si bien esta definición estuvo generalmente aceptada como la más conveniente al hablar de generación de lenguaje natural durante muchos años, \cite{gatt2018survey} puntualizan que es una afirmación que solo engloba una parte de la generación de textos, ya que se refiere únicamente a aquellos sistemas cuya entrada es una ``representación no lingüística [...] de la información'' o datos, como veremos más adelante en el apartado \ref{cap:nglD2T}.

Desde hace muchos años, la GLN es empleada en numerosos proyectos de distinta naturaleza como la traducción de textos \citep{Cho2014LearningPR}, realización de resúmenes y fusión de documentos \citep{clarke2010discourse}, corrección automática de ortografía y gramática \citep{islam2018bangla}, redacción de noticias \citep{leppanen2017data}, informes meteorológicos \citep{sripada2014case} y financieros \citep{ren2021hybrid}, generación de resúmenes sobre la información de recién nacidos en un contexto clínico \citep{BabyTalk}... Todos estos sistemas tienen en común la generación de un texto (normalmente de una alta calidad) a partir de muy diferentes fuentes de información.

En los ejemplos de proyectos listados con anterioridad que emplean la generación de lenguaje natural para redactar distintos textos, los datos utilizados como fuente de información son muy dispares, no solo en su contenido sino también en el tipo de dato. Así, si para la traducción de textos se utiliza texto ya existente como entrada, en otros sistemas como en la generación de informes meteorológicos se emplean datos no lingüísticos. De esta manera, se consideran dos posibles enfoques en los sistemas GLN dependiendo del tipo de entrada: texto a texto (\textit{text-to-text}) y dato a texto (\textit{data-to-text}).

\subsection{Generación \textit{text-to-text} (T2T)}
Los sistemas de generación texto a texto, conocidos como \textit{text-to-text} en inglés o T2T por sus siglas, toman textos escritos en lenguaje natural como entrada y producen un texto nuevo y coherente como salida. La entrada de estos sistemas puede abarcar desde pequeñas oraciones a extensos escritos. Existen muchas aplicaciones en los sistemas GLN que utilizan T2T. Además de los mencionados anteriormente, pertenecen a este tipo la fusión de documentos y generación de resúmenes \citep{clarke2010discourse}, simplificación de textos complejos \citep{sulem2018simple}, o autocorrectores gramaticales \citep{Ge2019AutomaticGE}, entre otros. 

Sin embargo, el ejemplo más claro de este tipo de generación de lenguaje corresponde a un traductor automático. Este tipo de sistema ampliamente utilizado en la vida cotidiana toma una entrada textual correspondiente a un escrito en un idioma y genera un texto de salida en otro idioma. La traducción automática es un proceso muy complejo, puesto que no solamente tiene en cuenta el significado del corpus, sino que también hace falta interpretar y analizar de manera correcta todos los elementos del texto, así como comprender la influencia de unas palabras en otras con la finalidad de  generar un texto fluido y coherente. 

\subsection{Generación \textit{data-to-text} (D2T)}\label{cap:nglD2T}
Estos tipos de sistemas permiten la generación de texto como salida a partir de entradas no textuales. Además, el formato de los datos que pueden tomar como entrada son muy diversos. Aunque es muy común encontrar sistemas que parten de datos numéricos como hojas de cálculo, hay que considerar otros orígenes de datos de tipo estructurado tales como bases de datos, simulaciones de sistemas físicos o grafos de conocimientos. De manera general, podemos referirnos a la representación de la información de esta clase de sistema como datos estructurados o procesables.

Algunos autores prefieren emplear  el término \textit{concepto} en lugar de \textit{data}, motivo por el que algunos se refieren a este enfoque como generación concept-to-text (C2T) \citep{vicente2015generacion}.

\begin{figure}[t]
	\centering
	%
	\begin{SubFloat}
		{\label{fig:inputFoG}%
			Entrada del sistema FoG}%
		\includegraphics[width=0.7\textwidth]%
		{Imagenes/Bitmap/02EstadoDeLaCuestion/inputFoG}%
	\end{SubFloat}
	\qquad
	\begin{SubFloat}
		{\label{fig:outputFoG}%
			Salida del sistema FoG}%
		\includegraphics[width=0.7\textwidth]%
		{Imagenes/Bitmap/02EstadoDeLaCuestion/outputFoG}%
	\end{SubFloat}
	\caption{Sistema \textit{data-to-text} FoG%
		\label{fig:FoG}}
\end{figure}

Uno de los ejemplos más visuales que nos permite comprender este tipo de sistema sería el \textit{Forecast Generator}, sistema que forma parte del \textit{Forecaster's Production Assistant}, entorno desarrollado por \textit{CoGenTex} en 1992 para \textit{Environment Canada} con el fin ayudar a los meteorólogos a aumentar su productividad al redactar por ellos un informe meteorológico textual en inglés y en francés \citep{goldberg1994using}. En la figura  \ref{fig:inputFoG} se muestra el entorno sobre el que los meteorólogos modifican valores como la presión atmosférica, situación de frentes y otros datos (datos no textuales). Una vez se pulsa sobre \textit{Generar}, el sistema muestra el texto correspondiente al informe (figura \ref{fig:outputFoG}). 


En la figura \ref{fig:data-to-text}, explicada con más detalle en \cite{sai2020survey}, se muestran los datos de entrada y de salida de un sistema GLN D2T acercándonos a la generación de lenguaje desde una perspectiva distinta al ejemplo explicado anteriormente.  Los datos de entrada de este tipo de sistema toman la forma de grafo o cualquier otro tipo de datos semiestructurados como tablas (conjunto de tuplas del tipo [entidad, atributo, valor]). En la fila inferior, se muestran diferentes posibles soluciones como salida del sistema. Además, el autor introduce la necesidad de métodos de evaluación de la calidad del texto redactado ya que de las diferentes salidas, solo la tercera opción cubre toda la información de entrada y resulta ser fluida.

\figura{Bitmap/02EstadoDeLaCuestion/data-to-text}{width=.9\textwidth}{fig:data-to-text}%
{Ejemplo de D2T utilizado por \cite{sai2020survey}}

\section{Arquitectura tradicional y herramientas de un sistema GLN}
\label{sec:arquitectura_tradicional}
El objetivo final de un sistema de generación de lenguaje natural es mapear unos datos de entrada a un texto de salida \citep{reiter1997building}. Sin embargo, este proceso, aunque pueda parecer sencillo de entender, resulta complicado de llevar a cabo. Al principio del desarrollo de sistemas GLN, no había un consenso entre autores a la hora de establecer un proceso para construir este sistema. Finalmente, \cite{reiter1997building} propusieron una arquitectura asociada a una lista de tareas recomendables que se deben realizar a la hora de llevar a cabo dicha construcción. Esta arquitectura surgió de la observación de los diferentes sistemas que se habían llevado a cabo hasta la fecha.  Actualmente, es la solución más extendida y reconocida.

La arquitectura presentada por \cite{reiter1997building}, como se puede observar en la figura\ref{fig:arquitectura}, se divide en tres módulos: macroplanificación, microplanificación y realización. Además, cada módulo contiene una lista de tareas. Esta asignación tareas-módulo no es inamovible. Una tarea asociada a un módulo se puede realizar en otro si así se considera, incluso implementar su desarrollo a lo largo de varios módulos. Los módulos que se corresponden con las tareas iniciales suelen estar relacionados con adaptar datos o estructura al sistema de generación, mientras que los módulos finales corresponden a la transformación de los resultados intermedios en el texto final.

\figura{Bitmap/02EstadoDeLaCuestion/arquitectura}{width=1\textwidth}{fig:arquitectura}%
{Arquitectura de referencia para sistema GLN \citep{vicente2015generacion}}

\subsection{Macroplanificación}
Este es el primer módulo de un sistema de generación de lenguaje. Debe determinar qué decir, seleccionando para ello la información de entrada necesaria y organizarla en una estructura coherente, resultando de este proceso el plan del documento. Las tareas que intervienen se describen en los apartados siguientes. 

\subsubsection{Selección del contenido}
La selección o determinación del contenido puede definirse como el proceso de decidir qué información debe ser incluida en el texto generado y cuál no. Por lo general, la información de la que partimos contendrá más información de la que nos interesa, así debemos decidir qué información resulta innecesaria y por tanto tenemos que eliminar para la generación del texto final. También hay que tener en cuenta el público al que está dirigido el texto generado, ya que dependiendo de este podremos incluir cierta información de los datos entrantes o no.

Este proceso de selección de la información lleva a cabo el filtrado y resumen de esta en un conjunto de \textit{mensajes}. Cada uno de estos mensajes corresponde al significado de una palabra u oración y se le asigna una entidad, concepto o relación dominante.

\subsubsection{Estructuración del documento}
Definiendo el concepto \textit{texto} como ``unidad de comunicación completa, formada habitualmente por una sucesión ordenada de enunciados que transmiten un mensaje con las siguientes propiedades: adecuación, coherencia y cohesión'', podemos advertir que un texto no es un conjunto aleatorio de oraciones, sino que es necesaria la existencia de un orden en la presentación del texto final.

Dependiendo de la información que se comunique, este orden puede verse modificado o alterado. Es por ello que no hay una estructura fija, sino que hay que adecuarla al tipo de documento.

Una vez realizada la estructuración del texto, se obtiene un plan de discurso que corresponde a una representación estructurada y ordenada de los mensajes obtenidos en la tarea anterior.

\subsection{Microplanificación}
La microplanificación es el segundo módulo de la arquitectura. Parte del plan del documento resultante del módulo anterior para generar las oraciones evitando información redundante e innecesaria en el discurso. El resultado de este módulo es el plan de discurso. El proceso de generación de oraciones lo realiza mediante tres tareas.

\subsubsection{Agregación de oraciones}
La generación de una oración por cada uno de los mensajes puede resultar en la generación de un texto redundante y excesivamente estructurado. Una tarea en el proceso de construcción de un sistema GLN es la agregación de oraciones que pretender paliar este problema mediante la unión o agregación de contenidos de distintos mensajes en una sola oración. De esta manera los mensajes se combinan para obtener oraciones más largas y complejas, resultando en conjunto un texto menos estructurado y más fluido.

\subsubsection{Lexicalización}
En esta fase del proceso se empieza a generar el texto en lenguaje natural como tal. Para ello se debe decidir que palabras o estructuras sintácticas expresan mejor los conceptos y relaciones de las etapas anteriores. La dificultad de la generación en esta etapa reside en la gran cantidad de alternativas que encontramos para  expresar cada uno de estos conceptos o bloques de mensajes. 

Además, hace falta tener en cuenta un todavía mayor número de posibilidades ya que se debe considerar numerosas variables que podrían afectar al resultado final de la generación. Algunas de las variables a tener en cuenta pueden ser las necesidades o conocimiento de los usuarios, si el objetivo de la generación es generar textos con variaciones sintácticas o semánticas a lo largo del mismo, si es preferible un texto repetitivo y simple o diverso mediante la utilización de palabras sinónimas y para terminar, una apropiada selección de adjetivos.


\subsubsection{Generación de expresiones de referencia}
La diferenciación de unas entidades de otras para poder generar expresiones que se refieran a ellas es tratada en esta tarea con el objetivo de evitar la ambigüedad. Para realizar esta tarea se debe conseguir encontrar características particulares que contribuyan a diferenciar a una entidad del resto de entidades. Esta etapa está bastante consensuada en el campo GLN. 

La generación de expresiones de referencia (REG, por sus siglas en inglés) debe llevarse a cabo una vez que el plan del documento se haya generado y depende de este, esto implica que esta fase debe llevarse a cabo desde el primer momento después de que se hayan analizado los datos. Debemos adaptar el plan de documento del primer módulo a lo que necesita REG, es por ello que debemos tener conocimiento de ello desde el comienzo.

Un caso especialmente estudiado que aplica esta técnica es la descripción de imágenes, ya que debe tener en cuenta si un elemento se encuentra a la derecha de otro, detrás de otro, etc, para poder enriquecer el texto. Para ello es necesario reconocer y distinguir los elementos en escena unos de otros y así, obtener una descripción lo más fidedigna posible a la imagen real.

\subsection{Realización}
La realización constituye el último módulo de la arquitectura de un sistema GLN. El objetivo final corresponde en generar oraciones gramaticalmente correctas para comunicar mensajes. En este módulo deberán tenerse en cuenta reglas a cerca de la formación de verbos (elección del tiempo verbal adecuado y por tanto generación de las palabras correspondientes), reglas sobre concordancia de género y número entre palabras (\cite{reiter1997building} no tienen en cuenta el género de las palabras ya que focaliza la generación del lenguaje al inglés), generación de pronombres...
 
La entrada sobre la que se trabaja es el plan de discurso que contiene información sobre las oraciones generadas y la estructura utilizada en el texto final. En esta fase se traduce esta entrada en la salida que el usuario final recibirá.

Algunos autores consideran una única tarea de realización que engloba el convertir las especificaciones en oraciones y el dar un formato final al texto. Otros prefieren separar estas etapas para diferenciarlas y que sea más sencillo su estudio.

\subsubsection{Realización lingüística}
Con el objetivo de transformar las especificaciones de oraciones en las oraciones finales, en esta fase se ordenan los diferentes elementos constitutivos de una oración y se les asigna un formato correcto. Para elegir la forma morfológica correcta de una palabra se debe conjugar verbos, establecer concordancias de palabras, añadir formas pronominales en los lugares adecuados de las oraciones y establecer los signos de puntuación adecuados. 

\subsubsection{Realización de la estructura}
Esta etapa no está considerada por algunos autores como tal aunque aquí se muestra ya que puede ser relevante en ciertos contextos. 
En algunos documentos, es necesario añadir o modificar algunas líneas del texto para darle estructura al documento. Un ejemplo muy sencillo de entender es la generación de texto que utilice html o Latex como formato de salida. En ambos casos, la adición de etiquetas a lo largo del texto generado resulta crucial para un texto de cualquiera de estas naturalezas.

\subsection{Herramientas en la arquitectura tradicional}
Los módulos constituyentes de la anterior arquitectura presentada pueden ser complicados de construir. Debido a la existencia  de profesionales que quieren desarrollar un sistema de generación de lenguaje completo, basándose en esta arquitectura genérica de sistema NLG, o investigadores que se encuentren estudiando sus propias implementaciones de alguno de estos módulos y deseen enfocarse plenamente en él, se crearon una serie de herramientas que pueden ayudarlos con el arduo trabajo de implementación de las tareas que componen la arquitectura. De esta manera, algunas de las herramientas presentadas a continuación pueden utilizarse para tomar inspiración del proceso de desarrollo, mientras que otras se construyen con el objetivo de relegar por completo este proceso en la propia utilidad. En este apartado se realizará un breve análisis de las herramientas consideradas más relevantes, asociadas a cada uno de los módulos descritos. Cabe decir, que debido a la antigüedad de esta arquitectura, la mayoría de las herramientas encontradas son aproximadamente de entre diez y veinte años atrás, sin embargo, algunas de ellas todavía hoy en día conservan gran fama y usuarios.


En primer lugar, para la macroplanificación de la arquitectura tradicional de un sistema NLG, dedicado a  la selección de la información de entrada necesaria para el sistema junto a su posterior organización, se encontraron diferentes herramientas de diversa índole. Algunas como SPUR se construyeron expresamente para la realización de estas únicas tareas de macroplanificación. Concretamente, esta herramienta se desarrolló como parte de un sistema mayor, conocido como MATCH \citep{johnston-etal-2002-match}, construido con el objetivo de realizar comparaciones entre restaurantes en Nueva York y obtener recomendaciones personalizadas. De esta manera, SPUR recibía como entrada los atributos de las opciones a comparar entre restaurantes y la salida correspondía al plan del documento constituido por los atributos más importantes de cada una de las opciones. Otras proyectos como PESCaDO \citep{pescado} o NaturalOWL \citep{Androutsopoulos_2014} además de tareas de planificación también incluyen componentes para el proceso de microplanificación e incluso realización .


En segundo lugar, para el módulo de microplanificación se desarrollaron herramientas como SPaRKy, utilizada también en el sistema MATCH anteriormente mencionado. En este caso, la entrada a la herramienta corresponde al plan de documento de la herramienta SPUR y su salida corresponde al plan de discurso con las afirmaciones a incluir en la salida del sistema. Otros proyectos mucho mas grandes como BabyTALK \citep{gatt_2009_babytalk} o NaturalOWL también incluyen entre sus funcionalidades herramientas para el desarrollo de la microplanificación.

Para acabar, son muchos los proyectos mencionado anteriormente que incluyen herramientas internas que realizan funciones de realización, sin embargo, destaca la existencia de la herramienta SimpleNLG, que no está ligada al empleo de un proyecto en concreto sino que puede utilizarse con cualquier tipo de datos.

SimpleNLG \citep{simplenlg_gatt} es una API de Java que proporciona interfaces que ofrecen un control directo sobre la tarea de realización. Define un conjunto de tipos léxicos, correspondientes a las principales categorías gramaticales, así como formas de combinarlos y establecer valores de características. Está orientada a la generación de oraciones gramaticalmente correctas en sistemas \textit{data-to-text}. Aunque originalmente solo estaba disponible para textos de lengua inglesa, actualmente se encuentra versionado para muchos idiomas, entre ellos el español. La versión española de esta herramienta es conocida como SimpleNLG-ES \citep{aramossoto2017adapting} y realmente se trata de una adaptación bilingüe de la versión original en inglés.

Esta herramienta se basa en la flexibilidad a la hora de generar textos mediante la utilización de manera combinada de sistemas basados en esquemas y otros enfoques más avanzados; la robustez generando salidas (aunque en ocasiones incorrectas) pese a que las entradas sean erróneas o incompletas; y por último, la independencia entre las operaciones de decisión de morfología y sintácticas.

Para definir una oración con SimpleNLG, se parte de sus constituyentes sintácticos básicos: sujeto, verbo y objeto. Una vez definido cada uno de ellos, por ejemplo estableciendo el sujeto como ``Manuel'', el verbo como ``bake'' y el objeto como ``a cake'', puede generarse una oración simple, siendo la salida resultante ``Manuel bakes a cake''. Cabe destacar la existencia de métodos que permiten establecer el tiempo verbal: pasado, presente o futuro, así como modificar el tipo de oración: enunciativa (por defecto), negativa o interrogativa. En el caso de este último tipo de oración, también permite seleccionar el tipo de pregunta formalizando el complemento circunstancial (quién, dónde, cómo...) o si debe formularse para que la respuesta sea sí o no. También permite establecer complementos a la oración y modificadores verbales. En la figura~\ref{fig:simplenlg} se puede apreciar a través del ejemplo presentado, el resultado de todas estas funcionalidades.


\figura{Bitmap/02EstadoDeLaCuestion/simplenlg}{width=1\textwidth}{fig:simplenlg}%
{Entrada y salida de la herramienta SimpleNLG}

\section{Algoritmos y modelos para la generación de lenguaje}
\label{sec:modelos}
Cualquier sistema informático dedicado al tratamiento del lenguaje natural necesita un modelo que le permita caracterizar y representar la lengua que trata. En esta sección se realiza una aproximación a los algoritmos y modelos utilizados a lo largo de la historia para la generación de lenguaje. Se comenzará estudiando a través del tiempo las distintas aproximaciones mayormente empleadas durante la época y se estudiará de una manera más profunda aquellos enfoques más relevantes en la actualidad como son los modelos estadísticas basados en cadenas de Markov o N-Grams, así como enfoques neuronales a través del estudio de diferentes tipos de redes. Para terminar, se presentarán algunas imprecisiones de generación que comenten las redes neuronales al ser aplicadas a la generación de lenguaje.


\subsection{Historia}
Antes de comenzar a describir los distintos tipos de modelos utilizados en tareas de generación de lenguaje natural, es interesante conocer como ha ido evolucionando este campo a lo largo de la historia y los modelos más relevantes en cada una de las etapas. En general, la historia del procesamiento de lenguaje natural se divide en dos grandes períodos marcados por la aparición del aprendizaje profundo o \textit{Deep Learning} \citep{louis-2021}. 

La era \textit{pre Deep Learning} (figura~\ref{fig:erapreDL}) comienza aproximadamente en el 1949, momento en el que   Warren Weaver sugería en su memorando ``Translation'' \footnote{ Lectura disponible en https://web.stanford.edu/class/linguist289/weaver001.pdf} que la traducción automática computacional era posible. Esta fue la primera aproximación estadística al procesamiento y generación de lenguaje. Supuso una revolución e inspiró numerosos experimentos y proyectos que probaron que realmente esto era posible aunque a muy pequeña escala. Estos sistemas se basaban principalmente en la búsqueda en diccionarios de las palabras necesarias para la traducción y la posterior reordenación de las palabras para ajustarse a las reglas sintácticas del idioma destino de la traducción.

Después de una década de investigaciones para conseguir mejores resultados en este campo y de pérdida de financiación, ya que las soluciones encontradas hasta ahora conseguían resultados muy pobres, Surgieron nuevas \textit{teorías de la gramática} mucho más manejables computacionalmente, y más tarde las \textit{ontologías conceptuales} que estructuraban la información del mundo real en datos comprensibles por la computadora.

En la década de 1980, surgieron los \textit{Modelos Simbólicos} basados en reglas. Estos sistemas asignaban manualmente los significados de las palabras y de esta manera determinista se creaban oraciones. Debido a la complejidad de creación de estas reglas, ya que se debían crear a mano, estos modelos fueron ampliamente sustituidos por los\textit{ Modelos Estadísticos} que supusieron una revolución para el procesamiento de lenguaje en aquella época y que hoy en día todavía tienen una gran relevancia en la lingüística computacional. 


\begin{figure}[t]
	\centering
	%
	\begin{SubFloat}
		{\label{fig:erapreDL}%
			Era pre \textit{Deep Learning}}%
		\includegraphics[width=0.9\textwidth]%
		{Imagenes/Bitmap/02EstadoDeLaCuestion/erapreDL}%
	\end{SubFloat}
	\qquad
	\begin{SubFloat}
		{\label{fig:erapostDL}%
			Era \textit{Deep Learning}}%
		\includegraphics[width=0.9\textwidth]%
		{Imagenes/Bitmap/02EstadoDeLaCuestion/erapostDL}%
	\end{SubFloat}
	\caption{Etapas del Procesamiento de Lenguaje Natural%
		\label{fig:etapasDL}}
\end{figure}

Con el avance computacional de las \textit{Redes Neuronales}, comienzan a usarse en la década del 2000 en el modelado del lenguaje para la generación de textos y dan lugar a la era \textit{Deep Learning} (figura~\ref{fig:erapostDL}). \cite{bengio_2000} propuso el primer modelo de lenguaje neuronal utilizando una Red Neuronal Prealimentada  \textit{(FeedForward Neural Network)} de una capa oculta. Otros autores sustituyeron progresivamente esta arquitectura de red por Redes Neuronales Recurrentes (\textit{Recurrent Neural Networks}) y Redes de Memoria a Corto Plazo (\textit{Long Short-Term Memory}) aunque los componentes básicos de la arquitectura original se encuentran todavía en la mayoría de modelos de lenguaje neuronales.

Más tarde \cite{collobertWeston} introdujeron el \textit{Aprendizaje Multitarea} al procesamiento de lenguaje, utilizando una Red Neuronal Convolucional (\textit{Convolutional Neural Network}) para conseguir que varias tareas de aprendizaje se resolvieran de manera simultánea, resultando en una mejora de la eficiencia. 

Tras varios avances, como la introducción de modelos \textit{Word Embeddings} o la adopción general de redes neuronales para el modelado de lenguaje, surge la arquitectura Secuencia a Secuencia (\textit{Seq2Seq}). Estos sistemas estaban compuesto por dos componentes clave: el codificador o \textit{encoder} y el decodificador o \textit{decoder}. La revolución que supuso esta arquitectura fue significativa y todavía se siguen utilizando.

En 2014, \cite{Bahdanau} introduce los \textit{mecanismos de atención} que alivia el problema de cuello de botella de los modelos predecesores, los \textit{Seq2Seq}.

La última innovación en el mundo del Procesamiento de Lenguaje son los grandes Modelos de Lenguaje Preentrenados (\textit{Pretrained Moldels}). Debido a todo el esfuerzo computacional de días, semanas e incluso meses que supone entrenar un modelo de lenguaje, se proponen una serie de modelos que ya tienen realizado este entrenamiento. La finalidad de estos sistemas es el ajuste o \textit{fine-tune} de los mismos de acuerdo al objetivo que se quiera conseguir.

\subsection{Modelos estadísticos}
Estos tipos de modelos utilizan técnicas estadísticas y reglas lingüísticas para aprender las distribuciones de probabilidad de las palabras pertenecientes a un conjunto finito conocido como \textit{vocabulario} y, de esta manera, generar secuencias de palabras específicas. Entre las técnicas más utilizadas y que mejores resultados han arrojado en este ámbito encontramos los modelos basados en cadenas de Markov y en N-gramas.

\subsubsection{Modelo Markov Chain}

Este modelo, introducido por el matemático ruso Andrey Markov en 1913, es un modelo estocástico discreto que describe una secuencia de posibles eventos. Aplicado a la generación de texto, podemos resumirlo en un sistema que se basa en una distribución de probabilidades aleatorias para generar la siguiente palabra a un grupo de palabras o secuencia. 

Para que un proceso se considere Markov debe satisfacer una condición conocida como la \textit{propiedad de Markov}. Esta propiedad establece que la probabilidad del siguiente evento (en el caso de generación de lenguaje, la siguiente palabra) depende únicamente del evento actual. Si se considera, como es el caso, que las palabras futuras no dependen en absoluto de las palabras previas, la secuencia de palabras generada evoluciona de manera no determinista y entonces puede considerarse que la palabra $X_n$ es una variable aleatoria. La colección de variables aleatorias es la definición de \textit{proceso estocástico}, y sirve como modelo para representar la evolución aleatoria de una frase a lo largo del tiempo \citep{MoyotlHernndez2016MtodoPA}.

La fórmula \ref{eqn:markov_property} representa los fundamentos de la \textit{ propiedad de Markov}, donde \textit{X} es una variable aleatoria (palabra) que toma un valor en el espacio de estado dado \textit{s} (vocabulario); y \textit{n} es una variable discreta que representa el paso de tiempo \citep{howell_2022}. Como se puede observar, suponiendo que nos encontramos en el paso de tiempo \textit{n}, las probabilidades teniendo en cuenta los estados de los eventos anteriores y la probabilidad teniendo en cuenta únicamente el estado actual para modelar la siguiente palabra son equivalentes, por lo que la información anterior al estado actual carece de relevancia para el cálculo de la probabilidad del siguiente evento.

\begin{equation}
\label{eqn:markov_property}
\begin{aligned}
P(X_{n+1} = s_{n+1}|X_{n} = s_{n}, X_{n-1} = s_{n-1}, ..., X_{0} = s_{0}) &= P(X_{n+1} = s_{n+1}|X_{n} = s_{n})
\end{aligned}
\end{equation}

Viendo un ejemplo concreto, en la figura~\ref{fig:ex_markov} se puede apreciar una cadena de Markov aprendida del corpus ``A man, a plan, a canal: Panama! A dog, a panic in a pagoda!''. Esta cadena está formada por un conjunto de estados, cada uno representa una palabra única del corpus, unidas por transiciones, que simbolizan la probabilidad de pasar de un estado a otro. 

\begin{figure}[!h]
	\centering
	%
	\includegraphics[width=0.8\textwidth]%
	{Imagenes/Bitmap/02EstadoDeLaCuestion/ex_markov}%
	\caption{Ejemplo de cadena de Markov \citep{make-school-no-date}%
		\label{fig:ex_markov}}
\end{figure}

Para generar una secuencia de palabras con esta cadena, se comienza eligiendo aleatoriamente un estado inicial, supongamos ``A'' como elección. A continuación, se deben tomar una de las transiciones posibles en base a una elección aleatoria. Esta elección será ``man,'' o ``dog,'' con probabilidad 1/2; digamos que tomamos ``dog,''. Ahora debemos escoger ``a'' ya que no tenemos más posibilidades y seleccionar la siguiente palabra en base a las probabilidades. Tenemos cuatro opciones con similar probabilidad igual 1/4: ``plan,'',``canal,'', ``pagoda!'' y ``panic''. Suponiendo que escogemos ``pagoda!'', hasta ahora se ha formado la frase ``A dog, a pagoda!''. Este proceso se puede realizar indefinidamente hasta que se decida parar o se llegue a un estado sin transiciones de salida.




Debido a las características particulares de la propiedad de Markov, este modelo es un modelo sin memoria, ya que se desprecian todos los estados anteriores, por lo que no se retiene información relevante de un texto como las posiciones de las palabras en una oración o la relación entre palabras. Sin embargo, precisamente por carecer de memoria, es simple de comprender y rápido de ejecutar \citep{fumagalli_2020}.




\subsubsection{Modelo N-gram}

Otro modelo estadístico de gran transcendencia es el modelo \textit{N-Gram}. Este modelo va un poco más allá del modelo \textit{Markov Chain}, ya que se basa en realizar una predicción estadística de una secuencia de palabras teniendo en cuenta un conjunto de palabras anteriores. Esta secuencia de N palabras es representada mediante un N-grama.
Así, este modelo trata de predecir el N-grama más probable dentro de cualquier secuencia de palabras dado el historial de las N-1 palabras anteriores. Para su mejor comprensión, se expone un ejemplo ejemplo concreto a partir del siguiente extracto de ``La vida es sueño'' de Calderón de la Barca:


\begin{verse}
	¿Qué es la vida? Un frenesí.\\
	¿Qué es la vida? Una ilusión,\\
	una sombra, una ficción,\\
	y el mayor bien es pequeño;\\
	que toda la vida es sueño,\\
	y los sueños, sueños son.\\
\end{verse}

Podemos construir N-gramas a partir del texto anterior teniendo en cuenta que un N-grama está formado por N palabras que aparecen consecutivas en el corpus. A continuación se muestran unigramas, bigramas y trigramas extraídos del texto.

\begin{verse}
	\textbf{Unigramas} \\
	\{ qué, es, la vida, un frenesí, una, ilusión, sombra, ficción, y, el, mayor,... \}\\
	\textbf{Bigramas} \\
	\{ qué es, es la, la vida, vida un, un frenesí, frenesí qué, una ilusión,... \}\\
	\textbf{Trigramas} \\
	\{ qué es la, es la vida, la vida un, vida un frenesí, un frenesí qué,...\}
\end{verse}


Este proceso se realiza de manera iterativa hasta llegar al final del corpus teniendo en cuenta que no se pueden repetir dos N-gramas iguales dentro de un mismo conjunto. Una vez construidos los N-gramas se puede calcular la probabilidad condicional mediante las siguientes fórmulas dependientes de la ocurrencia de un sub n-gram dentro del conjunto.

\begin{equation}
	\label{eq:probabilidadesngram}
	\begin{aligned}
		\textrm{\textbf{Unigrama}}\:\:\:\:
		P_{(W_i)} & = \frac{C_{(W_i)}}{N}   \\     
		\\
		\textrm{\textbf{Bigrama}}\:\:\:\:
		P_{(W_i|W_{i-1})} & = \frac{C(W_{i-1}W_i)}{C({W_{i-1}})}\\
		\\
		\textrm{\textbf{Trigrama}}\:\:\:\:
		P_{(W_i|W_{i-2}W_{i-1})} & = \frac{C(W_{i-2}W_{i-1}W_i)}{C({W_{i-2}W_{i-1}})}
	\end{aligned}
\end{equation}

De esta manera, para un modelo N-gram, se calcula la probabilidad condicional a partir de las n-1 palabras anteriores. Volviendo al ejemplo de ``La vida es sueño'', para conocer la probabilidad de que a la secuencia \textit{la vida} le siga la secuencia \textit{es}, calculamos la probabilidad condicional \textit{es|la vida}. Según las fórmulas anteriores, esta probabilidad es igual al número de ocurrencias de \textit{la vida es} dividido por el número de ocurrencias de la secuencia \textit{la vida}.

\begin{equation}
	\label{eq:probabilidadesngram_ejemplo}
	\begin{aligned}
		P_{(es|la\;vida)} & = \frac{C_{(la\;vida\;es)}}{C_{(la\;vida)}}  &= \frac{1}{3}
	\end{aligned}
\end{equation}

Para conocer la probabilidad de una secuencia completa deberíamos multiplicar las probabilidades de manera iterativa. Para el ejemplo anterior, la $P(la\:vida\:es)= P(la)\:x\:P(vida|la)\:x\:P(es|la\:vida)$ y generalizando la fórmula anterior  $P(w1, w2, w3)= P(w1)\:x\:P(w2|w1)x\:P(w3|w2)$.

La realización de este proceso de manera iterativa nos lleva a la generación de oraciones, párrafos o libros completos. Sin embargo, la dependencia de generación de la palabra siguiente a una secuencia dada con respecto al conjunto de palabras generadas inmediatamente anteriores puede llevar a generaciones erróneas que no tengan en cuenta otros contextos anteriores.

\subsection{Algoritmos de \textit{Deep Learning} par la generación de texto}
\subsubsection{Redes Neuronales Recurrentes (RNNs)}
\label{sec:rnn}
Las redes neuronales recurrentes o \textit{Recurrent Neural Networks (RRN)} son una clase especial de red neuronal profunda que nos permite analizar datos tratando la dimensión ``tiempo''.
Aunque este tipo de red aparece por primera vez en el 1982 introducida por \cite{firstrnn}, debido a los requisitos computacionales que necesitaban no se pudieron llevar a la práctica hasta muchos años más tarde; cuando llegaron los avances necesarios para su puesta en marcha. La principal área de aplicación de este tipo de algoritmo de \textit{deep learning} es la resolución de problemas que involucran datos secuenciales (y por tanto, temporales) como traducción automática, procesamiento de lenguaje natural, descripción de imágenes o reconocimiento de voz. 

Teóricamente, una red neuronal recurrente está formada por \textit{neuronas recurrentes}. Mientras que otros tipos de redes utilizan como función de activación de la neurona una función que actúa en una sola dirección, desde la primera capa de entrada hasta la última capa de salida; este tipo de redes también incluyen conexiones hacia atrás, proporcionando al sistema cierta memoria. En cada instante de tiempo llamado \textit{timestep}, cada neurona de la red recibe como entrada la salida de la capa anterior así como su propia salida del instante de tiempo anterior. Este procedimiento se puede expresar con la notación de la ecuación \ref{eq:activation_rnn}, donde $x = (x_1,...,x_T)$ representa la secuencia de entrada procedente de la anterior capa, $y$ la secuencia de salida de la capa actual, $W_x$ los pesos a aplicar a los datos de entrada procedentes de la salida la capa anterior, $W_y$ los pesos que se aplican sobre los datos procedentes de la salida de la propia capa obtenidos en el anterior instante de tiempo y \textit{b} un bias a partir del cual centrar los datos.

\begin{equation}
	\label{eq:activation_rnn}
	\Large
	y_{(t)} = f_{activation}(W_{x}X_{(t)} + W_{y}Y_{(t-1)}+b)
\end{equation}

Otra forma más intuitiva a través de la cual comprender este proceso que en realidad no dista demasiado del algoritmo de una red neuronal convencional, es desarrollando esta neuronal a través de los pasos de tiempo \textit{t}. En la figura ~\ref{fig:rnn_neuron} se puede comprobar el proceso de este algoritmo desde un punto de vista más esquemático. A la izquierda, se muestra la neurona recurrente sin desarrollar y a la derecha, la neurona desplegada en el tiempo \citep{neuron}.

\figura{Bitmap/02EstadoDeLaCuestion/rnn_neuron}{width=1\textwidth}{fig:rnn_neuron}%
{Neurona recurrente desplegada en el tiempo}

La parte de la neurona donde se preserva un estado a través del tiempo se denomina \textit{memory cell}. La finalidad de este componente es recordar información relevante sobre un estado anterior que recibieron para poder realizar predicciones más precisas.

Mediante la unión y configuración en capas de varias neuronas de este tipo, pueden llegar a construirse grandes redes neuronales de tipo recurrente. Estas redes no solo modifican, con respecto a una red neuronal convencional, el tipo de neuronas y conexiones entre ellas; sino también algoritmos internos que permiten su adecuado funcionamiento. En concreto, el procedimiento de \textit{Backpropagation} convencional se sustituye por una versión del mismo dependiente de la dimensión ``tiempo'', conocido como \textit{Backpropagration Through Time (BTTT)}. Este algoritmo mantiene la función tradicional del mismo, que no es otra que ir hacia atrás en la red con el fin de encontrar las derivadas parciales del error con respecto a los pesos de las neuronas. Estas derivadas son utilizadas en el \textit{descenso de gradiente} para ajustar los pesos dependiendo del comportamiento de \textit{Loss}. Sin embargo, debido a la inclusión del ``tiempo'' en este algoritmo, el coste computacional aumenta haciendo a este modelo mucho más lento.

El problema de este tipo de redes es conocido como \textit{Vanishing Gradients}. Como mencionamos anteriormente, de la aplicación del \textit{Backpropagration} se obtenían unas derivadas parciales del error, cada una de estas derivadas es un \textit{gradiente}. El problema de desvanecimiento de gradiente ocurre porque el gradiente se reduce a medida que se propaga hacia atrás a través del tiempo. Cuando los valores de un gradiente son extremadamente pequeños, estos valores no contribuyen al aprendizaje perdiendo peso en el resultado. 

\subsubsection{\textit{Long Short-Term Memory (LSTM)}}
Estas redes, propuestas por \citep{lstm} en el año 1997, surgieron como una evolución de las redes neuronales recurrentes. Su principal objetivo es ampliar la memoria para poder recordar no solo información reciente sino datos producidos mucho más tiempo atrás, ya que las redes neuronales recurrentes convencionales no eran capaces de recordar información que se había producido hacía varios \textit{timestep}; llevando a una memoria limitada para recordar secuencias de entrada más largas. Este problema es resultado del \textit{Vanishing Gradients} de los gradientes más lejanos en el tiempo.

\figura{Bitmap/02EstadoDeLaCuestion/lstm}{width=1\textwidth}{fig:lstm}%
{Neurona LSTM}

Para solventar esta limitación, las redes \textit{Long Short-Term Memory} proponen una variación de las neuronas. Estas neuronas poseían una memoria o \textit{memory cell} donde almacenaban la información relevante de estados anteriores dependiendo de los pesos calculados. En cada neurona LSTM existen tres puertas a esta celda: la puerta de entrada (\textit{input gate}), la puerta de olvidar (\textit{forget gate}) y la puerta de salida (\textit{output gate}). Estas puertas regulan el flujo de información dentro y fuera de la celda. Deciden si se permite una nueva entrada a la memoria, si se elimina la información o si se deja que afecte a la salida del instante de tiempo actual. Estas puertas podemos codificarlas mediante una función de activación sigmoide, lo que hace posible incluirlas en la \textit{Backpropagation} solucionando el problema de \textit{Vanishing Gradients}. Toda esta explicación está representada en la figura .


\subsubsection{Modelos Seq2Seq y mecanismos de atención}
El modelo Sequence-to-Sequence (Seq2Seq) caracterizado por la utilización de una arquitectura especial de Red Neuronal Recurrente (RNN), ha alcanzado un gran éxito a la hora de resolver problemas complejos de Procesamiento de Lenguaje Natural, incluso llegando a superar a los modelos estadísticos de lenguaje en su efectividad \citep{analytics_vidhya_2020}. Esto se debe a que aproximaciones estadísticas como los \textit{N-grams} no eran capaces de capturar dependencias de palabras de corpus de gran tamaño, se necesitaría demasiado espacio y memoria RAM para poder guardar las probabilidades de todas posibles combinaciones de N-gramas. Sin embargo, las redes neuronales recurrentes que implementa este modelo no están limitadas a observar únicamente las palabras previas a una secuencia, sino que permiten propagar información desde el comienzo de una oración hasta el final consiguiendo mejores predicciones.

\paragraph{Arquitectura Encoder-Decoder}\hfill


Considerando los diferentes tipos de redes neuronales recurrentes descritas en apartados anteriores, se da paso a la explicación propia del modelo Seq2Seq. Desde un punto de vista muy general, podríamos representar este modelo como un sistema que toma una secuencia de elementos como entrada (input) y genera otra secuencia de elementos de salida (output). Como se muestra en la figura \ref{fig:basicseq2seq}, la arquitectura de este sistema sigue una arquitectura \textit{Encoder-Decoder}, compuesta internamente por dichos componentes, un \textit{encoder} y un \textit{decoder} que implementan redes neuronales recurrentes, concretamente LSTM o en menor número de casos GRU (\textit{Gated Recurrent Units}).


\figura{Bitmap/02EstadoDeLaCuestion/basicseq2seq}{width=1\textwidth}{fig:basicseq2seq}%
{Arquitectura de un sistema Seq2Seq}

La tarea del \textit{encoder} consiste en resumir la información de la secuencia que se introdujo como entrada en forma de un vector de estado oculto o \textit{context} y enviar los datos resultantes al \textit{decoder}. El objetivo principal de este vector es encapsular la información de todos los elementos de entrada para ayudar al \textit{decoder} a realizar predicciones precisas. Para calcular el estado oculto t-ésimo de la secuencia se utiliza la fórmula representada en la ecuación \ref{eq:encoder}, donde  $x_t$ corresponde a la secuencia de entrada en el instante de tiempo \textit{t} y \textit{W} representa la matriz de pesos a aplicar sobre los datos de entrada $W^{hx}$ y sobre la salida de la celda del instante anterior $W^{hh}$. Para cada una de las celdas del \textit{encoder} se calcula su vector de estado oculto, generando la última celda (en el instante de tiempo \textit{t}) el vector de estados finales.

\begin{equation}
	\label{eq:encoder}
	\Large
	h_t = f(W^{(hx)}x_t+W^{(hh)}h_{t-1})
\end{equation}


Por su parte, el \textit{decoder} utiliza como estado inicial la salida del \textit{encoder} correspondiente al vector de estados finales, calculando cada celda su estado oculto con la fórmula \ref{eq:decoder}. Una vez que se obtiene el estado oculto $h_{t}$, puede generarse la secuencia de palabras final aplicando al dataset de palabras junto con $h_{t}$ la función \textit{softmax}.

\begin{equation}
	\label{eq:decoder}
	\Large
	h_t = f(W^{(hh)}h_{t-1})
\end{equation}

Aunque esta aproximación parece solucionar muchos de los problemas de modelos anteriores, añade o mantiene limitaciones. Una de ellas es el cuello de botella que se genera en el último estado oculto del codificador ya que toda la información de la entrada debe atravesar el \textit{encoder} hasta este último punto para poder pasarle toda la información junta al \textit{decoder}. Además, ya que se intenta mapear una secuencia de longitud variable en una memoria de longitud fija y en el caso de textos largos podría perderse parte de la información. 


\paragraph{Mecanismos de atención}\hfill

Ante los problemas mencionados anteriormente propios de los modelos secuencia a secuencia, se plantea la utilización de mecanismos de atención que permiten que el \textit{decoder} no tenga que recibir toda la información del \textit{encoder}, sino que se fija en aquellas palabras más importante que producen los estados ocultos de codificador en cada uno de sus pasos. Estos mecanismos de atención fueron introducidos inicialmente por \cite{Bahdanau} para la traducción automática aunque posteriormente se ha aplicado a una multitud de áreas. 


Para conseguir estos beneficios del mecanismo de atención, se modifica ligeramente la arquitectura del sistema añadiendo una capa intermedia entre el codificador y decodificador que recibe los estados ocultos que se van generando en el \textit{encoder}. Sin embargo, no se espera a que todos los estados ocultos estén calculados sino solo los más importantes a los que se les establece un mayor peso. Para que el almacenamiento de los estados ocultos no sea ineficiente, los estados ocultos recibidos se combinan en un vector llamado \textit{vector de contexto} que contendrá más o menos información de las palabras dependiendo de su peso (figura ~\ref{fig:basicseq2seq_attention2}). Estos pesos se calculan comparando el último estado oculto del \textit{decoder} con cada uno de los estados del codificador determinando así las palabras más importantes.


\figura{Bitmap/02EstadoDeLaCuestion/basicseq2seq_attention2}{width=1\textwidth}{fig:basicseq2seq_attention2}%
{Arquitectura de un sistema Seq2Seq con mecanismo de atención}


%\subsubsection{Gated Recurrent Unit (GRU)}
%\subsubsection{BiDirectional RNNs (BRNNs)}

\subsubsection{Modelos preentrenados: Transformers}
Los modelos preentrenados surgen como una evolución de los modelos Seq2Seq. Es por esto que aún podemos encontrar características de estos últimos sistemas
en la arquitectura externa de este tipo de modelo como en la existencia de algunos de los dos componentes clave: el \textit{encoder} y el \textit{decoder}. 

La necesidad de este tipo de modelo en el modelado de lenguaje viene del requerimiento de grandes bases de datos para entrenar modelos de redes recurrentes junto con su consecuente enorme tiempo para entrenar todo un corpus en el modelo. Además, los requisitos computacionales precisados en para el entrenamiento de estas colecciones de datos haría imposible esta tarea por parte de pequeños investigadores que no dispusieran de estos medios.

Los modelos preentrenados se basan en crear un modelo previamente capacitado para la realización de ciertas tareas generales de procesamiento de texto y que permiten ser ajustados o \textit{finetune} sobre bases de datos de pequeña escala. De esta manera, no se debe construir un modelo desde cero para resolver problemas de procesamiento de lenguaje.

Dentro de los modelos preentrenados, la arquitectura Tranformers está en el centro de casi todos lo desarrollos recientes más importantes en el campo de procesamiento de lenguaje natural. El rendimiento de esta arquitectura es mejor que el de las redes neuronales recurrentes y redes neuronales convolucionales (estás últimas serán explicadas posteriormente en al apartado~\ref{sec:cnn}).

Es posible acceder a la arquitectura de los Transformers a través de la herramienta de Python de nombre homónimo.
Esta herramienta proporciona una serie de sistemas de propósito general para Natural Language Understading (NLU) y Natural Language Generation (NLG). Ofrece más de 32 modelos preentrenados en más de 100 idiomas, entre los que se encuentra el español. Entre los modelos más utilizados encontramos los famosos GPT-2, BERT y T5 junto con un gran número de variaciones de ellos dependiendo de los datos utilizados para su preentrenamiento.

%TODO - HABLAR DE MASKED Y CASUAL GENERATION.

\paragraph{GPT-2}\hfill

GPT-2 (\textit{Generative Pretrained Transformer}) es un modelo de generación de lenguaje natural presentado por OpenAI en el año 2019 basado en redes neuronales para secuencias, basadas en la autoatención%TODO - REVISAR LO DE MASKED PORQUE CREO QUE ES CASUAL
 enmascarada(\textit{masked self-attention}), y que ha sido construido sobre una arquitectura Transformer. El objetivo de este sistema es construir una distribución de probabilidad en la que para cada palabra posible a generar se le asigna una probabilidad en función del contexto anterior. Se trata de un modelo que ha sido preentrenado con un conjunto de datos correspondiente a las 8 millones de páginas web mejor valoradas en Reddit, lo que resulta en una gran base de conocimiento para generar textos automáticamente de manera muy correcta.

La potencia de este modelo es tal que sus creadores no quisieron en un primer momento publicar la versión completa por miedo de que se pudiera utilizar de manera ilícita. Según fueron pasando los años, se fueron liberando progresivamente diferentes versiones del modelo original ya que comenzaban a surgir otros proyectos con potencias igualmente competitivas. Estas diferentes versiones se diferenciaban en el número de parámetros que admitía la arquitectura y de esta manera se conseguía limitar su funcionamiento. La primera versión contaba con 117 miles de millones de parámetros mientras que la última versión, publicada en 2020, posee 1,5 billones.

GPT-2 únicamente está disponible en inglés aunque puede hacer uso de GoogleTranslate API para generar textos en otros idiomas. Es importante resaltar que al depender del traductor se puede ver disminuida la calidad de generación de lenguaje.


\paragraph{BERT}\hfill

BERT (Bidirectional Encoder Representations from Transformers) es un modelo NLP desarrollado por Google y publicado a finales de 2018 \citep{Devlin2019BERTPO}. Está basado redes neuronales bidireccionales que tratan de predecir las palabras perdidas (enmascaradas) en una oración y determinar si dos oraciones consecutivas son continuación lógica entre sí para determinar si están conectadas por su significado. Aunque originalmente no estaba destinado a la generación de textos, \cite{wang-cho-2019-bert} publicaron un método de utilización de este sistema para conseguir la generación de lenguaje que parece dar muy buenos resultados. De hecho, consiguió mejorar los resultados de la versión publicada en aquellos tiempos por GPT-2. 

De este modelo han surgido numerosas variaciones que se han publicado a lo largo de estos años. Como se puede apreciar en la figura \ref{fig:modelos_bert}, encontramos distintas ramificaciones que podemos dividir en dos grupos: modelos preentrenados con un corpus específico perteneciente a un dominio y modelos \textit{fine-tuned} que se ajustan a una tarea específica utilizando un modelo previamente entrenado \citep{rajasekharan_2019}.
Otras variaciones de BERT corresponden a los modelos construidos a partir de él pero entrenados en otros lenguajes para generar textos en otra lengua distinta al inglés, que es la original. Beto es la versión en Español de BERT \citep{CaneteCFP2020} y ha sido entrenado con una gran corpus en dicho idioma.

\figura{Bitmap/02EstadoDeLaCuestion/modelos_bert}{width=0.9\textwidth}{fig:modelos_bert}%
{Modelos surgidos a partir de BERT}





\subsection{Otras técnicas de \textit{Deep Learning} aplicadas al modelado de lenguaje}

En este apartado se van a describir algoritmos de redes neuronales que no fueron utilizados en un inicio a la generación de texto pero que actualmente se han estado aplicando al campo con la finalidad de obtener mejores resultados y estudiar como afectarían las ventajas de este tipo de redes a la generación de lenguaje. Entre todas las técnicas que se han encontrado, se han seleccionado las dos más relevantes: Redes Neuronales Convolucionales y Redes Neuronales Generativas Adversarias.

\subsubsection{Redes Neuronales Convolucionales (CNNs)}
\label{sec:cnn}
Las redes neuronales convolucionales o \textit{Convolutional Neural Neetworks} son uno de los algoritmos de mayor éxito en la visión artificial. Este tipo de redes se ha empleado en diversas áreas de inteligencia artificial con imágenes incluyendo la clasificación de fotografías o la detección y reconocimiento de objetos en imágenes. Debido al gran éxito que ha tenido este tipo de algoritmo en la visión artificial, se ha exportado a otros subcampos de la inteligencia artificial como es la generación de lenguaje natural, logrando resultados muy interesantes. 


Una red neuronal convolucional es un tipo de red neuronal profunda caracterizada por la utilización de \textit{convoluciones} en al menos una de sus capas ocultas. Este tipo de arquitectura red fue introducida por primera vez en 1990 por \citep{LeCun1989HandwrittenDR}, quien adaptó la operación de convolución para las redes neuronales con el objetivo de reconocer dígitos escritos a mano. Más tarde, modificó su sistema para aplicarlo a cualquier tipo de imagen y no solo reconocimiento de números \citep{cnn_lecun}.

Las redes convolucionales reciben, al igual que las redes neuronales convencionales, una entrada. En este tipo concreto de red, la entrada generalmente se tratará de una imagen o bajo el punto de vista del computador, una matriz de píxeles. Así, la altura y ancho de la matriz de entrada dependerá de la resolución de la imagen y la profundidad, de si se trata de una imagen en blanco y negro (con profundidad igual a uno) o si por el contrario, está compuesta por colores RGB, en cuyo caso la profundidad de la matriz será igual a tres.

Una vez preparados los datos de entrada, son procesados a lo largo de todas las capas que componen la red neuronal convolucional, que en este tipo de red corresponde a las conocidas como \textit{capas de convolución}. La función de esta capa es extraer las características de los datos presentados como entrada dependiendo del filtro seleccionado, comprimiéndolas con el objetivo de reducir su tamaño inicial. 

Una vez presentada la arquitectura general de este tipo de red, resulta interesante estudiar el funcionamiento de capa una de estas capas de convolución que la componen. Para el ejemplo de la figura ~\ref{fig:cnn_conv1} y~\ref{fig:cnn_conv2} , se supone como datos de entrada una imagen de dimensiones 28x28x1.  

Par realizar la primera convolución (primera capa de la red), se van tomando pequeños grupos de píxeles cercanos y se aplican operaciones matemáticas de baja computación (producto escalar) contra una pequeña matriz denominada \textit{kernel}. El resultado obtenido después de este proceso es una matriz denominada \textit{convolución del kernel}, matriz a la que posteriormente se le aplicará la función de activación de neuronas \textit{ReLu (Rectifier Linear Unit)} para obtener el mapa de detección de características o {feature map}. Sin embargo, no solo un único \textit{kernel} será aplicado a la matriz de entrada de la capa convolucional de la red, sino un conjunto de ellos. Este conjunto de \textit{kernels} que operan sobre la matriz de entrada recibe el nombre de \textit{filtro}.

Por cada uno de los \textit{kernels} aplicados a la matriz de entrada obtendremos un mapa de características diferente lo que se traduce en un incremento enorme en el número de neuronas utilizadas y que posteriormente deban ser procesadas por otras capas, teniendo que aumentar considerablemente el esfuerzo computacional requerido. Para solucionar este problema, se aplica al conjunto de mapas de características un método de muestreo conocido como \textit{Max-Pooling} que comprime el conjunto de matrices preservando las características más importantes detectadas. Este proceso se puede comprender de manera más gráfica en la figura~\ref{fig:cnn_conv1}.

\begin{figure}[!h]
	\centering
	%
	\includegraphics[width=0.8\textwidth]%
	{Imagenes/Bitmap/02EstadoDeLaCuestion/cnn_conv1}%
	\caption{Primera Convolución de Red Neuronal Convolucional%
		\label{fig:cnn_conv1}}
\end{figure}

La segunda convolución (así como el resto de convoluciones), tomaría como entrada una matriz con toda la información de salida de la anterior convolución y realizaría el mismo proceso descrito anteriormente (figura~\ref{fig:cnn_conv2}). Cabe decir, que a lo largo de una red convolucional podemos tener varias capa de convolución en la que cada una aplica un filtro diferente. Existen una multitud de filtros que se podrían aplicar a la matriz de entrada. Entre los filtros destacan aquellos que permiten detectar los bordes de los objetos en una imagen (filtro derivador) o formas geométricas. Otro filtros conocidos son filtros que permiten reducir el ruido de una fotografía. 

\begin{figure}[!h]
	\centering
	%
	\includegraphics[width=0.8\textwidth]%
	{Imagenes/Bitmap/02EstadoDeLaCuestion/cnn_conv2}%
	\caption{Segunda Convolución de Red Neuronal Convolucional%
		\label{fig:cnn_conv2}}
\end{figure}

Para finalizar, la salida de la última capa de convolución correspondiente a una matriz (potencialmente de profundidad igual a 3), se aplana para convertirla en una capa de neuronas tradicionales y se conecta a una capa oculta de neuronas de tipo \textit{feedforward}. Posteriormente, se le aplica la función \textit{softmax} a esta red neuronal tradicional, resultando en la capa de salida del sistema. Esta arquitectura completa se puede visualizar en la figura~\ref{fig:cnn_arq}.


\begin{figure}[t]
	\centering
	%
	\includegraphics[width=0.8\textwidth]%
	{Imagenes/Bitmap/02EstadoDeLaCuestion/cnn_arq}%
	\caption{Arquitectura de una CNN%
		\label{fig:cnn_arq}}
\end{figure}

En la aplicación de este tipo de red al procesamiento de lenguaje natural podríamos distinguir dos vertientes: sistemas \textit{text-to-text} en los que, a diferencia de los problemas de visión por ordenador en los que se utiliza como entrada la matriz de píxeles mencionada, las matrices estarían compuestas de palabras, oraciones o caracteres dependiendo del problema a resolver. Así, cada fila de la matriz representará a una palabra u oración. Estos sistemas suelen estar destinados a la clasificación de textos u oraciones  \citep{jacovi2018understanding,Lai2015RecurrentCN,Kim2014ConvolutionalNN}.

Otro campo de la generación de lenguaje en el que está incluyendo este tipo de arquitectura es la generación de descripciones de imágenes \citep{He2017DeepLF} que genera un texto en lenguaje natural representativo de la información contenida en una imagen dada como entrada. Estos últimos sistemas suelen estar compuestos por dos componentes, una red neuronal convolucional que extrae las características representativas de la imagen y una red neuronal recurrente vista en el apartado \ref{sec:rnn}, que transforma estas características a un texto en lenguaje natural. La entrada de este tipo de sistema estaría formada por una matriz de píxeles y la salida sería el texto descriptivo de la imagen.

\subsubsection{Redes Neuronales Generativas Adversarias (GANs)}
Las redes neuronales generativas adversarias o antagónicas son un sistema de aprendizaje no supervisado en el que dos inteligencias compiten entre sí para lograr un objetivo. Este tipo de red se ha convertido en un gran éxito en el mundo de la visión artificial, especialmente conocidas por la generación de imágenes hiperrealistas como el sistema \textit{StyleGAN}, que hace uso de este tipo de red para generar imágenes de alta resolución de rostros de personas \citep{Karras2019ASG}. Más reciente es su aplicación al mundo de la generación de lenguaje natural, empleándose especialmente en la generación de texto basada en estilos.

Las GANs internamente están formadas por dos componentes: el \textit{generador} cuya tarea es generar ejemplos ya sea creándolos él mismo (ejemplos falsos) o extrayéndolos de un conjunto de datos (ejemplos reales); y el \textit{discriminador} que clasifica los ejemplos recibidos en falsos o reales según se mencionó anteriormente.

Así, el objetivo del generador es producir ejemplo cercanos a los ejemplos reales de manera que pueda engañar al discriminador. Por otra parte, el discriminador debe clasificar de manera correcta estos datos producidos por el generador en los dos tipos de datos. Para que el generador mejore, produciendo cada vez ejemplos más cercanos a las soluciones reales, recibe continuamente retroalimentación desde el discriminador sobre qué tan bien las muestras generadas lograron engañarle.


Todo este procedimiento se basa en teoría de juegos, en el que la función objetivo es la función \textit{minimax}. Esta función es mostrada en la ecuación~\ref{eq:gan_minimax}, donde $D(x)$ representa la probabilidad de que $x$ sea real según el discriminador y $G(z)$ es una muestra producida por el generador. Intuitivamente se deduce que el discriminador trata de maximizar la función objetivo (que los datos reales se identifiquen como reales y los falsos como falsos), mientras que el generador trata de minimizarla (engañar al discriminador).

\begin{equation}
	\label{eq:gan_minimax}
	\underset{G}{\min}
	\underset{D}{\max}
	V(D,G) = E_{x~P_{data}(x)}[logD(x)] + E_{z~P_{z}(z)}[log(1-D(G(z)))]
\end{equation}


La utilización de este tipo de red para el modelado de lenguaje es un desafío debido a la naturaleza no derivable de los símbolos discretos empleados en este tipo de generación, mientras que en la visión artificial se emplean datos continuos como las imágenes. Algunos sistemas que tratan de resolver este obstáculo se basan en técnicas como la \textit{Maximun Likelihood Estimation} (MLE) \citep{li-etal-2017-adversarial}, el algoritmo \textit{Reinforce} o \textit{Policity Gradient} \citep{Yu2017SeqGANSG}, estos dos últimos basados en aprendizaje por refuerzo \citep{li-etal-2018-paraphrase, Shi2018TowardDT}.


\subsection{Efectos de las aplicación de redes neuronales a la generación de texto}

Con el avance de los modelos de generación de lenguaje natural, se ha empezado a prestar más atención a las limitaciones y riesgos potenciales de este tipo de sistemas. Los sistemas más modernos y en los que los investigadores fijan principalmente su atención son modelos de \textit{Deep Learning} basados esencialmente en redes neuronales profundas que han sido capaz de mejorar drásticamente la calidad de generación de lenguaje respecto a otros sistemas anteriores. Sin embargo, junto con estas mejoras, debido a las características intrínsecas de estos modelos computacionales, estos modelos son más propensos a fenómenos que conllevan una generación errónea de textos. Por una parte la llamada \textit{degeneración} produce salidas incoherentes o atascada en bucles repetitivos de palabras o expresiones. Otros modelos GLN en algunas ocasiones generan textos de salida sin sentido alguno o con datos para nada respaldados en la información introducida como entrada. Este fenómeno es conocido como \textit{alucinación} y perjudica seriamente la aplicabilidad de los modelos neuronales de generación de lenguaje en casos prácticos donde la precisión de la información es vital y el nivel de tolerancia hacia las alucinaciones es nulo.

\subsubsection{Alucinaciones}

Con \textit{alucinación} nos referimos al fenómeno en el que un modelo, especialmente de tipo neuronal ``\textit{end-to-end}'', produce información de salida que no es fiel a los datos provistos como entrada al sistema. 

%Ejemplos: Caso de imagenes, caso de texto
Este fenómeno se da en una diversidad de sistemas condicionales de generación de lenguaje. \cite{hallucinations_data2text} en su artículo, \textit{Controlling Hallucinations at Word Level in Data-to-Text Generation} destaca la existencia de alucinaciones en la generación \textit{data-to-text} en un modelo neuronal entrenado a partir de bases de datos como \textit{Totto} \citep{parikh-etal-2020-totto}. La entrada al sistema es una tabla. Una vez generado el texto de salida se puede comprobar que la palabra ``Italian'' a la que denomina \textit{enunciado divergente} no es respaldada por los datos de entrada (figura~\ref{fig:d2thallucinations}).

Por otro lado, \cite{rohrbach-etal-2018-object} subraya la existencia de alucinaciones en la generación de descripciones de imágenes. Estos tipos de sistemas se componen de dos modelos diferenciados. Por una parte, un modelo de predicción de imagen que trata de extraer los objetos de la misma y por otra, un modelo de predicción de lenguaje basado en la probabilidad de la siguiente palabra a generar. De esta forma, se analizaron las diferencias de predicción entre ambos modelos (figura~\ref{fig:imagehallucinations}) y llegaron a la conclusión de que en la mayoría de los casos la descripción generada se basaba principalmente en el modelo de lenguaje con el objetivo de conseguir una descripción más consistente semántica y sintácticamente. En el caso de estudio, la imagen sirve de entrada al sistema y se comprueba la predicción de ambos modelos nombrados anteriormente para la última palabra a generar. Mientras que el modelo de imagen predice palabras como ``bol'', ``brocoli'' o ``zanahoria'', el modelo de lenguaje propone ``tenedor'', ``cuchara'' o ``bol''. Finalmente, la descripción generada utiliza ``tenedor'' para completar la frase aunque no aparece en la imagen produciéndose una alucinación.


\begin{figure}[t]
	\centering
	%
	\begin{SubFloat}
		{\label{fig:d2thallucinations}%
			Alucinaciones en generación DT2}%
		\includegraphics[width=0.7\textwidth]%
		{Imagenes/Bitmap/04Analisis/alucinacion_d2t}%
	\end{SubFloat}
	\qquad
	\begin{SubFloat}
		{\label{fig:imagehallucinations}%
			Alucinaciones en generación de descripciones de imágenes}%
		\includegraphics[width=0.8\textwidth]%
		{Imagenes/Bitmap/04Analisis/alucinacion_image}%
	\end{SubFloat}
	\caption{Alucinaciones en distintos sistemas%
		\label{fig:hallucinations}}
\end{figure}


%Caso especial medico -> mucha precision y privacidad

%Tipos de alucinaciones: intrinsecas| extrinsecas

Aunque nos referimos a las alucinaciones de manera general como datos generados erróneamente. Atendiendo al resultado de la generación y por tanto a las consecuencias que puede tener esta generación, \citep{hallucination_survey} distingue dos tipos de alucinaciones.

Con \textit{alucinaciones intrínsecas} (figura~\ref{fig:inthallucinations})se refiere a la generación de textos de salida que contradicen los datos de entrada. Mientras que las \textit{alucinaciones extrínsecas} (figura~\ref{fig:exthallucinations}) son aquellas que generan una salida que no puede ser verificada a partir de los datos de entrada. Ambos tipos de alucinaciones generan datos no respaldados por la información que constituye los datos de entrada. Sin embargo, este último tipo de alucinaciones no siempre genera una salida errónea ya que no se puede asegurar que los datos generados sean incorrectos.


\begin{figure}[t]
	\centering
	%
	\begin{SubFloat}
		{\label{fig:inthallucinations}%
			Alucinación intrínseca}%
		\includegraphics[width=0.7\textwidth]%
		{Imagenes/Bitmap/04Analisis/alucinacion_intrinseca}%
	\end{SubFloat}
	\qquad
	\begin{SubFloat}
		{\label{fig:exthallucinations}%
			Alucinación extrínseca}%
		\includegraphics[width=0.7\textwidth]%
		{Imagenes/Bitmap/04Analisis/alucinacion_extrinseca}%
	\end{SubFloat}
	\caption{Tipos de alucinaciones%
		\label{fig:hallucinations_types}}
\end{figure}

%Tipos de alucinaciones: A partir de datos por los datos de entrenamiento e inferencia

Si nos preguntamos el porqué de la existencia de las alucinaciones cuando se introducen unos datos de entrada a un sistema entrenado, potencialmente bajo un modelo de red neuronal, de manera satisfactoria y haciendo uso de una base de datos que nos permite realizar la tarea que tenemos como objetivo; tenemos que tener en cuenta las posibles causas origen que generan este problema.

La generación errónea de los datos de salida, según \cite{hallucination_survey}, puede deberse a una divergencia en los datos utilizados para entrenar el modelo. Esta divergencia aparece cuando la relación entre los datos fuente-referencia está mal construida. Aunque el modelo base funcione correctamente, el modelo que ha sido entrenado bajo esta base de datos con divergencias puede alentar a generar una salida que no es fiel a los datos proporcionados como entrada. Otro escenario problemático emerge con la existencia de ejemplos de datos del conjunto duplicados y que han filtrados de una manera incorrecta. Cada vez más, los corpus de texto se incrementan en tamaño con el paso del tiempo y debido a la imposibilidad de revisión humana de todos estos grandes conjuntos de datos, pierden calidad respecto a los corpus más pequeños. \cite{lee2021deduplicating} afirma que el 10\% de los ejemplos de las bases de datos más empleadas en generación de lenguaje natural están repetidas en numerosas ocasiones. También destaca, que cuando estos ejemplos de datos duplicados pertenecientes a un conjunto se utilizan para entrenar un sistema, sesga el modelo para favorecer la generación de estas frases duplicadas. Si además también participaran de divergencias en entre la fuente y la referencia de los datos, la existencia de alucinaciones se multiplicaría.


Otras de las razones de la existencia de las alucinaciones, corresponde a las características propias del modelo de red neuronal. Aún partiendo de una base de datos perfecta, sin duplicados ni divergencias algunas, las opciones de entrenamiento y modelado de estos sistemas influirían generando textos de salida incorrectos. Por una parte, la incapacidad de comprensión del modelo de los datos de entrada debido a la generación de correlaciones incorrectas entre las diferentes partes de los datos de entrenamiento por parte del codificador, puede conllevar a un mal aprendizaje por parte del modelo. Así mismo, la estrategia de decodificación utilizada, correspondiendo en estos casos la elección a estrategias que añaden aleatoriedad o diversidad en la generación, están relacionadas directamente con el incremento de las alucinaciones.

\subsubsection{Degeneración}

Algunos modelos de redes neuronales conocidos, como GPT-2, se basan en la aleatoriedad de la salida como su objetivo principal frente a la maximización de la probabilidad. Esto se debe a que buscan la mayor similitud en la generación entre el procesamiento textual de la información por parte de un sistema y un humano. Para conseguir esta diversidad en la salida de la generación, se hace uso de estrategias de decodificación aleatorias, ya que las estrategias que buscan la maximización de la probabilidad para obtener mayores puntuaciones de similitud, especialmente en el caso de los textos largos, con frecuencia abocan a textos con repetitivos e incoherentes. Un ejemplo de estrategia de decodificación que alienta a degeneraciones es \textit{Beam Search}. En la figura~\ref{fig:degeneracion}, se muestra el texto de salida generado por GPT-2 que utiliza esta estrategia de decodificación. Destaca en color azul las repeticiones producidas en la salida, claro ejemplo de degeneración.

\figura{Bitmap/04Analisis/degeneracion}{width=.7\textwidth}{fig:degeneracion}%
{Ejemplo de degeneración con Beam Search }

\subsubsection{Falta de representación de los datos de entrada}

Esta limitación podría considerarse justo la contraria a las alucinaciones. Si en esta última se generaba mayor información de la proporcionada en los datos de entrada, también se da el caso de falta de representación de alguno o todos los datos de entrada. Se trata de un problema igual de grave que si se generaran datos erróneos (como las alucinaciones intrínsecas) en las que en el caso de una resolución médica, una mala interpretación de los datos de entrada podría llegar a poner en peligro la vida de una persona. En este caso, dependiendo del grado en que no aparezcan representados en la salida los datos introducidos como entrada, se tendría un impacto de diferente gravedad. Si unos datos muy importantes (en el ejemplo del caso médico) no se tuvieran en cuenta, podría llegarse también a un diagnóstico potencialmente diferente al que se generaría si se hubiera incluido este dato. Así mismo si el número de datos no introducidos fuera elevado. En el caso de no aparición en la salida de datos poco relevantes o de perder poca información, el impacto que supondría sería leve.







\section{Proyectos relacionados}
Muchos son los enfoques que se han estudiado para tratar de perfeccionar la generación de lenguaje natural. Los más tradicionales seguían la metodología presentada en el apartado \ref{sec:arquitectura_tradicional} de este documento. En ella dividían el problema principal en varios subproblemas o tareas. Entre ellas se incluía la selección de contenido, estructuración del texto, agregación, lexicalización, generación de expresiones de referencia y finalmente, la realización \citep{reiter1997building}. Sin embargo, en los últimos años ha crecido el interés por mirar más allá de aquella arquitectura. Los sistemas presentados en la sección \ref{sec:modelos} surgieron para romper con ella.

Todos estos sistemas presentados anteriormente tienen muy poca capacidad o poca calidad a la hora de generar textos de manera controlada. Algunos de ellos como GPT-2 generan textos a partir de una oración inicial, resultando su salida un texto de tamaño variable que da continuidad a la oración de entrada. Otros como BERT son capaces de generar palabras perdidas dentro de una oración. Sin embargo, pocos de ellos son capaces por sí mismos de generar contenido de manera controlada a partir de una información dada en todos los puntos de su generación.

 
Por todo esto, han surgido varios sistemas como DICE \citep{yang2020creative} que utilizarán estos modelos ajustándolos con el objetivo de controlar la generación del texto resultante. Como se muestra en la figura \ref{fig:DICE}, este sistema está compuesto por dos capas. La primera capa toma como entrada unas palabras clave introducidas por el usuario y forma un Grafo de Conocimiento. A continuación y para conectar ambas capas, utiliza tripletas como interfaz. Estas tripletas se pueden construir a partir del grafo o extraerse de un corpus de historias denominado ROCStory. Estas tripletas sirven como entrada a un sistema de generación de texto que utiliza GPT-2 para generar pequeñas historias.

\figura{Bitmap/02EstadoDeLaCuestion/DICE}{width=0.9\textwidth}{fig:DICE}%
{Arquitectura del sistema DICE}

Otro enfoque parecido utiliza una conocida dataset llamada WebNLG para entrenar un Modelo de Lenguaje. Esta base de datos contiene correspondencias entre textos y tripletas y es utilizada para ajustar el Modelo de Lenguaje T5. De esta manera, es capaz de generar textos a partir de tripletas introducidas como entrada al sistema. En la figura \ref{fig:T5}, podemos ver el formato de entrada.

\figura{Bitmap/02EstadoDeLaCuestion/T5}{width=0.9\textwidth}{fig:T5}%
{Entrada y salida del sistema T5}

Con respecto a propuestas dentro del ámbito de generación de lenguaje en terapias de reminiscencia, no son muchos los sistemas que encontramos. Por una parte, \citep{MoralesdeJess2020ACM} proponen desarrollar e implementar un modelo conversacional que pueda ayudar a los cuidadores y a sus propios pacientes con Alzheimer a realizar un mayor número de terapias de reminiscencia periódicas para así potenciar los beneficios de estas. Se centra en generar conversaciones personalizadas entre el prototipo del sistema conversacional y el paciente con el fin de recoger información relacionada con sus gustos, historial y estilo de vida. Su arquitectura, como se muestra en la figura \ref{fig:ConversationalModelArchitecture}, esta integrada por varios módulos: módulo de Reconocimiento Automático de Voz, Comprensión del Lenguaje Natural, Gestión de Diálogos, Modelo de Diálogos, Generación de Lenguaje Natural y Text-to-Speech.


\figura{Bitmap/02EstadoDeLaCuestion/ConversationalModelArchitecture}{width=.8\textwidth}{fig:ConversationalModelArchitecture}%
{Arquitectura modelo conversacional \citep{MoralesdeJess2020ACM}.}

En otra investigación (realizada por \cite{shi2012user}), se propone el desarrollo de un sistema computarizado llamado Life Story Book (LSB), que facilita el acceso y la recuperación de recuerdos almacenados que se utilizan como base para interacciones positivas entre ancianos y jóvenes, y especialmente entre personas con deterioro cognitivo y miembros de su familia o cuidadores. Para facilitar la gestión de la información y la generación dinámica de contenido, este artículo presenta un modelo semántico de LSB que se basa en el uso de ontologías y algoritmos avanzados para la selección de características y la reducción de dimensiones. Para terminar, propone un algoritmo llamado Onto-SVD que combina la selección de características semánticas y la ontología orientada al usuario con  la utilización de SVD como método de reducción de dimensiones para lograr la identificación de temas basada en la similitud semántica.

