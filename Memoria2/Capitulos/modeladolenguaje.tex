\chapter{Modelado de lenguaje y \textit{finetuning}}
\label{cap:desarrollo}


En este capítulo se realizará una aproximación a diferentes modelos de lenguaje basados en redes neuronales para la generación de texto. La motivación de este capítulo reside en la necesidad de comprender el funcionamiento de lo que podría ser considerado el núcleo de un sistema de generación: el modelo de lenguaje.
El cometido de este componente es la generación, a partir de una entrada dada, de una salida determinada acorde al propósito que se pretende conseguir con la construcción del sistema. 

Se realizará un acercamiento a los modelos de lenguaje más empleados en la actualidad: GPT-2, BERT y T5. La selección de estos tres modelos no es una decisión arbitraria sino que se escogieron aquellos modelos con mayor relevancia actualmente y que hubieran sido construidos con propósitos distintos de tal manera que se pueda discutir, según las características de cada uno de ellos, cuál/es serían los más apropiados para la tarea que se trata de realizar en este trabajo. Para poder considerar la adecuación del modelo se presentará el ajuste de cada uno de los modelos con respecto a un conjunto de datos de los presentados en el capítulo~\ref{cap:ConjuntosDeDatosYPreparacionParaElEntrenamiento}.


\section{Modelos pre-entrenados: Transformers}
\label{sec:transformers}
Los modelos pre-entrenados son modelos de aprendizaje profundo o \textit{Deep Learning} que surgieron como una evolución de los modelos Seq2Seq. Estos modelos de lenguaje son entrenados bajo grandes conjuntos de datos para realizar diversas tareas de Procesamiento de Lenguaje. Como parten de un conocimiento base, pueden ajustarse a tareas específicas sin requerir un entrenamiento desde cero. Este pre-entrenamiento es la clave de porque son tan valiosos, ya que permiten sin una gran esfuerzo computacional (normalmente tardan en entrenarse semanas o meses con los mejores computadores) construir un sistema de generación de lenguaje adaptándose al objetivo buscado. Otra ventaja de la existencia de este tipo de modelos es la posibilidad de elección de una pequeña \textit{dataset} para realizar el entrenamiento ya que los patrones lingüísticos generales ya se han aprendido durante el entrenamiento previo.



Dentro de este tipo de modelos pre-entrenados, destacan los \textit{Transformers} \citep{vaswani2017attention}. Estos modelos revolucionaron el Procesamiento de Lenguaje desde el momento en que se presentaron. Se basan en modelos Seq2Seq con mecanismos de atención y tratan de remediar los problemas de generación de este tipo de sistema. Recapitulando, la arquitectura neuronal recurrente propia del Secuencia a Secuencia implicaba un procesamiento secuencial para codificar la entrada en el \textit{encoder}. Posteriormente, se procesaba la información procedente del último estado oculto de codificador en el \textit{decoder} de la misma manera. Este procedimiento secuencial dificulta aplicar este tipo de modelos a la generación de textos largos, ya que tomaría mucho tiempo procesar todas las palabras de entrada a través de las distintas partes de la arquitectura. Ante esta problemática surgieron los mecanismos de atención que lograban paliar el cuello de botella producido en el último estado oculto del \textit{encoder}. Esta arquitectura mantenía las Redes Neuronales Recurrentes en codificador y decodificador como las LSTMs, sin embargo los \textit{Transformers} las sustituyeron por otras funciones lineales y no lineales que permitían un procesamiento mucho más rápido de la información.



El modelo \textit{Transformer} sustituye la capa de atención del modelo Seq2Seq implementada como un producto de matrices (\textit{Scaled Dot-Product Attention}), una función bilineal \citep{luong2015effective} o un perceptrón multicapa (\textit{Multi-layer Perceptron}), dependiendo del modelo, mejorando su rendimiento. El núcleo del modelo \textit{Transformers} reside en el mantenimiento de los productos escalares de matrices (\textit{Scaled Dot-Product Attention}) que posibilitan una gran eficiencia en tiempo y memoria ya que consiste únicamente en unas multiplicaciones básicas de matrices. Sin embargo, este mecanismo formará parte de la capa de atención multi-cabeza (\textit{Multi-Head Attention}) que viene sustituir la función completa de la capa de atención del Secuencia a Secuencia. 
El \textit{Multi-Head Attention} es un procedimiento consistente en varias capas en paralelo del anterior \textit{Scaled Dot-Product Attention}. Esta opción permite el procesamiento simultáneo de las diferentes entradas necesarias para la generación de texto que en el modelo con atención de Secuencia a Secuencia se realizaba de manera secuencial, lo que permite un procesamiento más eficiente, especialmente de grandes corpus de texto.


\figura{Bitmap/02EstadoDeLaCuestion/left-Scaled-Dot-Product-Attention-right-Multi-Head-Attention}{width=0.7\textwidth}{fig:comparationattention}%
{Capa de atención de \textit{Transformers} \citep{atencion_capas}}



La arquitectura externa del modelo \textit{Transformer} no dista demasiado de las explicadas anteriormente ya que se trata de una evolución de los modelos Seq2Seq, manteniendo la existencia del \textit{encoder} y del \textit{decoder}. Las modificaciones se realizan en la estructura interna de ambos componentes. 

El \textit{encoder} o codificador comienza con un módulo \textit{Multi-Head Attention} que realiza \textit{Scaled Dot-Product Attention} sobre la secuencia de entrada. A este procedimiento le siguen varias capas de normalización y conexión residual \footnote{Más información en https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55} para terminar con una capa de prealimentación (\textit{Feed-Forward Layer}) y otra capa de normalización y conexión residual. 

El \textit{decoder} o decodificador está compuesto de una estructura similar aunque algo más complicada. Comienza con un módulo compuesto por \textit{Multi-Head Attention} enmascarado para que posteriormente cada una de las palabras dependa únicamente de las palabras previas en el corpus. A continuación, una estructura semejante al \textit{encoder} recibe como entrada la salida del módulo anterior y la salida del codificador. Para finalizar, aplica una serie de funciones lineales y la función de activación \textit{Softmax}. De esta manera así obtiene diferentes probabilidades que generan la salida del sistema (figura ~\ref{fig:arquitectura_transformers}).


\figura{Bitmap/02EstadoDeLaCuestion/arquitectura_transformers}{width=0.72\textwidth}{fig:arquitectura_transformers}%
{Arquitectura del modelo Transformer}


\section{Tipos de tareas}
blablabla

WHEN to use WHAT?
MLM loss is preferred when the goal is to learn a good representation of the input document, whereas, CLM is mostly preferred when we wish to learn a system that generates fluent text. Also, intuitively this makes sense, because while learning good input representation for every word you would want to know the words that occur to it’s both left and right, whereas, when you want to learn a system that generates text, you can only see what all you have generated till now(it’s just like how humans write as well). So, making a system that could peek to the other side as well while generating text can introduce bias limiting the creative ability of the model.
Although you will often find both MLM and CLM losses when training the entire architecture having both encoder and decoder. Both have their advantages and limitations, a new model called XLNet uses a permutation technique to make use of the best of both worlds (MLM and CLM)



\subsection{\textit{Casual Language Models} (CLM)}

Los modelos de lenguaje casual o \textit{Casual Language Models} tienen como idea principal predecir una palabra o token enmascarado en una oración. Podemos ver una palabra enmascarada como un hueco en blanco en una frase. En este caso, el modelo solo consideraría el contexto anterior (palabras situadas a su izquierda) y dejaría de tener en cuenta el contexto posterior. El sentido de procesamiento (izquierda a derecha o derecha izquierda) no es relevante mientras que se realice en un único sentido, teniendo en cuenta para la predicción las palabras pertenecientes a ese contexto y dejando de lado el resto de palabras. Así, la propiedad clave de este tipo de modelos es la naturaleza unidireccional en su predicción de las palabras enmascaradas y se verá reflejado en el esquema de entrenamiento.

GPT-2 (\textit{Generative Pretrained Transformer}) es uno de los modelos más representativo de los modelos de lenguaje casual que sigue una arquitectura de tipo \textit{Transformer}. Creado por OpenAI en 2019, desde el primer momento fue considerado un gran éxito en el campo del Procesamiento de Lenguaje debido a sus más de 1.5 billones (americanos) de parámetros.

La arquitectura de GPT-2 es muy similar a la del modelo tranformer. Este modelo estaba formado por un \textit{encoder} y un \textit{decoder}. Esta arquitectura era apropiada para abordar determinadas tareas muy específicas de generación de lenguaje como era la traducción automática. Sin embargo, GPT-2 desecha esta arquitectura fija utilizando exclusivamente una pila de decodificadores del modelo transformer. El número de decodificadores apilados en esta pila varía con el tamaño de GPT-2 utilizado. En el caso de \textit{GPT-2 Small}, se utilizan únicamente doce \textit{decoders}, mientras que el modelo de mayor tamaño, \textit{GPT-2 Extra Large} ocupa hasta cuarenta y ocho \textit{decoders}. Estos datos se pueden contemplar en más profundidad en la figura ~\ref{fig:gpt2size}.



\figura{Bitmap/05Prototipos/gpt-2_size}{width=1\textwidth}{fig:gpt2size}%
{Distintos tamaños de GPT-2 y número de decodificadores que emplean}

Ya que se trata de un modelo unidireccional, se centra únicamente en la secuencia anterior a la última palabra y no tendrá en cuenta ninguna perteneciente a la secuencia posterior.
Una vez generada la nueva palabra, esta se añade a la secuencia de entrada. Esta nueva secuencia se convierte en la nueva entrada al modelo en el siguiente paso. Esta idea se denomina \textit{auto-regresión} o \textit{auto-regression}. De esta manera, se parte de una entrada (podría ser el \textit{token} de entrada \textit{<s>}) y obtiene la salida a través de la pila de decodificadores produciéndose un vector a lo largo del camino. Este vector se compara con el vocabulario del modelo (en el caso de GPT-2, este vocabulario está formado por 50000 palabras) y se selecciona el \textit{token} de mayor probabilidad. En el siguiente instante de tiempo, se añade este \textit{token} a la secuencia de entrada y se genera, de igual forma que para el primer \textit{token}, la salida a través de las capas de decodificadores. Al contrario que los modelos bidireccionales, en este segundo instante de tiempo no se va a reinterpretar el primer \textit{token}, ya que solo se genera hacia delante.


\subsection{\textit{Masked Language Models} (MLM)}
Con modelos de lenguaje enmascarados o \textit{Masked Language Models} nos referimos a aquellos modelos en los que se enmascaran un cierto porcentaje de palabras en una oración determinada y se espera que sea el modelo el que prediga dichas palabras en función del resto de la oración. Tal esquema hace que por naturaleza, el modelo sea bidireccional, dado que la representación de la palabra enmascarada depende tanto de las palabras situadas a su izquierda como de las palabras siguientes a ella en la oración.

El modelo representativo por excelencia de este tipo de técnica es el modelo BERT. BERT (\textit{Bidirectional Encoder Rrepresentarions from Transformers}) o Representación de Codificador Bidireccional de Transformadores \citep{Devlin2019BERTPO} es un modelo de lenguaje enmascarado de la familia transformers. %Al igual que GPT-2, se caracteriza por el preentrenamiento bajo una gran base de datos. En el caso del modelo BERT original se utilizan dos corpus de lengua inglesa: \textit{BookCorpus} y \textit{Wikipedia}.
Fue desarrollado por Google en el año 2018 y desde su publicación logró un rendimiento asombroso para diferentes de tareas de Procesamiento de Lenguaje Natural.

La innovación técnica clave que introduce BERT es aplicar una representación de lenguaje bidireccional, según se explicó anteriormente, esto significa que no solo se centra en la secuencia anterior o posterior a una palabra dada (procesamiento secuencial de los datos de izquierda a derecha o de derecha a izquierda que puede llegar a limitar el aprendizaje del contexto de una palabra), como ocurría en modelos unidirecionales como GPT-2, sino que también tiene en cuenta la secuencia contraria a este procesamiento (izquierda y derecha de la palabra). El objetivo de aplicar esta técnica es obtener un resultado con un sentido más profundo del contexto, ya que el modelo aprende el contexto de una palabra en función de su entorno por completo y un mejor flujo del lenguaje que los modelos de lenguaje unidireccionales.

En el caso de BERT, se rompe con la arquitectura de encoder-decoder, manteniendo únicamente una pila de codificadores de transformadores entrenados. Esta pila está formada por distinto número de capas de codificadores dependiendo de la versión de modelo utilizada: doce \textit{encoders} en el caso del modelo BERT Base o veinticuatro \textit{encoders} en el caso del modelo BERT Large. 




\section{Aprendizaje por transferencia (\textit{Transfer Learning})}

En diversas ocasiones a lo largo del trabajo se ha mencionado la ventaja que suponen los modelos pre-entrenados frente a los modelos estadísticos u otros tipos de algoritmos de \textit{Deep Learning} debido al pre-entrenamiento bajo una gran base de datos y un pequeño ajuste posterior a realizar para adaptarlo a la tarea que se desee resolver, exigiendo mucho menos coste computacional y requiriendo para ello menos tiempo y esfuerzo; sin embargo, no nos habíamos parado a pensar en el origen de esta propiedad. Todo esto es posible gracias al aprendizaje por transferencia o \textit{Transfer Learning}.


Los algoritmos convencionales de aprendizaje automático como el aprendizaje supervisado tradicional, hasta ahora, se han diseñado para resolver tareas específicas. En el momento en el que se desea modificar la tarea que resuelve el modelo, se debe reconstruir dicho modelo desde cero. El aprendizaje por transferencia surge con la idea de superar el paradigma de aprendizaje aislado y utilizar el conocimiento adquirido por el modelo, superando una tarea en específico, para resolver tareas relacionadas \citep{pan2009survey}. 

La mayoría de los modelos que resuelven problemas complejos necesitan una gran cantidad de datos, y obtener grandes cantidades de datos etiquetados para modelos supervisados puede ser realmente difícil en tiempo y esfuerzo. Es por esto que el aprendizaje por transferencia trata de aprovechar el conocimiento de modelos pre-entrenados (como pesos, características extraídas, etc) y utilizarlo en la resolución de nuevos problemas. 

Hasta ahora, las principales áreas de aplicación de este tipo de aprendizaje, debido los buenos resultados obtenidos, son la visión artificial y el procesamiento de lenguaje natural. Ejemplos concretos de utilización de este paradigma se encuentra en sistemas de visión de artificial como \citep{xinyuan2018}. Este sistema aplica el aprendizaje por transferencia a un sistema basado en redes neuronales convolucionales para la clasificación de imágenes, de tal manera que la nueva tarea a realizar sea el reconocimiento de imágenes de perros, en específico. En la generación de lenguaje, los modelos pre-entrenados también hacen uso de esta técnica para lograr resolver tareas especificar como la detección de noticias falsas \citep{slovikovskaya2019transfer}.

Existen muchas variantes del aprendizaje por transferencia: adaptación de dominio, confusión de dominio, One-shot Learning, Zero-shot Learning (utilizado por GPT2) o Multi-taks Learning (empleado por T5). 

%Zero-shot Learning (utilizado por GPT2)
%https://medium.com/towards-data-science/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a

De entre todos los tipos de aprendizaje por transferencia, destaca el aprendizaje multitarea, utilizado por T5. La motivación detrás de esta variación es crear un modelo generalista que pueda resolver multiples tareas en lugar de crear varios modelos especialistas que estén capacitados para una sola tarea.
%https://medium.com/analytics-vidhya/a-primer-on-multi-task-learning-in-nlp-part-1-7154b4227c0e


\section{\textit{Finetuning} como estrategia del Aprendizaje por Transferencia}

Existen diferentes estrategias para conseguir este aprendizaje por transferencia. Una de las técnicas que mejores resultados consigue es el ajuste o \textit{fine-tuning} de un modelo ya entrenado. Esta técnica recibe este nombre porque ajusta ligeramente las representaciones más abstractas del modelo que se está reutilizando, con el objetivo de hacerlas más relevantes para el nuevo problema en cuestión.

%Antes de realizar el proceso de ajuste, es necesario seleccionar el modelo adecuado sobre el que se va a realizar el proceso de adaptación.



\subsection{Pre-entrenamiento del modelo}

Los modelos pre-entrenados han sido previamente capacitados para resolver una tarea mediante el aprendizaje de una serie de parámetros. Para realizar este pre entrenamiento, generalmente, se utiliza aprendizaje auto-supervisado (\textit{Self-supervised Language Modelling}). Este proceso se realiza con los textos sin procesar, es decir, sin que los humanos etiqueten los datos de entrada de ninguna manera. La ventaja reside en que debido a la gran cantidad de datos que contienen los corpus utilizados en el entrenamiento, este procedimiento de etiquetado no se podría llevar a cabo de manera manual. %TODO: Contar un poco más - lo de que se sustituyen capas, etc ...


En los grandes modelos transformers, este proceso de entrenamiento es realizado en grandes computadores capaces de entrenar estos modelos sobre grandes conjuntos de datos. A continuación, vamos a centrarnos en los modelos de lenguaje que hemos considerado estudiar y extraeremos las principales claves de su preentrenamiento. También, se realizarán pruebas de funcionamiento de dichos modelos para comprobar sus resultados generales.



\subsubsection{GPT-2}

El pre-entrenamiento de GPT-2 se realizó sobre un gran corpus de texto en inglés siguiendo el entrenamiento auto-supervisado (\textit{self-supervised}). El objetivo de este modelo es la predicción de la palabra siguiente dada una secuencia de palabras u oración.

Para pre-entrenar el modelo se creó una base de datos, llamada \textit{WebText}, formada a partir de la extracción de todas las páginas web de los enlaces salientes en Reddit que recibieron una determinada puntuación mínima, para garantizar la calidad y significancia del enlace. Las páginas de Wikipedia relacionadas con estos enlaces se eliminaron. Es por esto que GPT-2 no está entrenado bajo ningún texto de Wikipedia. El conjunto de datos resultante es un enorme corpus de 40GB de textos preparado para el entrenamiento de este modelo, GPT-2 \citep{radford2019language}.

Pese a todas las ventajas de este modelo, también tiene algunas limitaciones. Una de ellas, producida por el conjunto de datos seleccionado y por el propio algoritmo de entrenamiento aplicado, es la existencia de sesgos en la generación. Ya que la base de datos de partida está formada por una gran cantidad de contenido de internet sin filtrar, está influenciada por los sesgos representativos de los creadores de estos contenidos. En concreto, bajo la entrada ``El hombre blanco trabajaba como'', la salida generaba como posibles palabras de continuación a la oración ``periodista'' o ``conductor de autobús'', frente a la respuesta ``esclavo'' cuando se modificaba la raza de la persona de la oración de entrada. 

Para comprender el proceso de generación de texto con este modelo se realizaron una serie de pruebas de funcionamiento básico. Para la obtención de todos los módulos necesario se utilizó la API \textit{Transformers} de la herramienta \textit{Hugging Face} que proporciona Python para la descarga y entrenamiento de modelos preentrenados. El modelo pre-entrenado utilizado es \textit{GPT2HeadModel}, una configuración del modelo GPT2 preparado para modelado de lenguaje. Por otra parte, el \textit{tokenizer} empleado es \textit{GPT2Tokenizer} basado en el algoritmo \textit{Byte-Pair-Encoding} visto anteriormente.

Como apunte, la tokenización, en el campo del Procesamiento de Lenguaje Natual, se refiere al proceso de transformación de una secuencia de palabras o símbolos a \textit{tokens} para que la máquina pueda comprender el lenguaje humano y contexto detrás de él. El proceso de tokenización en GPT-2, se basa en la obtención de subpalabras mediante un algoritmo de codificación de pares de bytes (\textit{Byte Pair Encoding} o \textit{BPE}).

El algoritmo BPE \citep{gage1994new} es un algoritmo de tokenización basado en subpalabras. Su objetivo principal es la resolución de los problemas de otros tipos de tecnologías basadas en palabras o en caracteres mediante un enfoque intermedio. De manera teórica, BPE es una forma simple de comprensión de datos en el que el par más común de bytes de datos consecutivos se reemplaza con un byte que no aparece en esos datos. Esta idea garantiza que las palabras más comunes se representen en el vocabulario como un solo \textit{token}, mientras que las palabras menos habituales se dividen en dos o más tokens de subpalabras.

Volviendo al tokenizador concreto utilizado: \textit{GPT2Tokenizer}; tiene en cuenta los espacios y por tanto asignará diferentes \textit{tokens} teniendo en cuenta también dicho carácter. Se puntualiza que con el parámetro $add\_prefix\_space=True$ se puede sortear este comportamiento, aunque no es lo recomendable ya que el modelo no está pre-entrenado de esa manera y podría derivar en una disminución del rendimiento. El resultado de aplicar a un texto inicial el \textit{GPT2Tokenizer} se puede comprobar en el código \ref{lst:tokenizer-gpt2}.

\begin{lstlisting}[language=Python, caption=Ejemplo de uso de \textit{GPT2Tokenizer}, label={lst:tokenizer-gpt2}]
	tokenizer('I love Transformers', add_prefix_space=False)
	>> {'input_ids': [40, 1842, 39185], 'attention_mask': [1, 1, 1]}
	
	tokenizer(' I love you', add_prefix_space=False)
	>> {'input_ids': [314, 1842, 345], 'attention_mask': [1, 1, 1]}
\end{lstlisting} 

El resultado de aplicar la tokenización de \textit{GPT2Tokenizer} sobre una secuencia de palabras resulta en una lista denominada \textit{inputs\_ids} que asigna un número identificador a cada uno de los \textit{tokens} encontrados en dicha secuencia. En el ejemplo \ref{lst:enc_dec-gpt2} se puede comprobar el funcionamiento del proceso de codificar y decodificar. Dada una secuencia de entrada, en este caso \textit{'What is love?'}, se la pasamos al tokenizador y la codificamos. Este procedimiento devuelve los \textit{input\_its} en forma de objeto \textit{tensor} y a continuación decodificamos. La secuencia original y la resultante después de aplicar ambos proceso son similares.

\begin{lstlisting}[language=Python, caption= Encode y Decode, label={lst:enc_dec-gpt2}]
	>> original seq : What is love?
	>> input_ids    : tensor([[2061,  318, 1842,   30]])
	>> decoded seq  : What is love?
\end{lstlisting} 

A continuación se describe el proceso completo de generación de texto con GPT2 haciendo uso del tokenizador y del modelo. Para comenzar se codifica la secuencia de entrada al igual que se realizó en el ejemplo anterior. A continuación se crea el modelo a partir del \textit{GPT2LMHeadModel} y se genera la salida con el método \textit{generate} (para generar el resultado final se emplearon los parámetros $max\_lenght=50$ para establecer una longitud máxima, $num\_beans=5$ y $no\_repeat\_ngram\_size=2$). Para finalizar, se decodifica la salida del generador, ya que el resultado es un objeto \textit{tensor} similar al producido en la codificación. En el ejemplo~\ref{lst:model-gpt2} mostrado, se genera la continuación a la secuencia de entrada dada. El resultado es un texto coherente y cohesionado.

\begin{lstlisting}[language=Python, caption=Ejemplo de uso de \textit{GPT2LMHeadModel}, label={lst:model-gpt2}]
	>> What is love?
	Love is a word that has been around for a long time. It's a way of saying "I love you, but I don't know what it means to love someone else."	
\end{lstlisting} 

\subsubsection{BERT}


BERT es un modelo preentrenado bajo dos grandes corpus de lengua inglesa con datos sin etiquetar. Por una parte, \textit{BookCorpus} \citep{Zhu_2015_ICCV} disponible en \textit{Hugging Face} \footnote{https://huggingface.co/datasets/bookcorpus} es un conocido corpus de texto a gran escala destinado especialmente al aprendizaje no supervisado  de codificadores y decodificadores. \textit{BookCorpus}, está compuesto por 11038 libros (con alrededor de 74MB de oraciones y 1GB de palabras) de 16 diferentes subgéneros literarios. Otro de los corpus utilizados en el pre-entrenamiento del modelo es la Wikipedia inglesa, formada por textos de diversos temas y revisados por la comunidad de Wikipedia, lo que asegura una buena calidad y seguridad al entrenamiento del sistema.


Este modelo es capaz de realizar diversas tareas de procesamiento de lenguaje. Entre ellas destaca el Modelado de Lenguaje Enmascarado según se comentó en el apartado~\ref{sec:mlm}. Esta tarea de pre-entrenamiento del modelo se sustenta en un entrenamiento con una versión corrupta de los datos, generalmente enmascarando algunos tokens al azar y dejando que el modelo prediga el texto original. Este proceso garantiza la bidireccionalidad del modelo. Para llevar a cabo este procedimiento, antes de introducir una secuencia de palabras al modelo BERT, se reemplazan aproximadamente el 15\% de las palabras de dicha secuencia por el \textit{token} [MASK]. Seguidamente, el modelo trata de predecir el valor original de las palabras enmascaradas por el \textit{token} en función del contexto, proporcionado por el resto de palabras no enmascaradas de la secuencia. Para poder realizar la predicción de la palabra enmascarada, se modifica ligeramente la arquitectura añadiendo una capa de clasificación a la salida del codificador. Después, se multiplican los vectores de salida por la matriz de incrustación, para transformar dicho vector en una matriz de la dimensión del vocabulario. Para terminar, se calcula la probabilidad de cada palabra en el vocabulario con la función \textit{softmax}. Esta nueva arquitectura se muestra en la figura ~\ref{fig:bert-MLM} de manera más detallada.

\figura{Bitmap/05Prototipos/bert-MLM}{width=1\textwidth}{fig:bert-MLM}%
{Arquitectura BERT para MLM}


Otro de los procesos llevados a cabo durante el entrenamiento es la Prediccion de la Siguiente Palabra o NSP por sus siglas en inglés. Durante el proceso de preentrenamiento, el modelo BERT recibe pares de oraciones como entrada y aprende a predecir si la segunda oración corresponde a la siguiente oración en el documento original. Aproximadamente, el 50\% de estos pares de oraciones de entradas corresponden a dos pares seguidos de secuencias en el corpus original, mientras que el 50\% no son secuencias contiguas, sino que se realiza una elección aleatoria de cualquier otra oración del texto. Para que pueda realizar este procedimiento, se inserta el \textit{token} [CLS] al comienzo de la primera oración y el \textit{token} [SEP] para separar los pares de oraciones.
%Más cosas : Se agrega una incrustación posicional a cada token para indicar su posición en la secuencia.  https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270
Para predecir si la segunda oración está realmente contigua en el texto original a la primera, es necesario que toda la secuencia de entrada pase por el codificador del modelo. La salida del modelo del primer token [CLS] es un vector de dimensiones 2 x 1 y utilizando una capa de clasificación simple que se añade a la arquitectura, se calcula la probabilidad de la contigüidad de oraciones con la función \textit{softmax}.

Ambos procedimientos descritos se utilizan en conjunto durante el pre-entrenamiento del modelo con el objetivo de minimizar la función de pérdida combinada de ambas estrategias.

La entrada final al modelo se denomina \textit{input embeddings}. Este vector de entradas se constituye con la suma de los \textit{token embeddings}, \textit{segment embeddings} y \textit{position embeddings}. El primero de ellos se refiere a los \textit{tokens} resultantes de aplicar el tokenizador con los \textit{tokens} especiales [CLS] y [SEP]. El segundo \textit{embedding} indica la separación de oraciones. Por último, el \textit{position embedding} señala la posición de cada una de las palabras en la secuencia de entrada. Este concepto se muestra en la figura ~\ref{fig:bert-embeddings}.


\figura{Bitmap/05Prototipos/embeddings}{width=1\textwidth}{fig:bert-embeddings}%
{Constitución de la entrada del modelo BERT}



A continuación vamos a comprobar el funcionamiento del modelo BERT. Concretamente, se evaluará el proceso de predicción de la palabra más probable dada una secuencia con alguna palabra enmascarada con el \textit{token} [MASK].

Al igual que GPT-2, BERT necesita tokenizar los datos de entrada para poder manejarlos internamente. El tokenizador utilizado por este modelo de lenguaje es \textit{WordPiece} \citep{wordpiece} basado en subpalabras. Este algoritmo posee dos implementaciones: un enfoque ascendente de abajo hacia arriba y un enfoque descendente de arriba hacia abajo. El modelo BERT original utiliza el enfoque ascendente.

Este algoritmo no difiere demasiado del algoritmo BPE descrito anteriormente, ya que se trata de una versión modificada de dicho algoritmo. Sin embargo, \textit{WordPiece} trata de solucionar un problema común del BPE, limitado por la confusión de elección de un \textit{token} en el caso de las instancias que tiene más de una manera de ser codificadas. Debido a este problema, una misma entrada podría representarse mediante diferentes codificaciones pudiendo afectar a la precisión de las representaciones aprendidas.

Para realizar este proceso se utiliza el tokenizador \textit{BertTokenizer} que internamente
implementa el algoritmo \textit{WordPiece} descrito anteriormente. El primer ejemplo (código ~\ref{lst:tokenizer-bert}), muestra el resultado de tokenizar el texto de entrada. Se puede observar que la palabra ``thunderous'' no se encuentra en el vocabulario del tokenizador y por tanto la descompone en dos \textit{tokens}: ``thunder'' y ``\#\#ous''. Para indicar que estos \textit{tokens} no pertenecen a palabras separadas utiliza la doble almohadilla (\textit{\#\#}) como prefijo en el segundo \textit{token}.

\begin{lstlisting}[language=Python, caption=Resultado de aplicar \textit{BertTokenizer} a un texto de entrada, label={lst:tokenizer-bert}]
	sequence = "The thunderous roar of the jet overhead confirmed 
	her worst fears"
	
	>> ['The', 'thunder', '##ous', 'roar', 'of', 'the', 'jet', 
	'overhead', 'confirmed', 'her', 'worst', 'fears']
\end{lstlisting} 

A continuación, se muestra el proceso de predicción de una palabra enmascarada en una secuencia utilizada como entrada al modelo (código ~\ref{lst:model-bert}). El modelo utilizado es \textit{BertForMaskeLM}, una versión especial de BERT para la realización exclusiva de esta tarea. El proceso seguido para la predicción es muy sencillo: primero se obtienen los \textit{input\_ids} de los \textit{tokens} que conforman la entrada y se codifican; a continuación se obtiene el índice de las palabras enmascaradas (en el ejemplo mostrado hay un solo \textit{token} [MASK]); una vez obtenida la salida del modelo dada la secuencia de entrada, se aplica la función \textit{softmask} y finalmente se filtran las cinco palabras con mayor probabilidad. El resultado es una oración coherente mediante la generación de una palabra acorde con su entorno.

\begin{lstlisting}[language=Python, caption=Ejemplo de predicción de una palabra enmascarada en una secuencia, label={lst:model-bert}]
	text = "Every Monday, Mary goes to the " + tokenizer.mask_token + " to relax."
	
	>> Every Monday, Mary goes to the beach to relax.
	Every Monday, Mary goes to the library to relax.
	Every Monday, Mary goes to the bathroom to relax.
	Every Monday, Mary goes to the lake to relax.
	Every Monday, Mary goes to the gym to relax.
\end{lstlisting} 


\subsubsection{T5}


Para entrenar el modelo T5 se creó una enorme corpus de datos llamado \textit{``Colossal Clean Crawled Corpus''} (\textit{C4}). Este conjunto de datos contiene 750GB de información en lengua inglesa extraída mediante \textit{web scrappping} de la web. Ya que el corpus original son 20TB de datos no revisados y podían incluir lenguaje ofensivo, código fuente, textos en otros idiomas... en resumen, texto que no interesa para el entrenamiento, se siguieron una serie de pautas muy precisas para eliminar toda esta información, resultando un corpus limpio libre de todo dato no necesario.

Para realizar el preentrenamiento, se tiene en cuenta cada una de las tareas para las que se va a crear el modelo. Ya que se soporta en un enfoque de tipo \textit{Text-to-Text}, la entrada para cada una de las acciones a realizar será un texto, al igual que su salida. Para realizar tareas de traducción, sus autores precisan que la entrada al modelo fuese ``\textit{translate English to German: That is
	good.}'' en el caso de querer traducir del idioma inglés al alemán ``\textit{That is
	good}''. La salida del sistema sería ``\textit{Das ist gut.}''. En el caso de generación de resúmenes, la entrada estaría constituida por el texto a resumir seguida del texto ``\textit{TL;DR}'' (abreviación de \textit{too long, didn't read}). De esta manera se genera un resumen de un texto via decodificación autorregresiva. 

Al igual que BERT, utiliza una entrada tokenizada como entrada al modelo. En el caso de T5, modifica el algoritmo de tokenización que utiliza el anterior modelo, basándose ahora en el algoritmo \textit{SentencePiece} que opera sobre regulación de subpalabras. Este algoritmo de tokenización, implementado en C++ es, increíblemente rápido, lo que resulta en un entrenamiento y generación muchísimo más veloz en comparación con los tokenizadores utilizados en GPT-2 (BPE) o BERT (WordPiece). Otra de las ventajas de este tokenizador es que se utiliza directamente sobre los datos sin la necesidad de almacenar los datos tokenizados en discos, por lo que utiliza menos memoria en el proceso. Por otra parte, es agnóstico respecto a los espacios en blanco, confiriendo a idiomas que en ocasiones no hacen uso de ellos, como el chino o japones, la misma facilidad de tokenización que a cualquier otro lenguaje. En general, se basa en la idea de que la codificación de pares de bytes no es óptima para el entrenamiento previo del modelo de lenguaje \citep{bostrom-durrett-2020-byte}.

%https://yukyunglee-github-io.translate.goog/Transformer-to-T5-4/?_x_tr_sl=auto&_x_tr_tl=es&_x_tr_hl=es&_x_tr_pto=wapp
Para pre-entrenar el modelo se adoptó el método de enmascaramiento MLM. Sin embargo, la diferencia con el método existente utilizado en BERT reside en que los \textit{tokens} consecutivos se reemplazan con una máscara sin enmascarar un \textit{token} aleatorio. Específicamente, si antes a cada uno de las palabras enmascarados se les sustituía con el \textit{token} [MASK], este método los sustituye por <X>, <Y>, <Z>... y así sucesivamente hasta enmascarar todas las palabras introducidas. Estos \textit{tokens} se denominan \textit{tokens} centinela y tienen un tratamiento especial.




\subsection{\textit{Supervised fine-tuning}}
%https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%\section{GPT-2 (\textit{Generative Pretrained Transformer})}
%Como se expuso en la sección~\ref{sec:transformers}, GPT-2 (\textit{Generative Pretrained Transformer}) es un modelo de generación de lenguaje que sigue una arquitectura de tipo \textit{Transformer}. Creado por OpenAI en 2019, desde el primer momento fue considerado un gran éxito en el campo del Procesamiento de Lenguaje debido a sus más de 1.5 billones (americanos) de parámetros.

%\subsection{Arquitectura}\hfill


%La arquitectura de GPT-2 es muy similar a la del modelo tranformer. Este modelo estaba formado por un \textit{encoder} y un \textit{decoder}. Esta arquitectura era apropiada para abordar determinadas tareas muy específicas de generación de lenguaje como era la traducción automática. Sin embargo, GPT-2 desecha esta arquitectura fija utilizando exclusivamente una pila de decodificadores del modelo transformer. El número de decodificadores apilados en esta pila varía con el tamaño de GPT-2 utilizado. En el caso de \textit{GPT-2 Small}, se utilizan únicamente doce \textit{decoders}, mientras que el modelo de mayor tamaño, \textit{GPT-2 Extra Large} ocupa hasta cuarenta y ocho \textit{decoders}. Estos datos se pueden contemplar en más profundidad en la figura ~\ref{fig:gpt2size}.



%\figura{Bitmap/05Prototipos/gpt-2_size}{width=1\textwidth}{fig:gpt2size}{Distintos tamaños de GPT-2 y número de decodificadores que emplean}


%Otra idea importante es la característica unidireccionalidad de este modelo. Que un modelo de lenguaje sea unidireccional quiere decir que se centra únicamente en la secuencia anterior a la última palabra sin tener en cuenta ninguna secuencia posterior. Una vez generada la nueva palabra, esta se añade a la secuencia de entrada. Esta nueva secuencia se convierte en la nueva entrada al modelo en el siguiente paso. Esta idea se denomina \textit{auto-regresión} o \textit{auto-regression}. De esta manera, se parte de una entrada (podría ser el \textit{token} de entrada \textit{<s>}) y obtiene la salida a través de la pila de decodificadores produciéndose un vector a lo largo del camino. Este vector se compara con el vocabulario del modelo (en el caso de GPT-2, este vocabulario está formado por 50000 palabras) y se selecciona el \textit{token} de mayor probabilidad. En el siguiente instante de tiempo, se añade este \textit{token} a la secuencia de entrada y se genera, de igual forma que para el primer \textit{token}, la salida a través de las capas de decodificadores. Al contrario que los modelos bidireccionales, en este segundo instante de tiempo no se va a reinterpretar el primer \textit{token}, ya que solo se genera hacia delante.

%\subsection{Representación de la entrada}\hfill


%En el apartado anterior se habló del concepto \textit{token}. La tokenización, en el campo del Procesamiento de Lenguaje Natual, se refiere al proceso de transformación de una secuencia de palabras o símbolos a \textit{tokens} para que la máquina pueda comprender el lenguaje humano y contexto detrás de él. El proceso de tokenización en GPT-2, se basa en la obtención de subpalabras mediante un algoritmo de codificación de pares de bytes (\textit{Byte Pair Encoding} o \textit{BPE}).

%El algoritmo BPE \citep{gage1994new} es un algoritmo de tokenización basado en subpalabras. Su objetivo principal es la resolución de los problemas de otros tipos de tecnologías basadas en palabras o en caracteres mediante un enfoque intermedio. De manera teórica, BPE es una forma simple de comprensión de datos en el que el par más común de bytes de datos consecutivos se reemplaza con un byte que no aparece en esos datos. Esta idea garantiza que las palabras más comunes se representen en el vocabulario como un solo \textit{token}, mientras que las palabras menos habituales se dividen en dos o más tokens de subpalabras.

%\subsection{Pre-entrenamiento}\hfill


%El entrenamiento de GPT-2 se realizó sobre un gran corpus de texto en inglés conocido como \textit{WebText}, siguiendo un entrenamiento denominado auto-supervisado (\textit{self-supervised}). Este proceso se realiza con los textos sin procesar, es decir, sin que los humanos etiqueten los datos de entrada de ninguna manera. La ventaja reside en que debido a la gran cantidad de datos que contenía el corpus, este procedimiento de etiquetado no se podría llevar a cabo. Concretamente, este modelo se entrena para predecir la siguiente palabra en una oración dada como entrada. 

%Pese a todas las ventajas de este modelo, también tiene algunas limitaciones. Una de ellas, producida por el conjunto de datos seleccionado y por el propio entrenamiento aplicado, es la existencias de sesgos en la generación. Ya que la base de datos de partida está formada por una gran cantidad de contenido de internet sin filtrar, está influenciada por los sesgos representativos de los creadores de estos contenidos. En concreto, bajo la entrada ``El hombre blanco trabajaba como'', la salida generaba como posibles palabras de continuación a la oración ``periodista'' o ``conductor de autobús'', frente a la respuesta ``esclavo'' cuando se modificaba la raza de la persona de la oración de entrada. 

%Para entrenar el modelo se creó una base de datos, llamada \textit{WebText}, formada a partir de la extracción de todas las páginas web de los enlaces salientes en Reddit que recibieron una determinada puntuación mínima, para garantizar la calidad y significancia del enlace. Las páginas de Wikipedia relacionadas con estos enlaces se eliminaron. Es por esto que GPT-2 no está entrenado bajo ningún texto de Wikipedia. El conjunto de datos resultante es un enorme corpus de 40GB de textos preparado para el entrenamiento de este modelo, GPT-2 \citep{radford2019language}.


%\subsection{Prueba de funcionamiento}\hfill

%Para comprender el proceso de generación de texto con este modelo se realizaron una serie de pruebas de funcionamiento básico. Para la obtención del modelo y del tokenizador se utilizó la API \textit{Transformers} de la herramienta \textit{Hugging Face} que proporciona Python para la descarga y entrenamiento de modelos preentrenados. El modelo preentrenado utilizado es \textit{GPT2HeadModel}, una configuración del modelo GPT2 preparado para modelado de lenguaje. Por otra parte, el \textit{tokenizer} empleado es \textit{GPT2Tokenizer} basado en el algoritmo \textit{Byte-Pair-Encoding} visto anteriormente. Este tokenizador tiene en cuenta los espacios y por tanto asignará diferentes \textit{tokens} teniendo en cuenta también dicho carácter. Se puntualiza que con el parámetro $add\_prefix\_space=True$ se puede sortear este comportamiento aunque no es lo recomendable ya que el modelo no está preentrenado de esa manera y podría derivar en una disminución del rendimiento. El resultado de aplicar a un texto inicial el \textit{GPT2Tokenizer} se puede comprobar en el código \ref{lst:tokenizer-gpt2}.

%\begin{lstlisting}[language=Python, caption=Ejemplo de uso de \textit{GPT2Tokenizer}, label={lst:tokenizer-gpt2}]
%	tokenizer('I love Transformers', add_prefix_space=False)
%	>> {'input_ids': [40, 1842, 39185], 'attention_mask': [1, 1, 1]}
	
%	tokenizer(' I love you', add_prefix_space=False)
%	>> {'input_ids': [314, 1842, 345], 'attention_mask': [1, 1, 1]}
%\end{lstlisting} 

%El resultado de aplicar la tokenización de \textit{GPT2Tokenizer} sobre una secuencia de palabras resulta en una lista denominada \textit{inputs\_ids} que asigna un número identificador a cada uno de los \textit{tokens} encontrados en dicha secuencia. En el ejemplo \ref{lst:enc_dec-gpt2} se puede comprobar el funcionamiento del proceso de codificar y decodificar. Dada una secuencia de entrada, en este caso \textit{'What is love?'}, se la pasamos al tokenizador y la codificamos. Este procedimiento devuelve los \textit{input\_its} en forma de objeto \textit{tensor} y a continuación decodificamos. La secuencia original y la resultante después de aplicar ambos proceso son similares.

%\begin{lstlisting}[language=Python, caption= Encode y Decode, label={lst:enc_dec-gpt2}]
%	>> original seq : What is love?
%	>> input_ids    : tensor([[2061,  318, 1842,   30]])
%	>> decoded seq  : What is love?
%\end{lstlisting} 

%A continuación se describe el proceso completo de generación de texto con GPT2 haciendo uso del tokenizador y del modelo. Para comenzar se codifica la secuencia de entrada al igual que se realizó en el ejemplo anterior. A continuación se crea el modelo a partir del \textit{GPT2LMHeadModel} y se genera la salida con el método \textit{generate} (para generar el resultado final se emplearon los parámetros $max\_lenght=50$ para establecer una longitud máxima, $num\_beans=5$ y $no\_repeat\_ngram\_size=2$). Para finalizar, se decodifica la salida del generador, ya que el resultado es un objeto \textit{tensor} similar al producido en la codificación. En el ejemplo~\ref{lst:model-gpt2} mostrado, se genera la continuación a la secuencia de entrada dada. El resultado es un texto coherente y cohesionado.

%\begin{lstlisting}[language=Python, caption=Ejemplo de uso de \textit{GPT2LMHeadModel}, %label={lst:model-gpt2}]
%	>> What is love?
%	Love is a word that has been around for a long time. It's a way of saying "I love you, but I don't know what it means to love someone else."
	
	
%\end{lstlisting} 



%\section{BERT (\textit{Bidirectional Encoder Rrepresentarions from Transformers})}
%BERT (\textit{Bidirectional Encoder Rrepresentarions from Transformers}) o Representación de Codificador Bidireccional de Transformadores \citep{Devlin2019BERTPO} es un modelo de lenguaje enmascarado de la familia transformers. Al igual que GPT-2, se caracteriza por el preentrenamiento bajo una gran base de datos. En el caso del modelo BERT original se utilizan dos corpus de lengua inglesa: \textit{BookCorpus} y \textit{Wikipedia}.
%Fue desarrollado por Google en el año 2018 y desde su publicación logró un rendimiento asombroso para diferentes de tareas de Procesamiento de Lenguaje Natural.

%La innovación técnica clave que introduce BERT es aplicar una representación de lenguaje bidireccional. Esto significa que no solo se centra en la secuencia anterior o posterior a una palabra dada (procesamiento secuencial de los datos de izquierda a derecha o de derecha a izquierda que puede llegar a limitar el aprendizaje del contexto de una palabra), como ocurría en modelos unidirecionales como GPT-2, sino que también tiene en cuenta la secuencia contraria a este procesamiento (izquierda y derecha de la palabra). El objetivo de aplicar esta técnica es obtener un resultado con un sentido más profundo del contexto, ya que el modelo aprende el contexto de una palabra en función de su entorno por completo  y un mejor flujo del lenguaje que los modelos de lenguaje unidireccionales.

%Al igual que GPT2, rompe con la arquitectura de encoder-decoder manteniendo únicamente una pila de codificadores de transformadores entrenados. Esta pila está formada por distinto número de capas de codificadores dependiendo de la versión de modelo utilizada: doce \textit{encoders} en el caso del modelo BERT Base o veinticuatro \textit{encoders} en el caso del modelo BERT Large. 


%\subsection{Datos de entrada}\hfill

%https://towardsdatascience.com/wordpiece-subword-based-tokenization-algorithm-1fbd14394ed7
%Al igual que GPT-2, BERT necesita tokenizar los datos de entrada para poder manejarlos internamente. El tokenizador utilizado por este modelo de lenguaje es \textit{WordPiece} \citep{wordpiece} basado en subpalabras. Este algoritmo posee dos implementaciones: un enfoque ascendente de abajo hacia arriba y un enfoque descendente de arriba hacia abajo. El modelo BERT original utiliza el enfoque ascendente.

%Este algoritmo no difiere demasiado del algoritmo BPE descrito anteriormente, ya que se trata de una versión modificada de dicho algoritmo. Sin embargo, \textit{WordPiece} trata de solucionar un problema común del BPE, limitado por la confusión de elección de un \textit{token} en el caso de las instancias que tiene más de una manera de ser codificadas. Debido a este problema, una misma entrada podría representarse mediante diferentes codificaciones pudiendo afectar a la precisión de las representaciones aprendidas.


%\subsection{Pre-entrenamiento}\hfill


%BERT es un modelo preentrenado bajo dos grandes corpus de lengua inglesa con datos sin etiquetar. Por una parte, \textit{BookCorpus} \citep{Zhu_2015_ICCV} disponible en \textit{Hugging Face} \footnote{https://huggingface.co/datasets/bookcorpus} es un conocido corpus de texto a gran escala destinado especialmente al aprendizaje no supervisado  de codificadores y decodificadores. \textit{BookCorpus}, está compuesto por 11038 libros (con alrededor de 74MB de oraciones y 1GB de palabras) de 16 diferentes subgéneros literarios. Otro de los corpus utilizados en el preentrenamiento del modelo es la Wikipedia inglesa, formada por textos de diversos temas y revisados por la comunidad de Wikipedia, lo que asegura una buena calidad y seguridad al entrenamiento del sistema.


%Este modelo es capaz de realizar diversas tareas de procesamiento de lenguaje. Entre ellas destaca el Modelado de Lenguaje Enmascarado o MLM por sus siglas en inglés. Esta tarea de preentrenamiento del modelo se sustenta en un entranamiento con una versión corrupta de los datos, generalmente enmascarando algunos tokens al azar y dejando que el modelo prediga el texto original. Este proceso garantiza la bidireccionalidad del modelo. Para llevar a cabo este procedimiento, antes de introducir una secuencia de palabras al modelo BERT, se reemplazan aproximadamente el 15\% de las palabras de dicha secuencia por el \textit{token} [MASK]. Seguidamente, el modelo trata de predecir el valor original de las palabras enmascaradas por el \textit{token} en función del contexto, proporcionado por el resto de palabras no enmascaradas de la secuencia. Para poder realizar la predicción de la palabra enmascarada, se modifica ligeramente la arquitectura añadiendo una capa de clasificación a la salida del codificador. Después, se multiplican los vectores de salida por la matriz de incrustación, para transformar dicho vector en una matriz de la dimensión del vocabulario. Para terminar, se calcula la probabilidad de cada palabra en el vocabulario con la función \textit{softmax}. Esta nueva arquitectura se muestra en la figura ~\ref{fig:bert-MLM} de manera más detallada.

%\figura{Bitmap/05Prototipos/bert-MLM}{width=1\textwidth}{fig:bert-MLM}%
%{Arquitectura BERT para MLM}


%Otro de los procesos llevados a cabo durante el entrenamiento es la Prediccion de la Siguiente Palabra o NSP por sus siglas en inglés. Durante el proceso de preentrenamiento, el modelo BERT recibe pares de oraciones como entrada y aprende a predecir si la segunda oración corresponde a la siguiente oración en el documento original. Aproximadamente, el 50\% de estos pares de oraciones de entradas corresponden a dos pares seguidos de secuencias en el corpus original, mientras que el 50\% no son secuencias contiguas, sino que se realiza una elección aleatoria de cualquier otra oración del texto. Para que pueda realizar este procedimiento, se inserta el \textit{token} [CLS] al comienzo de la primera oración y el \textit{token} [SEP] para separar los pares de oraciones.
%Más cosas : Se agrega una incrustación posicional a cada token para indicar su posición en la secuencia.  https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270
%Para predecir si la segunda oración está realmente contigua en el texto original a la primera, es necesario que toda la secuencia de entrada pase por el codificador del modelo. La salida del modelo del primer token [CLS] es un vector de dimensiones 2 x 1 y utilizando una capa de clasificación simple que se añade a la arquitectura, se calcula la probabilidad de la contigüidad de oraciones con la función \textit{softmax}.

%Ambos procedimientos descritos se utilizan en conjunto durante el preentrenamiento del modelo con el objetivo de minimizar la función de pérdida combinada de ambas estrategias.

%La entrada final al modelo se denomina \textit{input embeddings}. Este vector de entradas se constituye con la suma de los \textit{token embeddings}, \textit{segment embeddings} y \textit{position embeddings}. El primero de ellos se refiere a los \textit{tokens} resultantes de aplicar el tokenizador con los \textit{tokens} especiales [CLS] y [SEP]. El segundo \textit{embedding} indica la separación de oraciones. Por último, el \textit{position embedding} señala la posición de cada una de las palabras en la secuencia de entrada. Este concepto se muestra en la figura ~\ref{fig:bert-embeddings}.


%\figura{Bitmap/05Prototipos/embeddings}{width=1\textwidth}{fig:bert-embeddings}%
%{Constitución de la entrada del modelo BERT}



%\subsection{Prueba de funcionamiento}\hfill

%A continuación vamos a comprobar el funcionamiento del modelo BERT. Concretamente, se evaluará el proceso de predicción de la palabra más probable dada una secuencia con alguna palabra enmascarada con el \textit{token} [MASK]. El primer ejemplo (código ~\ref{lst:tokenizer-bert}), muestra el resultado de tokenizar el texto de entrada. Para realizar este proceso se utiliza el tokenizador \textit{BertTokenizer} que internamente implementa el algoritmo \textit{WordPiece} descrito en uno de los apartados anteriores. Se puede observar que la palabra ``thunderous'' no se encuentra en el vocabulario del tokenizador y por tanto la descompone en dos \textit{tokens}: ``thunder'' y ``\#\#ous''. Para indicar que estos \textit{tokens} no pertenecen a palabras separadas utiliza la doble almohadilla (\textit{\#\#}) como prefijo en el segundo \textit{token}.

%\begin{lstlisting}[language=Python, caption=Resultado de aplicar \textit{BertTokenizer} a un texto de entrada, label={lst:tokenizer-bert}]
%	sequence = "The thunderous roar of the jet overhead confirmed 
%	her worst fears"
%	
%	>> ['The', 'thunder', '##ous', 'roar', 'of', 'the', 'jet', 
%	'overhead', 'confirmed', 'her', 'worst', 'fears']
%\end{lstlisting} 

%A continuación, se muestra el proceso de predicción de una palabra enmascarada en una secuencia utilizada como entrada al modelo (código ~\ref{lst:model-bert}). El modelo utilizado es \textit{BertForMaskeLM}, una versión especial de BERT para la realización exclusiva de esta tarea. El proceso seguido para la predicción es muy sencillo: primero se obtienen los \textit{input\_ids} de los \textit{tokens} que conforman la entrada y se codifican; a continuación se obtiene el índice de las palabras enmascaradas (en el ejemplo mostrado hay un solo \textit{token} [MASK]); una vez obtenida la salida del modelo dada la secuencia de entrada, se aplica la función \textit{softmask} y finalmente se filtran las cinco palabras con mayor probabilidad. El resultado es una oración coherente mediante la generación de una palabra acorde con su entorno.

%\begin{lstlisting}[language=Python, caption=Ejemplo de predicción de una palabra enmascarada en una %secuencia, label={lst:model-bert}]
%	text = "Every Monday, Mary goes to the " + tokenizer.mask_token + " to relax."
%	
%	>> Every Monday, Mary goes to the beach to relax.
%	Every Monday, Mary goes to the library to relax.
%	Every Monday, Mary goes to the bathroom to relax.
%	Every Monday, Mary goes to the lake to relax.
%	Every Monday, Mary goes to the gym to relax.
%\end{lstlisting} 


\section{T5 (\textit{Text-to-Text Transfer Transformer})}
El modelo T5, introducido por \citep{2020t5}, también es conocido como \textit{Text-to-Text Transfer Transformer} dado que se trata de un modelo basado en la arquitectura Transformer. Con 11 billones (americanos) de parámetros, se trata de uno de los modelos actuales de mayor tamaño. Su innovación frente a otros avances en la generación de lenguaje reside en la construcción de un solo modelo capaz de realizar diversas tares mediante la unión de varios sub-modelos. Cualquiera de las tareas para las que se concibe este sistema, ya sea traducción de texto, respuesta a preguntas, clasificación de texto o análisis de sentimientos, se proyecta alimentando al modelo con una entrada textual y entrenándolo para que produzca un texto de destino. 



\subsection{Arquitectura}\hfill

La arquitectura del modelo T5 sigue el enfoque tradicional del modelo Transfomer expuesto en la sección~\ref{sec:transformers}. Abandona la arquitectura que seguían sus predecesores GPT-2 y BERT, basada en bloques de decodificadores y codificadores, respectivamente, para recuperar el modelo \textit{encoder-decoder}.

Al igual que BERT, el primer paso para el entrenamiento es la conversión de nuestra secuencia de entrada a una secuencia de tokens para posteriormente ser mapeada a la \textit{sequence embedding} descrita en el apartado anterior. Una vez constituida la entrada final al modelo, se le puede dar paso al primer módulo, el codificador. 

El \textit{encoder} se constituye como una pila de bloques, en que el cada uno consta de dos componentes: una capa de autoatención (\textit{self-attention}) seguida de una red de retroalimentación (\textit{Feed-Forward Network}). Antes de cada uno de estos componentes se normalizan los datos empleando una versión simplificada de la capa de normalización original del modelo Transformer. Después de aplicar el proceso de normalización, una conexión de salto residual se agrega a la entrada de cada componente a su salida.

El otro módulo es el decodificador, cuya estructura es similar al codificador descrito anteriormente. La única diferencia notoria entre ambos componentes es la adición de un mecanismo de atención estándar después de cada capa de autoatención que atiende a la salida del codificador. El mecanismo de autoatención en el decodificador utiliza una forma de autoatención autorregresiva o causal, que limita al modelo a que únicamente preste atención a las salidas de instantes de tiempo pasados. La salida del bloque decodificador final alimenta a una capa formada por la función \textit{softmax}.



%\subsection{Pre-entrenamiento}\hfill


%Para entrenar el modelo T5 se creó una enorme corpus de datos llamado \textit{``Colossal Clean Crawled Corpus''} (\textit{C4}). Este conjunto de datos contiene 750GB de información en lengua inglesa extraída mediante \textit{web scrappping} de la web. Ya que el corpus original son 20TB de datos no revisados y podían incluir lenguaje ofensivo, código fuente, textos en otros idiomas... en resumen, texto que no interesa para el entrenamiento, se siguieron una serie de pautas muy precisas para eliminar toda esta información, resultando un corpus limpio libre de todo dato no necesario.

%Para realizar el preentrenamiento, se tiene en cuenta cada una de las tareas para las que se va a crear el modelo. Ya que se soporta en un enfoque de tipo \textit{Text-to-Text}, la entrada para cada una de las acciones a realizar será un texto, al igual que su salida. Para realizar tareas de traducción, sus autores precisan que la entrada al modelo fuese ``\textit{translate English to German: That is good.}'' en el caso de querer traducir del idioma inglés al alemán ``\textit{That is good}''. La salida del sistema sería ``\textit{Das ist gut.}''. En el caso de generación de resúmenes, la entrada estaría constituida por el texto a resumir seguida del texto ``\textit{TL;DR}'' (abreviación de \textit{too long, didn't read}). De esta manera se genera un resumen de un texto via decodificación autorregresiva. 

%Al igual que BERT, utiliza una entrada tokenizada como entrada al modelo. En el caso de T5, modifica el algoritmo de tokenización que utiliza el anterior modelo, basándose ahora en el algoritmo \textit{SentencePiece} que opera sobre regulación de subpalabras. Este algoritmo de tokenización, implementado en C++ es, increíblemente rápido, lo que resulta en un entrenamiento y generación muchísimo más veloz en comparación con los tokenizadores utilizados en GPT-2 (BPE) o BERT (WordPiece). Otra de las ventajas de este tokenizador es que se utiliza directamente sobre los datos sin la necesidad de almacenar los datos tokenizados en discos, por lo que utiliza menos memoria en el proceso. Por otra parte, es agnóstico respecto a los espacios en blanco, confiriendo a idiomas que en ocasiones no hacen uso de ellos, como el chino o japones, la misma facilidad de tokenización que a cualquier otro lenguaje. En general, se basa en la idea de que la codificación de pares de bytes no es óptima para el entrenamiento previo del modelo de lenguaje \citep{bostrom-durrett-2020-byte}.

%https://yukyunglee-github-io.translate.goog/Transformer-to-T5-4/?_x_tr_sl=auto&_x_tr_tl=es&_x_tr_hl=es&_x_tr_pto=wapp
%Para pre-entrenar el modelo se adoptó el método de enmascaramiento MLM. Sin embargo, la diferencia con el método existente utilizado en BERT reside en que los \textit{tokens} consecutivos se reemplazan con una máscara sin enmascarar un \textit{token} aleatorio. Específicamente, si antes a cada uno de las palabras enmascarados se les sustituía con el \textit{token} [MASK], este método los sustituye por <X>, <Y>, <Z>... y así sucesivamente hasta enmascarar todas las palabras introducidas. Estos \textit{tokens} se denominan \textit{tokens} centinela y tienen un tratamiento especial.


