{"nbformat":4,"nbformat_minor":0,"metadata":{"interpreter":{"hash":"f993294f222aa43e7d2ef032cb7262ce65d4bb70d5d6a0b5429ac56c13f0bb1e"},"kernelspec":{"display_name":"Python 3.9.7 64-bit (windows store)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4,"colab":{"name":"bert_en.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"QMe2ORSqw7sF"},"source":["# Transformers and BERT"]},{"cell_type":"code","metadata":{"id":"JNjFqi3yxHhH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636194358227,"user_tz":-60,"elapsed":72451,"user":{"displayName":"MARÍA CRISTINA ALAMEDA SALAS","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00427886758994799022"}},"outputId":"6d6dc761-26cf-4b9f-954b-7c0d60ab0feb"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"FPNaR3I5w7sM"},"source":["Primero, importamos GPT2LMHeadModel para la generatción de texto y GPT2Tokenizer como tokenizer del texto."]},{"cell_type":"code","metadata":{"id":"og7xPkF1w7sN","executionInfo":{"status":"ok","timestamp":1636194999068,"user_tz":-60,"elapsed":293,"user":{"displayName":"MARÍA CRISTINA ALAMEDA SALAS","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00427886758994799022"}}},"source":["from transformers import BertTokenizer, BertForMaskedLM\n","from torch.nn import functional as F\n","import torch"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DtKrOb5Kw7sQ"},"source":["A continuación, cargamos el tokenizer y se lo pasamos al modelo."]},{"cell_type":"code","metadata":{"id":"b1gifXyiw7sR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636195015174,"user_tz":-60,"elapsed":2739,"user":{"displayName":"MARÍA CRISTINA ALAMEDA SALAS","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00427886758994799022"}},"outputId":"2fa3d6b1-53ed-4146-9118-9fb4f5c5f16f"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","tokenizer('Hello world')"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [101, 7592, 2088, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"FAnCneUow7sT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636195017391,"user_tz":-60,"elapsed":2227,"user":{"displayName":"MARÍA CRISTINA ALAMEDA SALAS","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00427886758994799022"}},"outputId":"1db6a6fe-bda2-44d7-d630-ccd4b53d8a52"},"source":["model = BertForMaskedLM.from_pretrained('bert-base-uncased',return_dict = True)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"markdown","metadata":{"id":"joWq4ovHw7sU"},"source":["Después, para generar el texto, añadimos un primer texto a nuestro modelo y después a partir de él, generamos el texto. Antes de todo tenemos que preprocesar (tokenizar) ese primer texto que pasamos al modelo.\n","\n","\n","'pt' significa PyTorch Tensors\n","\n","Con endode pasamos de texto a números y con decode pasamos de números a texto.\n","\n","**ENCODE**\n","\n","Ponemos el truncation a True porque a este tokenizer solo puede gestionar 512 tokens de una vez.\n","\n","También podemos utilizar la función encode_plus que devuelve más información."]},{"cell_type":"code","metadata":{"id":"VM17olzFw7sV","executionInfo":{"status":"ok","timestamp":1636195645292,"user_tz":-60,"elapsed":3582,"user":{"displayName":"MARÍA CRISTINA ALAMEDA SALAS","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00427886758994799022"}}},"source":["text = \"Every Monday, Mary goes to the \" + tokenizer.mask_token + \" to relax.\"\n","input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n","mask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)"],"execution_count":53,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hlUO7Ltow7sZ"},"source":["Para finalizar, generamos el texto a partir del modelo."]},{"cell_type":"code","metadata":{"id":"D2eJe4lFw7sZ","executionInfo":{"status":"ok","timestamp":1636195646388,"user_tz":-60,"elapsed":15,"user":{"displayName":"MARÍA CRISTINA ALAMEDA SALAS","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00427886758994799022"}}},"source":["output = model(**input)"],"execution_count":54,"outputs":[]},{"cell_type":"code","metadata":{"id":"s7vRtYl9w7sa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636195646389,"user_tz":-60,"elapsed":13,"user":{"displayName":"MARÍA CRISTINA ALAMEDA SALAS","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00427886758994799022"}},"outputId":"e96c0120-9257-4cc7-eb21-65dc2be136d6"},"source":["logits = output.logits\n","softmax = F.softmax(logits, dim = -1)\n","mask_word = softmax[0, mask_index, :]\n","top_10 = torch.topk(mask_word, 10, dim = 1)[1][0]\n","for token in top_10:\n","   word = tokenizer.decode([token])\n","   new_sentence = text.replace(tokenizer.mask_token, word)\n","   print(new_sentence)"],"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Every Monday, Mary goes to the beach to relax.\n","Every Monday, Mary goes to the spa to relax.\n","Every Monday, Mary goes to the hospital to relax.\n","Every Monday, Mary goes to the gym to relax.\n","Every Monday, Mary goes to the pool to relax.\n","Every Monday, Mary goes to the library to relax.\n","Every Monday, Mary goes to the hotel to relax.\n","Every Monday, Mary goes to the bathroom to relax.\n","Every Monday, Mary goes to the park to relax.\n","Every Monday, Mary goes to the house to relax.\n"]}]}]}