{"nbformat":4,"nbformat_minor":0,"metadata":{"interpreter":{"hash":"f993294f222aa43e7d2ef032cb7262ce65d4bb70d5d6a0b5429ac56c13f0bb1e"},"kernelspec":{"display_name":"Python 3.9.7 64-bit (windows store)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4,"colab":{"name":"bert_en.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"QMe2ORSqw7sF"},"source":["# Prueba de BERT"]},{"cell_type":"markdown","source":["## Descarga e instalación de recursos"],"metadata":{"id":"EXbdMuQzoZkO"}},{"cell_type":"markdown","metadata":{"id":"FPNaR3I5w7sM"},"source":["Para realizar las pruebas sobre BERT vamos a necesitar una serie de recursos que nos permitan acceder a su funcionalidad.\n","\n","La biblioteca que utilizaremos para acceder a ellos será la biblioteca Transformers de HuggingFace. Este módulo contiene una serie de funcionalidades que permiten realizar funciones de procesamiento de lenguaje, en nuestro caso concreto, generación de lenguaje natural."]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yZTfwKnLBZ43","executionInfo":{"status":"ok","timestamp":1651912087860,"user_tz":-120,"elapsed":15071,"user":{"displayName":"MARÍA CRISTINA ALAMEDA SALAS","userId":"00427886758994799022"}},"outputId":"2fd609d3-b069-4580-be7c-6ad726b5e323"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 43.0 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 4.0 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 2.7 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 47.0 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=22fe89b5b0b0f604a8528d0f236f417c515de17a998fbcce59502c2384d9640b\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.18.0\n"]}]},{"cell_type":"markdown","source":["## Bibliotecas necesarias"],"metadata":{"id":"yr0PC0ZNoelx"}},{"cell_type":"markdown","source":["A continuación vamos a importar los módulos necesarios para probar el funcionamiento de bert.\n","\n","En nuestro caso utilizaremos, el módulo *BertForMaskedLM* que implementa el modelo de lenguaje BERT para lenguaje enmascrado y *BertTokenizer* utilizado para el proceso de tokenización."],"metadata":{"id":"AC9QI3HRohLR"}},{"cell_type":"code","metadata":{"id":"og7xPkF1w7sN","executionInfo":{"status":"ok","timestamp":1651912243653,"user_tz":-120,"elapsed":6946,"user":{"displayName":"MARÍA CRISTINA ALAMEDA SALAS","userId":"00427886758994799022"}}},"source":["from transformers import BertTokenizer, BertForMaskedLM\n","from torch.nn import functional as F\n","import torch"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Tokenizer"],"metadata":{"id":"ruwSO-Gnopb5"}},{"cell_type":"markdown","source":["El primer paso para poder probar el modelo es convertir los datos de entrada a una secuencia de identificdores de tokens o *tokens ids* comprensibles por el modelo de lenguaje. Utilizaremos la version 'bert-case-uncased' del tokenizador *BertTokenizer* para realizar este proceso."],"metadata":{"id":"ceWBcBWiorFx"}},{"cell_type":"code","metadata":{"id":"b1gifXyiw7sR","executionInfo":{"status":"ok","timestamp":1651913646807,"user_tz":-120,"elapsed":831,"user":{"displayName":"MARÍA CRISTINA ALAMEDA SALAS","userId":"00427886758994799022"}}},"source":["tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"],"execution_count":105,"outputs":[]},{"cell_type":"markdown","source":["En la siguiente oración, se puede comprobar como separa el tokenizador cada una de las palabras atendiendo a los subtokens de los que están compuestas. De esta manera, 'thunderous' se compone de 'thunder' y '##ous'"],"metadata":{"id":"-DY7J3HEpNdu"}},{"cell_type":"code","source":["sequence = \"The thunderous roar of the jet overhead confirmed her worst fears\"\n","tokenized_sequence = tokenizer.tokenize(sequence)\n","print(tokenized_sequence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pyYD40LPpHWK","executionInfo":{"status":"ok","timestamp":1651913646808,"user_tz":-120,"elapsed":6,"user":{"displayName":"MARÍA CRISTINA ALAMEDA SALAS","userId":"00427886758994799022"}},"outputId":"a334362f-e0d3-44ba-e3c7-7251c3999582"},"execution_count":106,"outputs":[{"output_type":"stream","name":"stdout","text":["['the', 'thunder', '##ous', 'roar', 'of', 'the', 'jet', 'overhead', 'confirmed', 'her', 'worst', 'fears']\n"]}]},{"cell_type":"markdown","source":["A continuación, comprobamos la salida de este proceso de tokenización. Los resultados devueltos por este procedimiento son los *input_ids* correspondientes de la secuencia, *token_type_ids* y *attention_mask* o máscara de atención."],"metadata":{"id":"vXq4yNUXozrq"}},{"cell_type":"code","source":["tokenizer('Hello world?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l11LLBUZBYVR","executionInfo":{"status":"ok","timestamp":1651913647147,"user_tz":-120,"elapsed":5,"user":{"displayName":"MARÍA CRISTINA ALAMEDA SALAS","userId":"00427886758994799022"}},"outputId":"afe5fb8f-0a44-406a-cdab-ced0e4af9283"},"execution_count":107,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [101, 7592, 2088, 1029, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"]},"metadata":{},"execution_count":107}]},{"cell_type":"code","source":["sequence = \"Hello world?\"\n","tokenized_sequence = tokenizer.tokenize(sequence)\n","print(tokenized_sequence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QabnXaphGBBu","executionInfo":{"status":"ok","timestamp":1651913647724,"user_tz":-120,"elapsed":4,"user":{"displayName":"MARÍA CRISTINA ALAMEDA SALAS","userId":"00427886758994799022"}},"outputId":"7db672cc-3f52-45d6-a0a7-86eecf4890a7"},"execution_count":108,"outputs":[{"output_type":"stream","name":"stdout","text":["['hello', 'world', '?']\n"]}]},{"cell_type":"markdown","source":["## Carga del modelo"],"metadata":{"id":"sjsPSvrZp7rz"}},{"cell_type":"markdown","source":["Primero descargamos el modelo concreto que vamos a utilizar. En nuestro caso, emplearemos la versión 'bert-base-uncased' del modelo."],"metadata":{"id":"fDcNspZnp-U8"}},{"cell_type":"code","metadata":{"id":"FAnCneUow7sT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651913655320,"user_tz":-120,"elapsed":2729,"user":{"displayName":"MARÍA CRISTINA ALAMEDA SALAS","userId":"00427886758994799022"}},"outputId":"04123039-7e5d-4a48-e311-1349a1807db8"},"source":["model = BertForMaskedLM.from_pretrained('bert-base-uncased',return_dict = True)"],"execution_count":110,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"markdown","source":["## Resultados"],"metadata":{"id":"9BYm28SVsVsK"}},{"cell_type":"markdown","metadata":{"id":"joWq4ovHw7sU"},"source":["A continuación, se comprobará un ejemplo concreto. En este caso estamos utilizando BERT para generación de lenguaje enmascarado, así que dada una oración, ocultaremos alguna de sus palabras bajo el token de máscara del tokenizador y le pasaremos al modelo esta entrada tokenizada."]},{"cell_type":"code","source":["tokenizer.mask_token"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"MfgQqPK5qyf7","executionInfo":{"status":"ok","timestamp":1651913655893,"user_tz":-120,"elapsed":7,"user":{"displayName":"MARÍA CRISTINA ALAMEDA SALAS","userId":"00427886758994799022"}},"outputId":"523f6941-4fa1-4094-9bbe-06200e339653"},"execution_count":111,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'[MASK]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":111}]},{"cell_type":"code","source":["tokenizer.mask_token_id"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FFEpJpU0q2_i","executionInfo":{"status":"ok","timestamp":1651913656265,"user_tz":-120,"elapsed":3,"user":{"displayName":"MARÍA CRISTINA ALAMEDA SALAS","userId":"00427886758994799022"}},"outputId":"eac9ed32-7730-4a5f-fe7c-0dcf98d42e8e"},"execution_count":112,"outputs":[{"output_type":"execute_result","data":{"text/plain":["103"]},"metadata":{},"execution_count":112}]},{"cell_type":"code","metadata":{"id":"VM17olzFw7sV","executionInfo":{"status":"ok","timestamp":1651913657933,"user_tz":-120,"elapsed":283,"user":{"displayName":"MARÍA CRISTINA ALAMEDA SALAS","userId":"00427886758994799022"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"41fdfc33-edce-4738-95ce-92cf195cd0cb"},"source":["text = \"Every Monday, Mary goes to the \" + tokenizer.mask_token + \" to relax.\"\n","input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n","# indice del token [MASK]\n","mask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)\n","print(f'index:  {int(mask_index[0][0])}')"],"execution_count":113,"outputs":[{"output_type":"stream","name":"stdout","text":["index:  8\n"]}]},{"cell_type":"code","source":["tokenizer.decode(input['input_ids'][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"cO46PZdbrgEH","executionInfo":{"status":"ok","timestamp":1651913658236,"user_tz":-120,"elapsed":5,"user":{"displayName":"MARÍA CRISTINA ALAMEDA SALAS","userId":"00427886758994799022"}},"outputId":"97665f2a-4b5b-47e2-e23e-251215c03eb4"},"execution_count":114,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'[CLS] every monday, mary goes to the [MASK] to relax. [SEP]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":114}]},{"cell_type":"code","source":["input"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"09olkhjsJTh-","executionInfo":{"status":"ok","timestamp":1651913658491,"user_tz":-120,"elapsed":4,"user":{"displayName":"MARÍA CRISTINA ALAMEDA SALAS","userId":"00427886758994799022"}},"outputId":"ee612965-4a89-4add-91d2-e46e010cf51f"},"execution_count":115,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[ 101, 2296, 6928, 1010, 2984, 3632, 2000, 1996,  103, 2000, 9483, 1012,\n","          102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"]},"metadata":{},"execution_count":115}]},{"cell_type":"markdown","metadata":{"id":"hlUO7Ltow7sZ"},"source":["Para finalizar, generamos el texto a partir del modelo."]},{"cell_type":"code","metadata":{"id":"D2eJe4lFw7sZ","executionInfo":{"status":"ok","timestamp":1651913659441,"user_tz":-120,"elapsed":386,"user":{"displayName":"MARÍA CRISTINA ALAMEDA SALAS","userId":"00427886758994799022"}}},"source":["output = model(**input)"],"execution_count":116,"outputs":[]},{"cell_type":"code","source":["logits = output.logits\n","print(logits)\n","softmax = F.softmax(logits, dim = -1)\n","print(softmax)\n","mask_word = softmax[0, mask_index, :]\n","print(mask_word)\n","top_5 = torch.topk(mask_word, 5, dim = 1)[1][0]\n","for token in top_5:\n","   word = tokenizer.decode([token])\n","   new_sentence = text.replace(tokenizer.mask_token, word)\n","   print(new_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oOq9RB6LJeL_","executionInfo":{"status":"ok","timestamp":1651913660123,"user_tz":-120,"elapsed":342,"user":{"displayName":"MARÍA CRISTINA ALAMEDA SALAS","userId":"00427886758994799022"}},"outputId":"23a328dd-6eee-4137-a06c-989e7998f0bd"},"execution_count":117,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ -6.7618,  -6.7374,  -6.7210,  ...,  -6.0865,  -5.9108,  -4.0268],\n","         [ -8.7497,  -8.9794,  -9.1654,  ...,  -8.2566,  -6.8753,  -6.5967],\n","         [ -7.8636,  -8.0510,  -7.8461,  ...,  -6.3059,  -5.4562,  -7.7335],\n","         ...,\n","         [ -9.0204,  -9.4107,  -9.2717,  ...,  -8.7517,  -8.8832,  -5.2407],\n","         [-10.6601, -10.3545, -10.6067,  ...,  -8.4040,  -9.4216,  -6.8385],\n","         [-14.5783, -14.8022, -14.7373,  ..., -14.2305, -11.7988,  -8.1406]]],\n","       grad_fn=<AddBackward0>)\n","tensor([[[4.5826e-07, 4.6954e-07, 4.7731e-07,  ..., 9.0031e-07,\n","          1.0731e-06, 7.0611e-06],\n","         [9.3756e-14, 7.4516e-14, 6.1865e-14,  ..., 1.5352e-13,\n","          6.1099e-13, 8.0731e-13],\n","         [2.0734e-12, 1.7190e-12, 2.1099e-12,  ..., 9.8439e-12,\n","          2.3024e-11, 2.3614e-12],\n","         ...,\n","         [3.2964e-12, 2.2312e-12, 2.5640e-12,  ..., 4.3124e-12,\n","          3.7814e-12, 1.4439e-10],\n","         [9.2773e-16, 1.2593e-15, 9.7859e-16,  ..., 8.8554e-15,\n","          3.2010e-15, 4.2373e-14],\n","         [2.0182e-11, 1.6135e-11, 1.7216e-11,  ..., 2.8578e-11,\n","          3.2515e-10, 1.2614e-08]]], grad_fn=<SoftmaxBackward0>)\n","tensor([[8.2475e-08, 8.9671e-08, 9.8373e-08,  ..., 1.5744e-07, 9.0582e-08,\n","         7.8500e-08]], grad_fn=<IndexBackward0>)\n","Every Monday, Mary goes to the beach to relax.\n","Every Monday, Mary goes to the spa to relax.\n","Every Monday, Mary goes to the hospital to relax.\n","Every Monday, Mary goes to the gym to relax.\n","Every Monday, Mary goes to the pool to relax.\n"]}]},{"cell_type":"code","metadata":{"id":"s7vRtYl9w7sa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651913660724,"user_tz":-120,"elapsed":4,"user":{"displayName":"MARÍA CRISTINA ALAMEDA SALAS","userId":"00427886758994799022"}},"outputId":"1b49e05c-fbcf-43a1-f6ca-06d5f0d28540"},"source":["logits = output.logits\n","softmax = F.softmax(logits, dim = -1)\n","mask_word = softmax[0, mask_index, :]\n","top_5 = torch.topk(mask_word, 5, dim = 1)[1][0]\n","for token in top_5:\n","   word = tokenizer.decode([token])\n","   new_sentence = text.replace(tokenizer.mask_token, word)\n","   print(new_sentence)"],"execution_count":118,"outputs":[{"output_type":"stream","name":"stdout","text":["Every Monday, Mary goes to the beach to relax.\n","Every Monday, Mary goes to the spa to relax.\n","Every Monday, Mary goes to the hospital to relax.\n","Every Monday, Mary goes to the gym to relax.\n","Every Monday, Mary goes to the pool to relax.\n"]}]},{"cell_type":"markdown","source":["En los resultados anteriores podemos comprobar las 5 palabras que mayor probabilidad tienen de encajar en esta palabra oculta de la secuencia de entrada. En este orden, serían: \n","\n","  * beach\n","  * spa\n","  * hospital\n","  * gym\n","  * pool"],"metadata":{"id":"Q-PQEPWLsdsw"}}]}