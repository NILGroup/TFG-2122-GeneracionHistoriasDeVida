\chapter{Análisis del problema y especificación de requisitos}
\label{cap:analisisYRequisitos}

Evgeny Morazov, en su obra \citep{morozov2015la} manifiesta que ``todos se apresuran a  celebrar la victoria, pero nadie recuerda qué pretendía conseguir''. Con estas palabras se pretende exponer la importancia de una buena investigación previa y análisis. En este capítulo se abordarán diferentes problemas a tener en cuenta antes de hacer frente la construcción de un sistema para la consecución de los objetivos expuestos. Primero se describirá la dificultad de los terapeutas para la composición íntegra de una historia de vida. Así mismo, se abordaran una serie problemas implicados en la generación deficiente de textos relacionados con la utilización de aproximaciones neuronales para dicha generación.



\section{Dificultad de composición de historias de vida}
%Hablar sobre que es muy dificil por una persona crear una historia de vida

%Hablar de que apenas se han encontrado soluciones tecnológicas que ayuden

Las historias de vida tradicionalmente tienen un formato en papel. Esto supone una serie de limitaciones obvias. Por una parte, encontramos la necesidad de almacenamiento de una gran cantidad de información. Ya que se trata de recopilar la mayor información posible respecto a la vida completa de una persona, después del proceso de extracción de datos, es necesario almacenarla en algún lugar. De manera tradicional, suele apuntarse toda la información en libros o si se opta por una decisión más innovadora, en formato electrónico. Además, la gestión de todo este contenido es muy complicado. 
Por otra parte, como paso previo a la escritura de la historia de vida, es necesario realizar una selección adecuada de los datos que se van a incluir sin dejar ninguna información relevante atrás. También hace falta concebir la estructura del escrito y que datos corresponden a cada una de dichas partes.

Si se consigue hacer este proceso de manera satisfactoria, se puede comenzar con la redacción de la historia de vida. Quienes escriben estas historias son los propios terapeutas y como escribir no es para nada simple, pueden tener ciertas dificultades a la hora de buscar combinaciones perfectas de palabras que expresen ciertos conceptos, perder tiempo leyendo una y otra vez una misma oración que no parece convencerle o simplemente quedarse sin ideas.

Por otra parte, la existencia de sistemas que permitan ayudarles en la tarea de clasificación de datos y redacción es prácticamente nula. Con los enfoques adecuados de generación de lenguaje podría llegar a construirse un sistema capaz de ello pero todavía existen ciertas limitaciones.



\section{Problemas en la generación de lenguaje}

Con el avance de los modelos de generación de lenguaje natural, se ha empezado a prestar más atención a las limitaciones y riesgos potenciales de este tipo de sistemas. Los sistemas más modernos y en los que los investigadores fijan principalmente su atención son modelos de \textit{Deep Learning} basados esencialmente en redes neuronales profundas que han sido capaz de mejorar drásticamente la calidad de generación de lenguaje respecto a otros sistemas anteriores. Sin embargo, junto con estas mejoras, debido a las características intrínsecas de estos modelos computacionales, estos modelos son más propensos a fenómenos que conllevan una generación errónea de textos. Por una parte la llamada \textit{degeneración} produce salidas incoherentes o atascada en bucles repetitivos de palabras o expresiones. Otros modelos GLN en algunas ocasiones generan textos de salida sin sentido alguno o con datos para nada respaldados en la información introducida como entrada. Este fenómeno es conocido como \textit{alucinación} y perjudica seriamente la aplicabilidad de los modelos neuronales de generación de lenguaje en casos prácticos donde la precisión de la información es vital y el nivel de tolerancia hacia las alucinaciones es nulo.

\subsection{Alucinaciones}

Con \textit{alucinación} nos referimos al fenómeno en el que un modelo, especialmente de tipo neuronal ``\textit{end-to-end}'', produce información de salida que no es fiel a los datos provistos como entrada al sistema. 

%Ejemplos: Caso de imagenes, caso de texto
Este fenómeno se da en una diversidad de sistemas condicionales de generación de lenguaje. \cite{hallucinations_data2text} en su artículo, \textit{Controlling Hallucinations at Word Level in Data-to-Text Generation} destaca la existencia de alucinaciones en la generación \textit{data-to-text} en un modelo neuronal entrenado a partir de bases de datos como \textit{Totto} \citep{parikh-etal-2020-totto}. La entrada al sistema es una tabla. Una vez generado el texto de salida se puede comprobar que la palabra ``Italian'' a la que denomina \textit{enunciado divergente} no es respaldada por los datos de entrada (figura~\ref{fig:d2thallucinations}).

Por otro lado, \cite{rohrbach-etal-2018-object} subraya la existencia de alucinaciones en la generación de descripciones de imágenes. Estos tipos de sistemas se componen de dos modelos diferenciados. Por una parte, un modelo de predicción de imagen que trata de extraer los objetos de la misma y por otra, un modelo de predicción de lenguaje basado en la probabilidad de la siguiente palabra a generar. De esta forma, se analizaron las diferencias de predicción entre ambos modelos (figura~\ref{fig:imagehallucinations}) y llegaron a la conclusión de que en la mayoría de los casos la descripción generada se basaba principalmente en el modelo de lenguaje con el objetivo de conseguir una descripción más consistente semántica y sintácticamente. En el caso de estudio, la imagen sirve de entrada al sistema y se comprueba la predicción de ambos modelos nombrados anteriormente para la última palabra a generar. Mientras que el modelo de imagen predice palabras como ``bol'', ``brocoli'' o ``zanahoria'', el modelo de lenguaje propone ``tenedor'', ``cuchara'' o ``bol''. Finalmente, la descripción generada utiliza ``tenedor'' para completar la frase aunque no aparece en la imagen produciéndose una alucinación.


\begin{figure}[t]
	\centering
	%
	\begin{SubFloat}
		{\label{fig:d2thallucinations}%
			Alucinaciones en generación DT2}%
		\includegraphics[width=0.7\textwidth]%
		{Imagenes/Bitmap/04Analisis/alucinacion_d2t}%
	\end{SubFloat}
	\qquad
	\begin{SubFloat}
		{\label{fig:imagehallucinations}%
		 Alucinaciones en generación de descripciones de imágenes}%
		\includegraphics[width=0.8\textwidth]%
		{Imagenes/Bitmap/04Analisis/alucinacion_image}%
	\end{SubFloat}
	\caption{Alucinaciones en distintos sistemas%
		\label{fig:hallucinations}}
\end{figure}


%Caso especial medico -> mucha precision y privacidad

%Tipos de alucinaciones: intrinsecas| extrinsecas

\subsubsection{Tipos de alucinaciones}
Aunque nos referimos a las alucinaciones de manera general como datos generados erróneamente. Atendiendo al resultado de la generación y por tanto a las consecuencias que puede tener esta generación, \citep{hallucination_survey} distingue dos tipos de alucinaciones.

Con \textit{alucinaciones intrínsecas} (figura~\ref{fig:inthallucinations})se refiere a la generación de textos de salida que contradicen los datos de entrada. Mientras que las \textit{alucinaciones extrínsecas} (figura~\ref{fig:exthallucinations}) son aquellas que generan una salida que no puede ser verificada a partir de los datos de entrada. Ambos tipos de alucinaciones generan datos no respaldados por la información que constituye los datos de entrada. Sin embargo, este último tipo de alucinaciones no siempre genera una salida errónea ya que no se puede asegurar que los datos generados sean incorrectos.


\begin{figure}[t]
	\centering
	%
	\begin{SubFloat}
		{\label{fig:inthallucinations}%
			Alucinación intrínseca}%
		\includegraphics[width=0.7\textwidth]%
		{Imagenes/Bitmap/04Analisis/alucinacion_intrinseca}%
	\end{SubFloat}
	\qquad
	\begin{SubFloat}
		{\label{fig:exthallucinations}%
			Alucinación extrínseca}%
		\includegraphics[width=0.7\textwidth]%
		{Imagenes/Bitmap/04Analisis/alucinacion_extrinseca}%
	\end{SubFloat}
	\caption{Tipos de alucinaciones%
		\label{fig:hallucinations_types}}
\end{figure}

%Tipos de alucinaciones: A partir de datos por los datos de entrenamiento e inferencia

Si nos preguntamos el porqué de la existencia de las alucinaciones cuando se introducen unos datos de entrada a un sistema entrenado, potencialmente bajo un modelo de red neuronal, de manera satisfactoria y haciendo uso de una base de datos que nos permite realizar la tarea que tenemos como objetivo; tenemos que tener en cuenta las posibles causas origen que generan este problema.

La generación errónea de los datos de salida, según \cite{hallucination_survey}, puede deberse a una divergencia en los datos utilizados para entrenar el modelo. Esta divergencia aparece cuando la relación entre los datos fuente-referencia está mal construida. Aunque el modelo base funcione correctamente, el modelo que ha sido entrenado bajo esta base de datos con divergencias puede alentar a generar una salida que no es fiel a los datos proporcionados como entrada. Otro escenario problemático emerge con la existencia de ejemplos de datos del conjunto duplicados y que han filtrados de una manera incorrecta. Cada vez más, los corpus de texto se incrementan en tamaño con el paso del tiempo y debido a la imposibilidad de revisión humana de todos estos grandes conjuntos de datos, pierden calidad respecto a los corpus más pequeños. \cite{lee2021deduplicating} afirma que el 10\% de los ejemplos de las bases de datos más empleadas en generación de lenguaje natural están repetidas en numerosas ocasiones. También destaca, que cuando estos ejemplos de datos duplicados pertenecientes a un conjunto se utilizan para entrenar un sistema, sesga el modelo para favorecer la generación de estas frases duplicadas. Si además también participaran de divergencias en entre la fuente y la referencia de los datos, la existencia de alucinaciones se multiplicaría.


Otras de las razones de la existencia de las alucinaciones, corresponde a las características propias del modelo de red neuronal. Aún partiendo de una base de datos perfecta, sin duplicados ni divergencias algunas, las opciones de entrenamiento y modelado de estos sistemas influirían generando textos de salida incorrectos. Por una parte, la incapacidad de comprensión del modelo de los datos de entrada debido a la generación de correlaciones incorrectas entre las diferentes partes de los datos de entrenamiento por parte del codificador, puede conllevar a un mal aprendizaje por parte del modelo. Así mismo, la estrategia de decodificación utilizada, correspondiendo en estos casos la elección a estrategias que añaden aleatoriedad o diversidad en la generación, están relacionadas directamente con el incremento de las alucinaciones.

\subsection{Degeneración}

Algunos modelos de redes neuronales conocidos, como GPT-2, se basan en la aleatoriedad de la salida como su objetivo principal frente a la maximización de la probabilidad. Esto se debe a que buscan la mayor similitud en la generación entre el procesamiento textual de la información por parte de un sistema y un humano. Para conseguir esta diversidad en la salida de la generación, se hace uso de estrategias de decodificación aleatorias, ya que las estrategias que buscan la maximización de la probabilidad para obtener mayores puntuaciones de similitud, especialmente en el caso de los textos largos, con frecuencia abocan a textos con repetitivos e incoherentes. Un ejemplo de estrategia de decodificación que alienta a degeneraciones es \textit{Beam Search}. En la figura~\ref{fig:degeneracion}, se muestra el texto de salida generado por GPT-2 que utiliza esta estrategia de decodificación. Destaca en color azul las repeticiones producidas en la salida, claro ejemplo de degeneración.

\figura{Bitmap/04Analisis/degeneracion}{width=.7\textwidth}{fig:degeneracion}%
{Ejemplo de degeneración con Beam Search }

\subsection{Falta de representación de los datos de entrada}

Esta limitación podría considerarse justo la contraria a las alucinaciones. Si en esta última se generaba mayor información de la proporcionada en los datos de entrada, también se da el caso de falta de representación de alguno o todos los datos de entrada. Se trata de un problema igual de grave que si se generaran datos erróneos (como las alucinaciones intrínsecas) en las que en el caso de una resolución médica, una mala interpretación de los datos de entrada podría llegar a poner en peligro la vida de una persona. En este caso, dependiendo del grado en que no aparezcan representados en la salida los datos introducidos como entrada, se tendría un impacto de diferente gravedad. Si unos datos muy importantes (en el ejemplo del caso médico) no se tuvieran en cuenta, podría llegarse también a un diagnóstico potencialmente diferente al que se generaría si se hubiera incluido este dato. Así mismo si el número de datos no introducidos fuera elevado. En el caso de no aparición en la salida de datos poco relevantes o de perder poca información, el impacto que supondría sería leve.