\chapter{Estado de la Cuestión}
\label{cap:estadoDeLaCuestion}


\section{Alzheimer e historias de vida}
La pirámide de población modifica su estructura continuamente debido al progresivo envejecimiento de población generalizado. Según \cite{estalz}, en el año 2050 las personas mayores de 65 años constituirán el 16\% de la población mundial frente al 8\%  del año 2010. Una de las enfermedades más comunes dentro de este rango de población es la enfermedad del Alzheimer, cuya prevalencia a nivel global se espera que supere todo dato conocido hasta ahora, ya que se estima que en 2050 se incremente el número de casos a 152.8 millones frente a los 57.4 millones de 2019. 

\figura{Vectorial/03EstadoDeLaCuestion/CerebroPersonaAlzheimer}{width=.7\textwidth}{fig:CerebroPersonaAlzheimer}%
{Reducción del cerebro asociada al Alzheimer, \citet{mattson2004pathways}}

La enfermedad de Alzheimer es un trastorno neurológico caracterizado por cambios degenerativos en diferentes sistemas neurotransmisores que abocan finalmente a la muerte de las células nerviosas del cerebro encargadas del almacenamiento y procesamiento de la información. Las regiones del cerebro involucradas en la memoria y procesos de aprendizaje, asociadas a los lóbulos temporal y frontal, reducen su tamaño como resultado de la degeneración de las sinapsis y la muerte de las neuronas \citep{romano2007enfermedad,mattson2004pathways}. En la figura ~\ref{fig:CerebroPersonaAlzheimer} se puede comprobar de manera visual esta reducción del cerebro.

Es una enfermedad difícil de detectar ya que, por lo general, los síntomas iniciales de la enfermedad suelen atribuirse a un olvido puntual o la vejez. Nada más lejos de la realidad. Según avanza la enfermedad, sus síntomas lo hacen con ella, agravándose y aumentando cada vez más hasta que el deterioro cognitivo ocasionado llega a afectar significativamente a las actividades de la vida diaria y finalmente a las necesidades fisiológicas básicas.

La evolución del Alzheimer se puede dividir en tres fases o etapas: Inicial, en la que se observa un deterioro cognitivo leve como puede ser la pérdida paulatina de la memoria episódica. Intermedia, con pérdidas de la memoria reciente asociada a un deterioro mayor. Avanzada, en la que se pierde progresivamente la memoria de los acontecimientos más antiguos, acompañando también un gran deterioro físico.

Los síntomas del Alzheimer son muy diversos, ya que no solo provoca problemas de memoria, sino también alteraciones en el estado de ánimo y la conducta, dificultad de toma de decisiones, desorientación, problemas del lenguaje, dificultad para comer y movilidad reducida, entre otros. Estos síntomas dependen de la fase evolutiva de la enfermedad.

En la actualidad, el Alzheimer es una enfermedad irreversible. Sin embargo, existen diversos tratamientos que pretenden ralentizar el avance de la enfermedad y/o mejorar la calidad de vida de los pacientes. Podemos dividir estos tratamientos en dos ramas: tratamientos farmacológicos, que hacen uso de medicamentos, y tratamientos no farmacológicos o psicosociales, que no utilizan sustancias químicas. Ambos tratamientos son eficaces y de la combinación de ambos resulta el procedimiento más recomendado para tratar la enfermedad.\citep{romano2007enfermedad}

Existen una gran variedad de terapias no farmacológicas. Algunas de las más utilizadas son el entrenamiento y estimulación cognitiva, ejercicio físico, musicoterapia, etc. Además, en cada una de estas terapias podemos encontrar una enorme cantidad de técnicas, siendo la reminiscencia la más utilizada como terapia de estimulación cognitiva.

Según \cite{o2013cross}, la reminiscencia es el acto o proceso de recordar sucesos, eventos o información del pasado. Esto puede implicar el recuerdo de episodios particulares o genéricos que pueden o no haber sido olvidados previamente, y que son acompañados por la sensación de que estos episodios son relatos verídicos de las experiencias originales. Esta técnica es utilizada para estimular la memoria episódica autobiográfica mediante el encadenamiento de recuerdos, que se agrupan en categorías y se archivan en el tiempo mediante la elaboración de \textit{la historia de vida}.

La historia de vida es una técnica narrativa que se basa en organizar y estructurar recuerdos de una persona para componer una autobiografía. Según \cite{linde1993life}, una historia de vida debe cumplir dos criterios: primero, debe incluir algunos puntos de evaluación que comuniquen los valores morales de la persona; y segundo, los eventos incluidos en la historia de vida deben tener un significado especial y ser de importancia para ella. Estos eventos deben ser aspectos significativos de la vida pasada de la persona, su presente y su futuro.

Para componer la historia de vida de una persona con Alzheimer se recopilan historias a través de familiares u otras personas cercanas. Posteriormente, se documentan en forma de un libro o cuaderno, incluyendo experiencias y logros junto con fotografías y escritos sobre hechos importantes para la vida de la persona, a través de los cuales se muestra quién es esa persona.

Las historias de vida ayudan a las personas con Alzheimer a conectar con su identidad recordando épocas felices. El miedo y la frustración provocados por el olvido de las tareas de la vida cotidiana, nombres y rostros, se mitigan recordando quiénes eran a través de estas historias. Les ayuda a ser conscientes de los momentos especiales que han marcado su vida, las personas que han conocido en su infancia o trabajo. También pueden ser utilizados por los cuidadores para saber más de ellos y ayudarles en la reminiscencia de recuerdos. \citep{karlsson2014stories}



\section{Generación de lenguaje natural}

La Generación de Lenguaje Natural (GLN) se define como el ``subcampo de la inteligencia artificial y lingüística computacional que se ocupa de la construcción de sistemas informáticos que pueden producir textos comprensibles en inglés u otros lenguajes humanos a partir de alguna representación no lingüística subyacente de la información''  \citep{reiter1997building}. Si bien esta definición estuvo generalmente aceptada como la más conveniente al hablar de generación de lenguaje natural durante muchos años, \cite{gatt2018survey} puntualizan que es una afirmación que solo engloba una parte de la generación de textos, ya que se refiere únicamente a aquellos sistemas cuya entrada es una 'representación no lingüística [...] de la información" o datos, como veremos más adelante en el apartado \ref{cap:ngltiposentrada}.

Desde hace muchos años, el GNL es empleado en numerosos proyectos de distinta naturaleza como la traducción de textos \citep{Cho2014LearningPR}, realización de resúmenes y fusión de documentos \citep{clarke2010discourse}, corrección automática de ortografía y gramática \citep{Kukich1992TechniquesFA}, redacción de noticias \citep{molina2011generating}, informes meteorológicos \citep{goldberg1994using}, informes financieros \citep{Kukich1983DesignOA}, generación de resúmenes sobre la información de recien nacidos en un contexto clínico \citep{BabyTalk}... Todos estos sistemas construidos tienen en común la generación de un texto (normalmente de una alta calidad) a partir de muy diferentes fuentes de información. 

\subsection{Clasificación de sistemas NGL según su entrada}\label{cap:ngltiposentrada}
En los ejemplos de proyectos listados con anterioridad en los que se empleó generación de lenguaje natural para redactar distintos textos, podemos percatarnos de que los datos utilizados como fuente de información son muy dispares, no solo en su contenido sino también en el tipo de dato. Así, si para la traducción de textos se utiliza texto ya existente como entrada, en otros sistemas como en la generación de informes meteorológicos se emplean datos no lingüísticos. De esta manera, se consideran dos posibles enfoques en los sistemas NGL dependiendo del tipo de entrada: text-to-text y data-to-text.


\subsubsection{Generación text-to-text (T2T)}
Estos tipos de sistemas de generación toman textos existentes como entrada y produce un texto nuevo y coherente como salida. La entrada de estos sistemas puede abarcar desde pequeñas oraciones a extensos textos. Existen muchas aplicaciones en los sistemas NLG que utilizan T2T como pueden ser generación de resúmenes, fusión, simplificación de textos, corrección gramatical, entre otros.

Si consideramos un ejemplo concreto, cualquier traductor automático es de tipo text-to-text ya que utilizan una entrada textual correspondiente a un escrito en un idioma y genera un texto en otro. La traducción automática es un proceso muy complejo puesto que no solamente hay que tener en cuenta el significado del escrito, sino que también hace falta interpretar y analizar de manera correcta todos los elementos del texto y saber cómo influyen unas palabras en otras para generar un texto fluido y coherente.

\subsubsection{Generación data-to-text (D2T)}
Estos tipos de sistemas permiten la generación de texto como salida a partir de entradas no textuales. Además, el formato de los datos que puede tomar como entrada son muy diversos. Aunque es muy común encontrar sistemas que parten de datos numéricos como hojas de cálculo, hay que considerar otros orígenes de datos de tipo estructurado tales como bases de datos, simulaciones de sistemas físicos, hojas de cálculo o grafos de conocimientos. 

Algunos autores prefieren utilizar  el término \textit{concepto} en lugar de \textit{data}, motivo por el que algunos se refieren a este enfoque como generación concept-to-text (c2t), \citep{vicente2015generacion}.

\begin{figure}[t]
	\centering
	%
	\begin{SubFloat}
		{\label{fig:inputFoG}%
			Entrada del sistema FoG}%
		\includegraphics[width=0.7\textwidth]%
		{Imagenes/Bitmap/03EstadoDeLaCuestion/inputFoG}%
	\end{SubFloat}
	\qquad
	\begin{SubFloat}
		{\label{fig:outputFoG}%
			Salida del sistema FoG}%
		\includegraphics[width=0.7\textwidth]%
		{Imagenes/Bitmap/03EstadoDeLaCuestion/outputFoG}%
	\end{SubFloat}
	\caption{Sistema data-to-text FoG%
		\label{fig:FoG}}
\end{figure}

Uno de los ejemplos más visuales que nos permite comprender este tipo de sistema sería el Forecast Generator, sistema que forma parte del Forecaster's Production Assistan, entorno desarrollado por CoGenTex en 1992 para Environment Canada con el fin ayudar a los meteorólogos a aumentar su productividad al redactar por ellos un informe meteorológico textual en inglés y en francés \citep{goldberg1994using}. En la figura  \ref{fig:inputFoG} se muestra el entorno sobre el que los meteorólogos modifican valores como la presión atmosférica, situación de frentes y otros datos (datos no textuales). Una vez se pulsa sobre 'Generar', el sistema muestra el texto correspondiente al informe (figura \ref{fig:outputFoG}). 


En la figura \ref{fig:data-to-text}, explicada con más detalle en \cite{sai2020survey}, se muestran los datos de entrada y de salida de un sistema NLG D2T acercándonos a la generación de lenguaje desde una perspectiva distinta al ejemplo explicado anteriormente. La fila superior corresponde a la entrada. En este caso a partir de un texto de referencia se obtienen unos datos que se introducen en el sistema, ya sea en forma de grafo u otro tipo de datos semiestructurados como tablas (conjunto de tuplas del tipo [entidad,atributo,valor]). En la fila inferior, se muestran diferentes posibles soluciones como salida del sistema. Además, el autor introduce la necesidad de métodos de evaluación de la calidad del texto redactado ya que de las diferentes salidas, solo la tercera opción cubre toda la información de entrada y resulta ser fluida.

\figura{Bitmap/03EstadoDeLaCuestion/data-to-text}{width=.9\textwidth}{fig:data-to-text}%
{Ejemplo de D2T utilizado por \cite{sai2020survey}}

\section{Arquitectura de un sistema NLG}
El objetivo final de un sistema de generación de lenguaje natural es mapear unos datos de entrada a un texto de salida. \citep{reiter1997building}. Sin embargo, este proceso, aunque pueda parecer sencillo de entender, resulta complicado de llevar a cabo. Al principio del desarrollo de sistemas NLG, no había un consenso entre autores a la hora de establecer un proceso para construir este sistema. Finalmente, \cite{reiter1997building} propusieron una arquitectura asociada a una lista de tareas que se deben recomendablemente realizar a la hora de llevar a cabo dicha construcción. Esta arquitectura surgió de la observación de los diferentes sistemas que se habían llevado a cabo hasta la fecha.  Actualmente, es la solución más extendida y reconocida.

La arquitectura presentada por \cite{reiter1997building}, como se puede observar en la figura \ref{fig:arquitectura}, se divide en 3 módulos: macroplanificación, microplanificación y realización. Además, cada módulo contiene una lista de tareas. Esta asignación tareas-módulo no es inamovible, una tarea asociada a un módulo se puede realizar en otro si así se considera, incluso implementar su desarrollo a lo largo de varios módulos. Los módulos que se corresponden con las tareas iniciales suelen estar relacionados con adaptar datos o estructura a nuestro sistema, mientras que los módulos finales corresponden a la transformación de los resultados intermedios en en texto final.

\figura{Bitmap/03EstadoDeLaCuestion/arquitectura}{width=1\textwidth}{fig:arquitectura}%
{Arquitectura de referencia para sistema NLG, \citet{vicente2015generacion}}

\subsection{Macroplanificación}
Este es el primer módulo del sistema. Debe determinar qué decir, seleccionando para ello la información de entrada necesaria y organizarlo en una estructura coherente, resultando de este proceso el plan del documento. Las tareas que
intervienen se describen en los apartados siguientes: 

\subsubsection{Selección del contenido}
La determinación del contenido puede definirse como el proceso de decidir que información debe ser incluida en el texto generado y cual no. Por lo general, la información de la que partimos contendrá más información de la que nos interesa, así debemos decidir que información resulta innecesaria y por tanto tenemos que eliminar para la generación del texto final. También hay que tener en cuenta el público al que está dirigido el texto generado, ya que dependiendo de este podremos incluir cierta información de los datos entrantes o no.

Este proceso de selección de la información correspondiente lleva a cabo la filtración y resumen de esta en un conjunto de \textit{mensajes}. Cada uno de estos mensajes corresponde al significado de una palabra u oración y se le asigna una entidad, concepto o relación dominante.

\subsubsection{Estructuración del documento}
Definiendo el concepto \textit{texto} como 'unidad de comunicación completa, formada habitualmente por una sucesión ordenada de enunciados que transmiten un mensaje con las siguiente propiedades: adecuación, coherencia y cohesión', podemos advertir que un texto no es un conjunto aleatorio de oraciones, sino que es necesario la existencia de un orden en la presentación del texto final.

Dependiendo de la información que se comunique, este orden puede verse modificado u alterado, es por ello que no hay una estructura fija, sino que hay que adecuarla al tipo de documento.

Una vez realizada la estructuración del texto, se obtiene un plan de discurso que corresponde a una representación estructurada y ordenada de los mensajes obtenidos en la tarea anterior.

\subsection{Microplanificación}
Segundo módulo de la arquitectura que parte del plan del documento. Se generan las oraciones evitando información redundante e innecesaria en un discurso. El resultado de este módulo es el plan de discurso. Lo hace mediante tres tareas:

\subsubsection{Agregación de oraciones}
La generación de una oración por cada uno de los mensajes puede resultar en la generación de un texto redundante y excesivamente estructurado. Una tarea en el proceso de construcción de un sistema NLG es la agregación de oraciones que pretender paliar este problema mediante la unión o agregación de contenidos de distintos mensajes en una sola oración. De esta manera los mensajes se combinan para obtener oraciones más largas y complejas, resultando en conjunto un texto con una mayor simplicidad.

\subsubsection{Lexicalización}
En esta fase del proceso se empieza a generar el texto en lenguaje natural como tal, a partir de las estructuras sintácticas y palabras específicas de las etapas anteriores. La dificultad  la generación en esta etapa reside en la gran cantidad de alternativas que encontramos a una sola palabra u oración para expresar los bloques de mensajes, además debemos tener en cuenta un número mayor de posibilidades ya que debemos además considerar las necesidades o conocimiento de los usuarios, si el objetivo es generar textos con variaciones sintácticas o semánticas a lo largo del mismo, selección de palabras sinónimas, selección de adjetivos relacionados estrechamente con sustantivos, consideración de magnitudes... 


\subsubsection{Generación de expresiones de referencia}
La diferenciación de unas entidades de otras para poder generar expresiones que se refieran a ellas es tratada en esta tarea con el objetivo de evitar la ambigüedad. Para realizar esta tarea se debe conseguir encontrar características particulares que contribuyan a diferenciar a una entidad del resto de entidades. Esta etapa está bastante consensuada en el campo NLG. 

La generación de expresiones de referencia (REG, por sus siglas en inglés) debe llevarse a cabo una vez que el plan del documento se haya generado y depende de este, esto implica que esta fase debe llevarse a cabo desde el primer momento después de que se hayan analizado los datos. Debemos adaptar el plan de documento del primer módulo a lo que necesita REG, es por ello que debemos tener conocimiento de ello desde el comienzo.

Un caso especialmente estudiado que aplica esta técnica es la descripción de imágenes, ya que debe tener en cuenta si un elemento se encuentra a la derecha de otro, detrás de otro, etc, para poder enriquecer el texto. Para ello es necesario reconocer y distinguir los elementos en escena unos de otros y así, obtener una descripción lo más fidedigna posible a la imagen real.

\subsection{Realización}
La realización constituye el último módulo de la arquitectura de un sistemas NLG. El objetivo final corresponde en generar oraciones gramaticalmente correctas para comunicar mensajes. En este módulo deberán tenerse en cuenta reglas a cerca de la formación de verbos (elección del tiempo verbal adecuado y por tanto generación de las palabras correspondientes), reglas sobre concordancia de género y número entre palabras (\cite{reiter1997building} no tiene en cuenta el género de las palabras ya que focaliza la generación del lenguaje al inglés), generación de pronombres...
 
La entrada sobre la que se trabaja es el plan de discurso que contiene información sobre las oraciones generadas y la estructura utilizada en el texto final. En esta fase se traduce esta entrada en la salida que el usuario final recibirá.

Algunos autores consideran una única tarea de realización que engloba el convertir las especificaciones en oraciones y el dar un formato final al texto. Otros prefieren separar estas etapas para diferenciarlas y que sea más sencillo su estudio.

\subsubsection{Realización lingüística}
Con el objetivo de transformar las especificaciones de oraciones en las oraciones finales, en esta fase se ordenan los diferentes elementos constitutivos de una oración y se les asigna un formato correcto. Para elegir la forma morfológica correcta de una palabra se debe conjugar verbos, establecer concordancias de palabras, añadir formas pronominales el los lugares adecuados de las oraciones y establecer los signos de puntuación adecuados. 

\subsubsection{Realización de la estructura}
Esta etapa no está considerada por algunos autores como tal aunque aquí se muestra ya que puede ser relevante en ciertos contextos. 
En algunos documentos, es necesario añadir o modificar algunas líneas del texto para darle una estructura al documento. Es por eso, que si queremos generar un texto en formato html se debe añadir las etiquetas a lo largo del texto generado. Igualmente si el documento final utiliza Latex.


\section{Modelos y herramientas NLG}
\textbf{En esta sección se describirán los principales Modelos de Lenguaje (LM) que más relevantes han sido durante los años pasados para la generación de lenguaje. Además, se describirán herramientas utilizadas también en esta tarea.}


\subsection{Modelos de lenguaje estadísticos}
Estos tipos de modelos utilizan técnicas estadísticas y reglas lingüísticas para aprender la distribución de probabilidad de las palabras y, de esta manera generar lenguaje. Entre las técnicas más utilizadas en este ámbito encontramos el modelo N-Gram y Markov.

Un modelo N-Gram nos permite realizar una predicción estadística de un n-gram dentro de cualquier secuencia de palabras dado el historial de las N-1 palabras anteriores. Sin embargo, esta dependencia de generación de la palabra siguiente a una secuencia dada con respecto al conjunto de palabras generadas inmediatamente antes puede llevar a generaciones erróneas que no tengan en cuenta otros contextos anteriores.

\subsection{Seq2seq}
El modelo Sequence-to-Sequence (Seq2Seq), que sigue una clase especial de arquitectura de Red Neuronal Recurrente (RNN), ha alcanzado mucho éxito a la hora de resolver problemas complejos de Procesamiento de Lenguaje Natural, incluso llegando a superar a los modelos estadísticos de lenguaje en su efectividad \citep{analytics_vidhya_2020}.

Considerando una aproximación lo más sencilla posible, podríamos representarlo como un sistema que toma una secuencia de elementos como entrada y genera otra secuencia de elementos de salida. Como se muestra en la figura \ref{fig:seq2seq}, este sistema está compuesto internamente por un \textit{encoder} y un \textit{decoder} que tradicionalmente implementan modelos basados en redes neuronales como LSTM (o GRU en menor número de casos). 

\figura{Bitmap/03EstadoDeLaCuestion/seq2seq}{width=1\textwidth}{fig:seq2seq}%
{Arquitectura de un sistema Seq2Seq}

La tarea del \textit{encoder} consiste en resumir la información de la secuencia que se introdujo como entrada en forma de un vector de estado oculto o \textit{context} y enviar los datos resultantes al \textit{decoder}. El objetivo principal de este vector es encapsular la información de todos los elementos de entrada para ayudar al \textit{decoder} a realizar predicciones precisas. Para calcular el estado oculto t-ésimo de la secuencia se utiliza la fórmula representada en la figura \ref{fig:estado_oculto_encoder}, donde 't' corresponde al tamaño de la secuencia de entrada 'x' y 'W' representa a todas y cada una de las palabras del dataset. Para cada una de las celdas del \textit{encoder} se calcula su vector de estado oculto, generando la última celda el vector de estados finales.

\figura{Bitmap/03EstadoDeLaCuestion/estado_oculto_encoder}{width=0.6\textwidth}{fig:estado_oculto_encoder}%
{Cálculo del estado oculto en el encoder}

Por su parte, el \textit{decoder} utiliza como estado inicial la salida del \textit{encoder} correspondiente al vector de estados finales, calculando cada celda su estado oculto con la fórmula de la figura \ref{fig:estado_oculto_decoder}. Una vez que se obtiene el estado oculto $h_{t}$, puede generarse la secuencia de palabras final aplicando al dataset de palabras junto con $h_{t}$ la función \textit{softmax}.

\figura{Bitmap/03EstadoDeLaCuestion/estado_oculto_decoder}{width=0.4\textwidth}{fig:estado_oculto_decoder}%
{Cálculo del estado oculto en el decoder}



\subsection{Transformers}
Los Transformers surgen como una evolución de los modelos Seq2Seq. Es por esto que aún podemos encontrar características de estos últimos sistemas en la arquitectura externa de los Transformes como puede ser la existencia de dos componentes clave: el \textit{encoder} y el \textit{decoder}, cuyos objetivos coinciden en ambos.

A diferencia de los sistemas Seq2Seq, ya no se van a utilizar RNN  para implementar el \textit{encoder} y el \textit{decoder}, que de hecho van a verse divididos en dos subcapas (\textit{Feed Forward} y \textit{Multi-Head Attention}) como se muestra en la figura \ref{fig:arquitectura_transformers} en la que se representa la arquitectura final de este tipo de sistemas.

Además, la diferencia de potencia de procesamiento entre ambos modelos es enorme. Si en Seq2Seq el \textit{decoder} procesaba palabra a palabra la secuencia que le llegaba desde el \textit{encoder}, los Transformers permiten alimentar al \textit{decoder} con toda la secuencia de manera simultanea resultando en un mayor poder por parte de este tipo de modelos.

\figura{Bitmap/03EstadoDeLaCuestion/arquitectura_transformers}{width=0.8\textwidth}{fig:arquitectura_transformers}%
{Arquitectura del modelo Transformer}

Hace falta destacar que para poder utilizar este tipo de modelos, se creó una herramienta en Python denominada 'Transformers' que proporciona una serie de sistemas de propósito general para Natural Language Understading (NLU) y Natural Language Generation (NLG). Ofrece más de 32 modelos preentrenados en más de 100 idiomas, entre los que se encuentra el español. Entre los modelos más utilizados encontramos los famosos GPT-2 y BERT junto con un gran número de variaciones de ellos dependiendo de los datos utilizados para su entrenamiento.

\subsubsection{GPT-2}
GPT-2 es un modelo NLG presentado por OpenAI en el año 2019 basado en redes neuronales, concretamente está construido sobre una arquitectura Transformer, cuya finalidad es construir una distribución de probabilidad en la que para cada palabra posible a generar se le asigna una probabilidad en función del contexto anterior. Se trata de un modelo que ha sido preentrenado con un conjunto de datos correspondiente a las 8 millones de páginas web mejor valoradas en Reddit, lo que resulta en una gran base de conocimiento para generar textos automáticamente de manera muy correcta.

La potencia de este modelo es tal que sus creadores no quisieron en un primer momento publicar la versión completa por miedo de que se pudiera utilizar de manera ilícita. Según fueron pasando los años, se fueron liberando progresivamente diferentes versiones del modelo original ya que comenzaban a surgir otros proyectos con potencias igualmente competitivas. Estas diferentes versiones se diferenciaban en el número de parámetros que admitía la arquitectura, de esta manera se conseguía limitar su funcionamiento. La primera versión contaba con 117 miles de millones de parámetros mientras que la última versión, publicada en 2020, posee 1,5 billones.

GPT-2 únicamente está disponible en inglés aunque puede hacer uso de GoogleTranslate API para generar textos en otros idiomas aunque es importante resaltar que al depender del traductor se puede ver disminuida su calidad.

\subsubsection{BERT}
Modelo NLP desarrollado por Google a finales de 2018. Está basado redes neuronales bidireccionales que tratan de predecir las palabras perdidas (enmascaradas) en una oración y determinar si dos oraciones consecutivas son continuación lógica entre sí para determinar si están conectadas por su significado. Aunque originalmente no estaba destinado a la generación de textos, \cite{wang-cho-2019-bert} publicaron un método de utilización de este sistema para conseguir la generación de lenguaje que parece dar muy buenos resultados. De hecho, consiguió mejorar los resultados de la versión publicada en aquellos tiempos por GPT-2. 

Beto es la versión en Español de BERT \citep{CaneteCFP2020} y ha sido entrenado con una gran corpus en dicho idioma. Se trata de una herramienta de código abierto que se encuentra disponible en Github.




\subsection{SimpleNLG}
SimpleNLG es una API de Java que proporciona interfaces que ofrecen un control directo sobre la tarea de realización. Define un conjunto de tipos léxicos, correspondientes a las principales categorías gramaticales, así como formas de combinarlos y establecer valores de características. 

Esta orientado a la generación de oraciones gramaticalmente correctas en sistemas data-to-text. Aunque originalmente solo estaba disponible para textos de lengua inglesa, actualmente se encuentra en muchos idiomas, entre ellos el español. La versión española de esta herramienta se llama SimpleNLG-ES y realmente se trata de una adaptación bilingüe de la versión original en inglés.

Esta herramienta se basa la flexibilidad a la hora de generar textos mediante la utilización de manera combinada de sistemas basados en esquemas y otros sistemas más avanzados; robustez, generando salidas (aunque en ocasiones incorrectas) cuando las entradas estén erróneas o incompletas; e independencia entre las operaciones de decisión de morfología y sintácticas.

\section{Proyectos relacionados}

\subsection{A Conversational Model for the Reminiscence Therapy of Patiens with Early Stage of Alzheimer}
Este proyecto propuesto por \cite{deconversational} trata de desarrollar e implementar un modelo conversacional que pueda ayudar a los cuidadores y a sus propios pacientes con Alzheimer a realizar un mayor número de terapias de reminiscencia periódicas para así, potenciar los beneficios de estas. Se centra en generar conversaciones personalizadas entre el prototipo del sistema conversacional y el paciente con el fin de recoger información relacionada con sus gustos, historial y estilo de vida. Su arquitectura, como se muestra en la figura \ref{fig:ConversationalModelArchitecture}, esta integrada por varios módulos: módulo de Reconocimiento Automático de Voz, Comprensión del Lenguaje Natural, Gestión de Diálogos, Modelo de Diálogos, Generación de Lenguaje Natural y Text-to-Speech.


\figura{Bitmap/03EstadoDeLaCuestion/ConversationalModelArchitecture}{width=.8\textwidth}{fig:ConversationalModelArchitecture}%
{Arquitectura modelo conversacional \citep{deconversational}}

\subsection{User-oriented ontology-based clustering of stored memories}
Esta investigación realizada por \cite{shi2012user} se centra en el desarrollo de un sistema computarizado llamado Life Story Book (LSB), que facilita el acceso y la recuperación de recuerdos almacenados que se utilizan como base para interacciones positivas entre ancianos y jóvenes, y especialmente entre personas con deterioro cognitivo y miembros de su familia o cuidadores. Para facilitar la gestión de la información y la generación dinámica de contenido, este artículo presenta un modelo semántico de LSB que se basa en el uso de ontologías y algoritmos avanzados para la selección de características y la reducción de dimensiones. Para terminar, propone un algoritmo llamado Onto-SVD que combina la selección de características semánticas y la ontología orientada al usuario con  la utilización de SVD como método de reducción de dimensiones para lograr la identificación de temas basada en la similitud semántica.

