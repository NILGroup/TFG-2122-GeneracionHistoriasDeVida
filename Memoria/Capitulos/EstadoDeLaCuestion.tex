\chapter{Estado de la Cuestión}
\label{cap:estadoDeLaCuestion}


\section{Alzheimer e historias de vida}
La pirámide poblacional modifica su estructura continuamente debido al progresivo envejecimiento generalizado de la población. Según proyecciones de la \cite{estalz}, en el año 2050 las personas mayores de 65 años constituirán el 16 por ciento de la población mundial frente al 8 por ciento del año 2010. El aumento de la esperanza de vida en todo el mundo, principalmente en las sociedades más avanzadas, y la disminución de la natalidad, se encuentran entre las causas de la modificación de la distribución demográfica hacia edades más avanzadas. Este fenómeno es conocido como \textit{inversión de la pirámide poblacional} \citep{RCSP892}.

La realidad detrás de estas estadísticas: el incremento del número de personas de edad avanzada, y asociándose al envejecimiento la acumulación a lo largo del tiempo de una gran variedad de daños moleculares y celulares que lleva a un descenso gradual de las capacidades mentales y físicas, deriva en un mayor riesgo de determinadas enfermedades. 

La pérdida de la audición, las cataratas, la artritis y la artrosis son solo algunas de las enfermedades con mayor incidencia. Sin embargo, una de las dolencias más comunes y serias dentro de este rango de población es la enfermedad de Alzheimer, cuya prevalencia a nivel global se espera que supere todo dato conocido hasta ahora, dado que se estima que en el año 2050 se incremente el número de casos a 152,8 millones, sobrepasando considerablemente los 57,4 millones del año 2019 \citep{alzheimers_disease_international_2019}.


\subsection{Descripción general}
La enfermedad de Alzheimer es un trastorno neurológico caracterizado por cambios degenerativos en diferentes sistemas neurotransmisores que abocan finalmente a la muerte de las células nerviosas del cerebro encargadas del almacenamiento y procesamiento de la información. Las regiones del cerebro involucradas con la memoria y los procesos de aprendizaje, asociadas a los lóbulos temporal y frontal, reducen su tamaño como consecuencia de la degeneración de las sinapsis y la muerte de las neuronas \citep{romano2007enfermedad,mattson2004pathways}. En las etapas finales de esta patología, este proceso, también denominado \textit{atrofia cerebral} se extiende y provoca una pérdida significativa del volumen cerebral (figura~\ref{fig:CerebroPersonaAlzheimer} a).


\figura{Vectorial/03EstadoDeLaCuestion/CerebroPersonaAlzheimer}%
{width=.8\textwidth}%
{fig:CerebroPersonaAlzheimer}%
{Reducción del cerebro asociada al Alzheimer \citep{mattson2004pathways}}

En numerosas ocasiones son utilizadas imágenes similares a las mostradas en la figura~\ref{fig:CerebroPersonaAlzheimer} como indicativos de la enfermedad del Alzheimer. La figura~\ref{fig:CerebroPersonaAlzheimer} b  representa unas \textit{tomografías por emisión de positrones} o \textit{PET scans} en inglés. En ellas se reflejan los patrones de distribución espacial de la glucosa en el cerebro. En el cerebro de la persona con Alzheimer, el flujo glucolítico cerebral se minimiza provocando los síntomas de la enfermedad. Esta prueba se utiliza en el diagnóstico de la gravedad de la patología.


El proceso de detección de la enfermedad de Alzheimer es una tarea ardua de realizar dado que, por lo general, los síntomas iniciales de la enfermedad suelen atribuirse a un olvido puntual o la vejez. Nada más lejos de la realidad. Según avanza la enfermedad, sus síntomas lo hacen con ella, agravándose y aumentando cada vez más hasta que el deterioro cognitivo ocasionado llega a afectar significativamente a las actividades de la vida diaria y finalmente a las necesidades fisiológicas básicas.

La evolución del Alzheimer se puede dividir en tres fases o etapas. En una primera instancia, se comienza a observar un deterioro cognitivo leve como puede ser la pérdida paulatina de la memoria episódica, seguido de pérdidas de la memoria reciente asociadas a un deterioro mayor así como otras funciones mentales y de la personalidad. Para terminar, se produce una pérdida progresiva de la memoria referida a los acontecimientos más antiguos, acompañando además un importante deterioro físico.


\subsection{Síntomatología y pérdida de la memoria}
La amnesia o pérdida de la memoria es uno de los síntomas más representativos del Alzheimer. Sin embargo, se trata tan solo de la punta del iceberg debido a todos los desordenes que también se producen y que no son considerados o tenidos en cuenta por el personal no profesional: alteraciones del estado de ánimo y la conducta, dificultad de toma de decisiones, desorientación, problemas del lenguaje, dificultad para comer, movilidad reducida  y un largo etcétera son algunos de los síntomas que acompañan a esta enfermedad durante todo su camino. Todos estos síntomas dependen de la fase evolutiva de la enfermedad.

Podemos distinguir en cuanto a sintomatología dos fases marcadas por las alteraciones neurológicas: en una primera fase, conocida como fase predemencial, los signos de desordenes neurológicos todavía no se encuentran presentes; y la fase demencial, en la que se pueden observar grandes alteraciones motoras, cognitivas, sensoriales y emocionales.

En la etapa predemencial, durante la cual en numerosas ocasiones el paciente no se encuentra diagnosticado de la enfermedad, comienzan a producirse lesiones microscópicas en el cerebro. Sin embargo, no es hasta entre 10 y 20 años después que pueden aparecer las primeras alteraciones cognitivas. El conjunto de síntomas presentes en esta fase comprende principalmente alteraciones en la conducta como trastorno de la personalidad, apatía o cambios en el estado de ánimo; y deterioro gradual de la memoria, comenzando el paciente a olvidar pequeñas cosas hasta llegar a no ser capaz de recordar familia o amigos.

A medida que progresa el daño cerebral aparece progresivamente un deterioro más pronunciado del paciente, comenzando entonce la fase demencial de la enfermedad. En esta etapa comienzan a aparecer alteraciones neurológicas como pérdida del movimiento, temblores, alucinaciones,  trastornos en el lenguaje oral y escrito o alteraciones de la personalidad \citep{alberca-serrano-2010}.


\subsection{Tratamientos: historias de vida}
En la actualidad el Alzheimer es una enfermedad irreversible. Sin embargo, existen diversos tratamientos disponibles para ralentizar el avance de la enfermedad, así como mejorar la calidad de vida de los pacientes. Estos tratamientos se pueden dividir en dos ramas diferenciadas: tratamientos farmacológicos o farmacoterapia, que hacen uso de medicamentos; y tratamientos no farmacológicos o psicosociales, que no hacen uso de sustancias químicas. Ambos tipos de tratamientos resultan eficaces para tratar la enfermedad de Alzheimer. Sin embargo, de la combinación de ambos resulta el procedimiento más recomendado debido a su mayor efectividad. Esto es posible gracias a que ambos tipos de tratamientos no son mutuamente excluyentes \citep{romano2007enfermedad}.

Existen una gran variedad de terapias no farmacológicas. Algunas de las más utilizadas son el entrenamiento y estimulación cognitiva, ejercicio físico o musicoterapia. Además, en cada una de estas terapias podemos encontrar una enorme cantidad de técnicas, siendo la reminiscencia la más utilizada como terapia de estimulación cognitiva.

Según \cite{o2013cross}, la reminiscencia es el acto o proceso de recordar sucesos, eventos o información del pasado. Esto puede implicar el recuerdo de episodios particulares o genéricos que pueden o no haber sido olvidados previamente, y que son acompañados por la sensación de que estos episodios son relatos verídicos de las experiencias originales. Esta técnica es empleada en la estimulación del la memoria episódica autobiográfica mediante el encadenamiento de recuerdos, que se agrupan en categorías y se archivan en el tiempo mediante la elaboración de la \textit{historia de vida}.

La historia de vida es una técnica narrativa que se basa en organizar y estructurar recuerdos de una persona para componer una autobiografía. Según \cite{linde1993life}, una historia de vida debe cumplir dos criterios: primero, debe incluir algunos puntos de evaluación que comuniquen los valores morales de la persona; y segundo, los eventos incluidos en la historia de vida deben tener un significado especial y ser de importancia para ella. Estos eventos deben ser aspectos significativos de la vida pasada de la persona, su presente y su futuro.

Para componer la historia de vida de una persona con Alzheimer se recopilan historias a través de familiares u otras personas cercanas. Posteriormente, se documentan en forma de un libro o cuaderno, incluyendo experiencias y logros junto con fotografías y escritos sobre hechos importantes para la vida de la persona, a través de los cuales se muestra quién es esa persona.

Cada persona tiene su propia historia de vida única. Nuestras experiencias nos modelan y construyen la persona que somos. Las historias de vida ayudan a las personas con Alzheimer a conectar con su identidad recordando épocas felices. El miedo y la frustración provocados por el olvido de las tareas de la vida cotidiana, nombres y rostros, se mitigan recordando quiénes eran a través de estas historias. Les ayuda a ser conscientes de los momentos especiales que han marcado su vida, las personas que han conocido en su infancia o trabajo. También pueden ser utilizados por los cuidadores para comprender más sobre ellos, quiénes son, y ayudarles en la reminiscencia de recuerdos \citep{karlsson2014stories}.


Existen diferentes formatos en los que se pueden registrar estas experiencias de la persona. Ninguno de ellos es mejor o peor que otro, sino que lo ideal es utilizar aquel que mejor se adapte a la persona y a los hechos que se quieran transmitir.

Por una parte encontramos historias de vida más visuales, compuestas enteramente de imágenes (\textit{collages}) o videos, dirigidas especialmente a las personas con Alzheimer que se encuentran en una etapa tardía de la enfermedad. Otro formato se centra especialmente en textos. Los \textit{libros de vida}, destinados a los cuidadores y visitantes tanto como a la propia persona, combina las \textit{historias de vida}, en forma de texto claro y fácil de leer, con algunas imágenes. También nos encontramos los documentos de perfil personal que se centran en pequeñas versiones cortas de las historias de vida excluyendo las imágenes. Estos documentos son utilizados a menudo en hospitales y están diseñados para ayudar al personal a comprender las necesidades de la persona.

El contenido de una historia de vida es variable, aunque existen algunos temas básicos en los que se debe centrar: el perfil de la persona, incluyendo datos e información básica como es el nombre, edad, lugar de nacimiento o de residencia son esenciales para aproximarse de manera inicial a la persona. Otros temas como las relaciones significativas familiares y de amistad, infancia, lugares y eventos significativos y gustos o preferencias y aficiones son incluidos dentro de esta lista de posibles temas a tratar el la historia de vida \citep{thompsonlifestory}.

\section{Generación de lenguaje natural}

La Generación de Lenguaje Natural (GLN) se define como el ``subcampo de la inteligencia artificial y la lingüística computacional que se ocupa de la construcción de sistemas informáticos que pueden producir textos comprensibles en inglés u otros lenguajes humanos a partir de alguna representación no lingüística subyacente de la información''  \citep{reiter1997building}. Si bien esta definición estuvo generalmente aceptada como la más conveniente al hablar de generación de lenguaje natural durante muchos años, \cite{gatt2018survey} puntualizan que es una afirmación que solo engloba una parte de la generación de textos, ya que se refiere únicamente a aquellos sistemas cuya entrada es una ``representación no lingüística [...] de la información'' o datos, como veremos más adelante en el apartado \ref{cap:nglD2T}.

Desde hace muchos años, la GLN es empleada en numerosos proyectos de distinta naturaleza como la traducción de textos \citep{Cho2014LearningPR}, realización de resúmenes y fusión de documentos \citep{clarke2010discourse}, corrección automática de ortografía y gramática \citep{islam2018bangla}, redacción de noticias \citep{leppanen2017data}, informes meteorológicos \citep{sripada2014case} y financieros \citep{ren2021hybrid}, generación de resúmenes sobre la información de recién nacidos en un contexto clínico \citep{BabyTalk}... Todos estos sistemas tienen en común la generación de un texto (normalmente de una alta calidad) a partir de muy diferentes fuentes de información.

En los ejemplos de proyectos listados con anterioridad que emplean la generación de lenguaje natural para redactar distintos textos, los datos utilizados como fuente de información son muy dispares, no solo en su contenido sino también en el tipo de dato. Así, si para la traducción de textos se utiliza texto ya existente como entrada, en otros sistemas como en la generación de informes meteorológicos se emplean datos no lingüísticos. De esta manera, se consideran dos posibles enfoques en los sistemas GNL dependiendo del tipo de entrada: texto a texto (\textit{text-to-text}) y dato a texto (\textit{data-to-text}).

\subsection{Generación \textit{text-to-text} (T2T)}
Los sistemas de generación texto a texto, conocidos como \textit{text-to-text} en inglés o T2T por sus siglas, toman textos escritos en lenguaje natural como entrada y producen un texto nuevo, coherente como salida. La entrada de estos sistemas puede abarcar desde pequeñas oraciones a extensos escritos. Existen muchas aplicaciones en los sistemas GLN que utilizan T2T. Además de los mencionados anteriormente, pertenecen a este tipo la fusión de documentos y generación de resúmenes \citep{clarke2010discourse}, simplificación de textos complejos \citep{sulem2018simple}, autocorrectores gramaticales \citep{Ge2019AutomaticGE}, entre otros. 

Sin embargo, el ejemplo más claro de este tipo de generación de lenguaje corresponde a un traductor automático. Este tipo de sistema ampliamente utilizado en la vida cotidiana toma una entrada textual correspondiente a un escrito en un idioma y genera un texto de salida en otro idioma. La traducción automática es un proceso muy complejo puesto que no solamente tiene en cuenta el significado del corpus, sino que también hace falta interpretar y analizar de manera correcta todos los elementos del texto, así como comprender la influencia de unas palabras en otras con la finalidad de  generar un texto fluido y coherente. 

\subsection{Generación \textit{data-to-text} (D2T)}\label{cap:nglD2T}
Estos tipos de sistemas permiten la generación de texto como salida a partir de entradas no textuales. Además, el formato de los datos que pueden tomar como entrada son muy diversos. Aunque es muy común encontrar sistemas que parten de datos numéricos como hojas de cálculo, hay que considerar otros orígenes de datos de tipo estructurado tales como bases de datos, simulaciones de sistemas físicos o grafos de conocimientos. De manera general, podemos referirnos a la representación de la información de esta clase de sistema como datos estructurados o procesables.

Algunos autores prefieren emplear  el término \textit{concepto} en lugar de \textit{data}, motivo por el que algunos se refieren a este enfoque como generación concept-to-text (C2T) \citep{vicente2015generacion}.

\begin{figure}[t]
	\centering
	%
	\begin{SubFloat}
		{\label{fig:inputFoG}%
			Entrada del sistema FoG}%
		\includegraphics[width=0.7\textwidth]%
		{Imagenes/Bitmap/03EstadoDeLaCuestion/inputFoG}%
	\end{SubFloat}
	\qquad
	\begin{SubFloat}
		{\label{fig:outputFoG}%
			Salida del sistema FoG}%
		\includegraphics[width=0.7\textwidth]%
		{Imagenes/Bitmap/03EstadoDeLaCuestion/outputFoG}%
	\end{SubFloat}
	\caption{Sistema \textit{data-to-text} FoG%
		\label{fig:FoG}}
\end{figure}

Uno de los ejemplos más visuales que nos permite comprender este tipo de sistema sería el \textit{Forecast Generator}, sistema que forma parte del \textit{Forecaster's Production Assistant}, entorno desarrollado por \textit{CoGenTex} en 1992 para \textit{Environment Canada} con el fin ayudar a los meteorólogos a aumentar su productividad al redactar por ellos un informe meteorológico textual en inglés y en francés \citep{goldberg1994using}. En la figura  \ref{fig:inputFoG} se muestra el entorno sobre el que los meteorólogos modifican valores como la presión atmosférica, situación de frentes y otros datos (datos no textuales). Una vez se pulsa sobre \textit{Generar}, el sistema muestra el texto correspondiente al informe (figura \ref{fig:outputFoG}). 


En la figura \ref{fig:data-to-text}, explicada con más detalle en \cite{sai2020survey}, se muestran los datos de entrada y de salida de un sistema GLN D2T acercándonos a la generación de lenguaje desde una perspectiva distinta al ejemplo explicado anteriormente.  Los datos de entrada de este tipo de sistema toman la forma de grafo o cualquier otro tipo de datos semiestructurados como tablas (conjunto de tuplas del tipo [entidad, atributo, valor]). En la fila inferior, se muestran diferentes posibles soluciones como salida del sistema. Además, el autor introduce la necesidad de métodos de evaluación de la calidad del texto redactado ya que de las diferentes salidas, solo la tercera opción cubre toda la información de entrada y resulta ser fluida.

\figura{Bitmap/03EstadoDeLaCuestion/data-to-text}{width=.9\textwidth}{fig:data-to-text}%
{Ejemplo de D2T utilizado por \cite{sai2020survey}}

\section{Arquitectura tradicional de un sistema GLN}
\label{sec:arquitectura_tradicional}
El objetivo final de un sistema de generación de lenguaje natural es mapear unos datos de entrada a un texto de salida \citep{reiter1997building}. Sin embargo, este proceso, aunque pueda parecer sencillo de entender, resulta complicado de llevar a cabo. Al principio del desarrollo de sistemas GLN, no había un consenso entre autores a la hora de establecer un proceso para construir este sistema. Finalmente, \cite{reiter1997building} propusieron una arquitectura asociada a una lista de tareas recomendables que se deben realizar a la hora de llevar a cabo dicha construcción. Esta arquitectura surgió de la observación de los diferentes sistemas que se habían llevado a cabo hasta la fecha.  Actualmente, es la solución más extendida y reconocida.

La arquitectura presentada por \cite{reiter1997building}, como se puede observar en la figura \ref{fig:arquitectura}, se divide en tres módulos: macroplanificación, microplanificación y realización. Además, cada módulo contiene una lista de tareas. Esta asignación tareas-módulo no es inamovible. Una tarea asociada a un módulo se puede realizar en otro si así se considera, incluso implementar su desarrollo a lo largo de varios módulos. Los módulos que se corresponden con las tareas iniciales suelen estar relacionados con adaptar datos o estructura al sistema de generación, mientras que los módulos finales corresponden a la transformación de los resultados intermedios en el texto final.

\figura{Bitmap/03EstadoDeLaCuestion/arquitectura}{width=1\textwidth}{fig:arquitectura}%
{Arquitectura de referencia para sistema GLN \citep{vicente2015generacion}}

\subsection{Macroplanificación}
Este es el primer módulo de un sistema de generación de lenguaje. Debe determinar qué decir, seleccionando para ello la información de entrada necesaria y organizarla en una estructura coherente, resultando de este proceso el plan del documento. Las tareas que intervienen se describen en los apartados siguientes. 

\subsubsection{Selección del contenido}
La selección o determinación del contenido puede definirse como el proceso de decidir qué información debe ser incluida en el texto generado y cual no. Por lo general, la información de la que partimos contendrá más información de la que nos interesa, así debemos decidir qué información resulta innecesaria y por tanto tenemos que eliminar para la generación del texto final. También hay que tener en cuenta el público al que está dirigido el texto generado, ya que dependiendo de este podremos incluir cierta información de los datos entrantes o no.

Este proceso de selección de la información lleva a cabo la filtración y resumen de esta en un conjunto de \textit{mensajes}. Cada uno de estos mensajes corresponde al significado de una palabra u oración y se le asigna una entidad, concepto o relación dominante.

\subsubsection{Estructuración del documento}
Definiendo el concepto \textit{texto} como ``unidad de comunicación completa, formada habitualmente por una sucesión ordenada de enunciados que transmiten un mensaje con las siguientes propiedades: adecuación, coherencia y cohesión'', podemos advertir que un texto no es un conjunto aleatorio de oraciones, sino que es necesaria la existencia de un orden en la presentación del texto final.

Dependiendo de la información que se comunique, este orden puede verse modificado o alterado. Es por ello que no hay una estructura fija, sino que hay que adecuarla al tipo de documento.

Una vez realizada la estructuración del texto, se obtiene un plan de discurso que corresponde a una representación estructurada y ordenada de los mensajes obtenidos en la tarea anterior.

\subsection{Microplanificación}
La microplanificación es el segundo módulo de la arquitectura. Parte del plan del documento resultante del módulo anterior para generar las oraciones evitando información redundante e innecesaria en el discurso. El resultado de este módulo es el plan de discurso. El proceso de generación de oraciones lo realiza mediante tres tareas.

\subsubsection{Agregación de oraciones}
La generación de una oración por cada uno de los mensajes puede resultar en la generación de un texto redundante y excesivamente estructurado. Una tarea en el proceso de construcción de un sistema GLN es la agregación de oraciones que pretender paliar este problema mediante la unión o agregación de contenidos de distintos mensajes en una sola oración. De esta manera los mensajes se combinan para obtener oraciones más largas y complejas, resultando en conjunto un texto menos estructurado y más fluido.

\subsubsection{Lexicalización}
En esta fase del proceso se empieza a generar el texto en lenguaje natural como tal. Para ello se debe decidir que palabras u estructuras sintácticas expresan mejor los conceptos y relaciones de las etapas anteriores. La dificultad de la generación en esta etapa reside en la gran cantidad de alternativas que encontramos para  expresar cada uno de estos conceptos o bloques de mensajes. Además hace falta tener en cuenta un número mayor de posibilidades ya que debemos considerar numerosas variables que podrían afectar al resultado final de la generación. Las necesidades o conocimiento de los usuarios, si el objetivo de la generación es generar textos con variaciones sintácticas o semánticas a lo largo del mismo, si es preferible un texto repetitivo y simple o diverso mediante la utilización de palabras sinónimas, una apropiada selección de adjetivos... son algunas de las variables a tener en cuenta.


\subsubsection{Generación de expresiones de referencia}
La diferenciación de unas entidades de otras para poder generar expresiones que se refieran a ellas es tratada en esta tarea con el objetivo de evitar la ambigüedad. Para realizar esta tarea se debe conseguir encontrar características particulares que contribuyan a diferenciar a una entidad del resto de entidades. Esta etapa está bastante consensuada en el campo GLN. 

La generación de expresiones de referencia (REG, por sus siglas en inglés) debe llevarse a cabo una vez que el plan del documento se haya generado y depende de este, esto implica que esta fase debe llevarse a cabo desde el primer momento después de que se hayan analizado los datos. Debemos adaptar el plan de documento del primer módulo a lo que necesita REG, es por ello que debemos tener conocimiento de ello desde el comienzo.

Un caso especialmente estudiado que aplica esta técnica es la descripción de imágenes, ya que debe tener en cuenta si un elemento se encuentra a la derecha de otro, detrás de otro, etc, para poder enriquecer el texto. Para ello es necesario reconocer y distinguir los elementos en escena unos de otros y así, obtener una descripción lo más fidedigna posible a la imagen real.

\subsection{Realización}
La realización constituye el último módulo de la arquitectura de un sistema GLN. El objetivo final corresponde en generar oraciones gramaticalmente correctas para comunicar mensajes. En este módulo deberán tenerse en cuenta reglas a cerca de la formación de verbos (elección del tiempo verbal adecuado y por tanto generación de las palabras correspondientes), reglas sobre concordancia de género y número entre palabras (\cite{reiter1997building} no tienen en cuenta el género de las palabras ya que focaliza la generación del lenguaje al inglés), generación de pronombres...

La entrada sobre la que se trabaja es el plan de discurso que contiene información sobre las oraciones generadas y la estructura utilizada en el texto final. En esta fase se traduce esta entrada en la salida que el usuario final recibirá.

Algunos autores consideran una única tarea de realización que engloba el convertir las especificaciones en oraciones y el dar un formato final al texto. Otros prefieren separar estas etapas para diferenciarlas y que sea más sencillo su estudio.

\subsubsection{Realización lingüística}
Con el objetivo de transformar las especificaciones de oraciones en las oraciones finales, en esta fase se ordenan los diferentes elementos constitutivos de una oración y se les asigna un formato correcto. Para elegir la forma morfológica correcta de una palabra se debe conjugar verbos, establecer concordancias de palabras, añadir formas pronominales en los lugares adecuados de las oraciones y establecer los signos de puntuación adecuados. 

\subsubsection{Realización de la estructura}
Esta etapa no está considerada por algunos autores como tal aunque aquí se muestra ya que puede ser relevante en ciertos contextos. 
En algunos documentos, es necesario añadir o modificar algunas líneas del texto para darle estructura al documento. Un ejemplo muy sencillo de entender es la generación de texto que utilice html o Latex como formato de salida. En ambos casos, la adición de etiquetas a lo largo del texto generado resulta crucial para un texto de cualquiera de estas naturalezas.


\section{Modelos y herramientas GLN}
\label{sec:modelos}
En esta sección se describen los Modelos de Lenguaje más relevantes para la generación de lenguaje natural a lo largo de los últimos años junto con las herramientas que nos permiten utilizarlos. %TODO

\subsection{Historia}
Antes de comenzar a describir los distintos tipos de modelos utilizados en tareas de procesamiento de lenguaje natural, es interesante conocer como ha ido evolucionando este campo a lo largo de la historia y los modelos más relevantes en cada una de las etapas. En general, la historia del procesamiento de lenguaje natural se divide en dos grandes períodos marcados por la aparición del aprendizaje profundo o \textit{Deep Learning} \citep{louis-2021}. 

La era \textit{pre Deep Learning} (figura~\ref{fig:erapreDL}) comienza aproximadamente en el 1949, momento en el que   Warren Weaver sugería en su memorando ``Translation'' \footnote{ Lectura disponible en https://web.stanford.edu/class/linguist289/weaver001.pdf} que la traducción automática computacional era posible. Esta fue la primera aproximación estadística al procesamiento y generación de lenguaje. Supuso una revolución y inspiró numerosos experimentos y proyectos que probaron que realmente esto era posible aunque a muy pequeña escala. Estos sistemas se basaban principalmente en la búsqueda en diccionarios de las palabras necesarias para la traducción y la posterior reordenación de las palabras para ajustarse a las reglas sintácticas del idioma destino de la traducción.

Después de una década de investigaciones para conseguir mejores resultados en este campo y de pérdida de financiación, ya que las soluciones encontradas hasta ahora conseguían resultados muy pobres. Surgieron nuevas ``Teorías de la Gramática'' mucho más manejables computacionalmente, y más tarde las ``Ontologías Conceptuales'' que estructuraban la información del mundo real en datos comprensibles por la computadora.

En la década de 1980, surgieron los ``Modelos Simbólicos'' basados en reglas. Estos sistemas asignaban manualmente los significados de las palabras y de esta manera determinista se creaban oraciones. Debido a la complejidad de creación de estas reglas, ya que se debían crear a mano, estos modelos fueron ampliamente sustituidos por los ``Modelos Estadísticos'' que supusieron una revolución para el procesamiento de lenguaje en aquella época y que hoy en día todavía tienen una gran relevancia en la lingüística computacional. 


\begin{figure}[t]
	\centering
	%
	\begin{SubFloat}
		{\label{fig:erapreDL}%
			Era pre \textit{Deep Learning}}%
		\includegraphics[width=0.9\textwidth]%
		{Imagenes/Bitmap/03EstadoDeLaCuestion/erapreDL}%
	\end{SubFloat}
	\qquad
	\begin{SubFloat}
		{\label{fig:erapostDL}%
			Era \textit{Deep Learning}}%
		\includegraphics[width=0.9\textwidth]%
		{Imagenes/Bitmap/03EstadoDeLaCuestion/erapostDL}%
	\end{SubFloat}
	\caption{Etapas del Procesamiento de Lenguaje Natural%
		\label{fig:etapasDL}}
\end{figure}

Con el avance computacional de las ``Redes Neuronales'', comienzan a usarse en la década del 2000 en el modelado del lenguaje para la generación de textos y dan lugar a la era \textit{Deep Learning} (figura~\ref{fig:erapostDL}). \cite{bengio_2000} propuso el primer modelo de lenguaje neuronal utilizando una Red Neuronal Prealimentada  \textit{(FeedForward Neural Network)} de una capa oculta. Otros autores sustituyeron progresivamente esta arquitectura de red por Redes Neuronales Recurrentes (\textit{Recurrent Neural Networks}) y Redes de Memoria a Corto Plazo (\textit{Long Short-Term Memory}) aunque los componentes básicos de la arquitectura original se encuentran todavía en la mayoría de modelos de lenguaje neuronales.

Más tarde \cite{collobertWeston} introdujeron el ``Aprendizaje Multitarea'' al procesamiento de lenguaje, utilizando una Red Neuronal Convolucional (\textit{Convolutional Neural Network}) para conseguir que varias tareas de aprendizaje se resolvieran de manera simultánea, resultando en una mejora de la eficiencia. 

Tras varios avances, como la introduccion de modelos ``\textit{Word Embeddings}'' o la adopción general de redes neuronales para el modelado de lenguaje, surgen la arquitectura Secuencia a Secuencia (\textit{Seq2Seq}). Estos sistemas estaban compuesto dos componentes claves: el codificador o \textit{encoder} y el decodificador o \textit{decoder}, que serán explicados más adelante. La revolución que supuso esta arquitectura fue significativa y todavía se siguen utilizando.

En 2014, \cite{Bahdanau} introduce los mecanismos de atención que alivia el problema de cuello de botella de los modelos predecesores, los \textit{Seq2Seq}.

La última innovación en el mundo del Procesamiento de Lenguaje son los grandes Modelos de Lenguaje Preentrenados (\textit{Pretrained Moldels}). Debido a todo el esfuerzo computacional de días, semanas e incluso meses que supone entrenar un modelo de lenguaje, se proponen una serie de modelos que ya tienen realizado este entrenamiento. La finalidad de estos sistemas es el ajuste o \textit{fine-tuned} de los mismos de acuerdo al objetivo que se quiera conseguir.

\subsection{Modelos estadísticos: cadenas de Markov y N-grams}
Estos tipos de modelos utilizan técnicas estadísticas y reglas lingüísticas para aprender la distribución de probabilidad de las palabras y, de esta manera, generar lenguaje. Entre las técnicas más utilizadas y que mejores resultados han arrojado en este ámbito encontramos el modelo \textit{Markov Chain}.

\subsubsection{Modelo Markov Chain}

Este modelo, introducido por el matemático ruso Andrey Markov en 1913, es un modelo estocástico discreto que describe una secuencia de posibles eventos. Aplicado a la generación de texto que aquí se describe, podemos resumirlo en un sistema que se basa en una distribución de probabilidades aleatorias para generar la siguiente palabra a un grupo de palabras. Para que un proceso se considere Markov debe satisfacer una condición conocida como la \textit{propiedad de Markov}. Esta propiedad establece que la probabilidad del siguiente evento (en el caso de generación de lenguaje, la siguiente palabra) depende únicamente del evento actual. La fórmula \ref{eqn:markov_property} representa los fundamentos de esta propiedad. Donde \textit{X} es una variable aleatoria que toma un valor en el espacio de estado dado \textit{s} y \textit{n} representa el paso de tiempo \citep{howell_2022}. Como se puede observar, suponiendo que nos encontramos en el paso de tiempo \textit{n}, las probabilidades teniendo en cuenta los estados de los eventos anteriores y la probabilidad teniendo en cuenta únicamente el estado actual son iguales. Por lo que la información anterior al estado actual carece de relevancia para el cálculo de la probabilidad del siguiente evento.

\begin{equation}
	\label{eqn:markov_property}
	\begin{aligned}
		P(X_{n+1} = s_{n+1}|X_{n} = s_{n}, X_{n-1} = s_{n-1}, ..., X_{0} = s_{0}) &= P(X_{n+1} = s_{n+1}|X_{n} = s_{n})
	\end{aligned}
\end{equation}


Debido a las características propias de la propiedad de Markov, este modelo es un modelo sin memoria ya que se desprecian todos los estados anteriores, por lo que no se retiene información relevante de un texto como las posiciones de las palabras en una oración o la relación entre palabras. Sin embargo, precisamente por carecer de memoria es simple de comprender y rápido de ejecutar \citep{fumagalli_2020}.


\subsubsection{Modelo N-gram}

Otro modelo estadístico de gran transcendencia es el modelo \textit{N-Gram}. Este modelo va un poco más allá del modelo \textit{Markov Chain} ya que se basa en realizar una predicción estadística de una secuencia de palabras teniendo en cuenta un conjunto de palabras anteriores. Esta secuencia de N palabras es representada mediante un N-grama.
Así, este modelo trata de predecir el N-grama más probable dentro de cualquier secuencia de palabras dado el historial de las N-1 palabras anteriores. Para su mejor comprensión, se expone un ejemplo ejemplo concreto a partir del siguiente extracto de ``La vida es sueño'' de Calderón de la Barca:


\begin{verse}
	¿Qué es la vida? Un frenesí.\\
	¿Qué es la vida? Una ilusión,\\
	una sombra, una ficción,\\
	y el mayor bien es pequeño;\\
	que toda la vida es sueño,\\
	y los sueños, sueños son.\\
\end{verse}

Podemos construir N-gramas a partir del texto anterior teniendo en cuenta que un N-grama está formado por N palabras que aparecen consecutivas en el corpus. A continuación se muestran unigramas, bigramas y trigramas extraídos del texto.

\begin{verse}
	\textbf{Unigramas} \\
	\{ qué, es, la vida, un frenesí, una, ilusión, sombra, ficción, y, el, mayor,... \}\\
	\textbf{Bigramas} \\
	\{ qué es, es la, la vida, vida un, un frenesí, frenesí qué, una ilusión,... \}\\
	\textbf{Trigramas} \\
	\{ qué es la, es la vida, la vida un, vida un frenesí, un frenesí qué,...\}
\end{verse}


Este proceso se realiza de manera iterativa hasta llegar al final del corpus teniendo en cuenta que no se pueden repetir dos N-gramas iguales dentro de un mismo conjunto. Una vez construidos los N-gramas se puede calcular la probabilidad condicional mediante las siguientes fórmulas dependientes de la ocurrencia de un sub n-gram dentro del conjunto.

\begin{equation}
	\label{eq:probabilidadesngram}
	\begin{aligned}
		\textrm{\textbf{Unigrama}}\:\:\:\:
		P_{(W_i)} & = \frac{C_{(W_i)}}{N}   \\     
		\\
		\textrm{\textbf{Bigrama}}\:\:\:\:
		P_{(W_i|W_{i-1})} & = \frac{C(W_{i-1}W_i)}{C({W_{i-1}})}\\
		\\
		\textrm{\textbf{Trigrama}}\:\:\:\:
		P_{(W_i|W_{i-2}W_{i-1})} & = \frac{C(W_{i-2}W_{i-1}W_i)}{C({W_{i-2}W_{i-1}})}
	\end{aligned}
\end{equation}

De esta manera para un modelo N-gram, se calcula la probabilidad condicional a partir dadas las n-1 palabras anteriores. Volviendo al ejemplo de ``La vida es sueño''. Para conocer la probabilidad de que a la secuencia \textit{la vida} le siga la secuencia \textit{es}, calculamos la probabilidad condicional \textit{es|la vida}. Según las fórmulas anteriores, esta probabilidad es igual al número de ocurrencias de \textit{la vida es} dividido por el número de ocurrencias de la secuencia \textit{la vida}.

\begin{equation}
	\label{eq:probabilidadesngram_ejemplo}
	\begin{aligned}
		P_{(es|la\;vida)} & = \frac{C_{(la\;vida\;es)}}{C_{(la\;vida)}}  &= \frac{1}{3}
	\end{aligned}
\end{equation}

Para conocer la probabilidad de una secuencia completa deberíamos multiplicar las probabilidades de manera iterativa. Para el ejemplo anterior, la $P(la\:vida\:es)= P(la)\:x\:P(vida|la)\:x\:P(es|la\:vida)$ y generalizando la fórmula anterior  $P(w1, w2, w3)= P(w1)\:x\:P(w2|w1)x\:P(w3|w2)$.

La realización de este proceso de manera iterativa nos lleva a la generación de oraciones, párrafos o libros completos. Sin embargo, la dependencia de generación de la palabra siguiente a una secuencia dada con respecto al conjunto de palabras generadas inmediatamente anteriores puede llevar a generaciones erróneas que no tengan en cuenta otros contextos anteriores.


\subsection{Modelos Seq2Seq y mecanismos de atención}
El modelo Sequence-to-Sequence (Seq2Seq) caracterizado por la utilización de una arquitectura especial de Red Neuronal Recurrente (RNN), ha alcanzado un gran éxito a la hora de resolver problemas complejos de Procesamiento de Lenguaje Natural, incluso llegando a superar a los modelos estadísticos de lenguaje en su efectividad \citep{analytics_vidhya_2020}. Esto se debe a que aproximaciones estadísticas como los \textit{N-grams} no eran capaces de capturar dependencias de palabras de corpus de gran tamaño, se necesitaría demasiado espacio y memoria RAM para poder guardar las probabilidades de todas posibles combinaciones de N-gramas. Sin embargo, las redes neuronales recurrentes, que implementa este modelo, no están limitadas a observar únicamente las palabras previas a una secuencia, sino que permiten propagar información desde el comienzo de una oración hasta el final consiguiendo mejores predicciones.


\subsubsection{Redes Neuronales Recurrentes}
Las redes neuronales recurrentes o \textit{Recurrent Neural Networks (RRN)} son una clase especial de red neuronal profunda que nos permite analizar datos tratando la dimensión ``tiempo''.
Aunque este tipo de red aparece por primera vez en el 1982 introducida por \cite{firstrnn}, debido a los requisitos computacionales que necesitaban no se pudieron llevar a la práctica hasta muchos años más tarde; cuando llegaron los avances necesarios para su puesta en marcha. La principal área de aplicación de este tipo de algoritmo de \textit{deep learning} es la resolución de problemas que involucran datos secuenciales (y por tanto, temporales) como traducción automática, procesamiento de lenguaje natural, descripción de imágenes o reconocimiento de voz. 

Teóricamente, una red neuronal recurrente está formada por \textit{neuronas recurrentes}. Mientras que otros tipos de redes utilizan como función de activación de la neurona una función que actúa en una sola dirección, desde la primera capa de entrada hasta la última capa de salida; este tipo de redes también incluyen conexiones hacia atrás, proporcionando al sistema cierta memoria. En cada instante de tiempo llamado \textit{timestep}, cada neurona de la red recibe como entrada la salida de la capa anterior así como su propia salida del instante de tiempo anterior. Este procedimiento se puede expresar con la notación de la ecuación \ref{eq:activation_rnn}, donde $x = (x_1,...,x_T)$ representa la secuencia de entrada procedente de la anterior capa, $y$ la secuencia de salida de la capa actual, $W_x$ los pesos a aplicar a los datos de entrada procedentes de la salida la capa anterior, $W_y$ los pesos que se aplican sobre los datos procedentes de la salida de la propia capa obtenidos en el anterior instante de tiempo y \textit{b} un bias a partir del cual centrar los datos.

\begin{equation}
	\label{eq:activation_rnn}
	\Large
	y_{(t)} = f_{activation}(W_{x}X_{(t)} + W_{y}Y_{(t-1)}+b)
\end{equation}

Otra forma más intuitiva a través de la cual comprender este proceso que en realidad no dista demasiado del algoritmo de una red neuronal convencional, es desarrollando esta neuronal a través de los pasos de tiempo \textit{t}. En la figura ~\ref{rnn_neuron} se puede comprobar el proceso de este algoritmo desde un punto de vista más esquemático. A la izquierda, se muestra la neurona recurrente sin desarrollar y a la derecha, la neurona desplegada en el tiempo \citep{neuron}.

\figura{Bitmap/03EstadoDeLaCuestion/rnn_neuron}{width=1\textwidth}{fig:rnn_neuron}%
{Neurona recurrente desplegada en el tiempo}

La parte de la neurona donde se preserva un estado a través del tiempo se denomina \textit{memory cell}. La finalidad de este componente es recordar información relevante sobre un estado anterior que recibieron para poder realizar predicciones más precisas.

Mediante la unión y configuración en capas de varias neuronas de este tipo, pueden llegar a construirse grandes redes neuronales de tipo recurrente. Estas redes no solo modifican, con respecto a una red neuronal convencional, el tipo de neuronas y conexiones entre ellas; sino también algoritmos internos que permiten su adecuado funcionamiento. En concreto, el procedimiento de \textit{Backpropagation} convencional se sustituye por una versión del mismo dependiente de la dimensión ``tiempo'', conocido como \textit{Backpropagration Through Time (BTTT)}. Este algoritmo mantiene la función tradicional del mismo, que no es otra que ir hacia atrás en la red con el fin de encontrar las derivadas parciales del error con respecto a los pesos de las neuronas. Estas derivadas son utilizadas en el \textit{descenso de gradiente} para ajustar los pesos dependiendo del comportamiento de \textit{Loss}. Sin embargo, debido a la inclusión del ``tiempo'' en este algoritmo, el coste computacional aumenta haciendo a este modelo mucho más lento.

El problema de este tipo de redes es conocido como \textit{Vanishing Gradients}. Como mencionamos anteriormente, de la aplicación del \textit{Backpropagration} se obtenían unas derivadas parciales del error, cada una de estas derivadas es un \textit{gradiente}. El problema de desvanecimiento de gradiente ocurre porque el gradiente se reduce a medida que se propaga hacia atrás a través del tiempo. Cuando los valores de un gradiente son extremadamente pequeños, estos valores no contribuyen al aprendizaje perdiendo peso en el resultado. 

\subsubsection{\textit{Long Short-Term Memory (LSTM)}}
Estas redes, propuestas por \citep{lstm} en el año 1997, surgieron como una evolución de las redes neuronales recurrentes. Su principal objetivo es ampliar la memoria para poder recordar no solo información reciente sino datos producidos mucho más tiempo atrás, ya que las redes neuronales recurrentes convencionales no eran capaces de recordar información que se había producido hacía varios \textit{timestep}; llevando a una memoria limitada para recordar secuencias de entrada más largas. Este problema es resultado del \textit{Vanishing Gradients} de los gradientes más lejanos en el tiempo.

\figura{Bitmap/03EstadoDeLaCuestion/lstm}{width=1\textwidth}{fig:lstm}%
{Neurona LSTM}

Para solventar esta limitación, las redes \textit{Long Short-Term Memory} proponen una variación de las neuronas. Estas neuronas poseían una memoria o \textit{memory cell} donde almacenaban la información relevante de estados anteriores dependiendo de los pesos calculados. En cada neurona LSTM existen tres puertas a esta celda: la puerta de entrada (\textit{input gate}), la puerta de olvidar (\textit{forget gate}) y la puerta de salida (\textit{output gate}). Estas puertas regulan el flujo de información dentro y fuera de la celda. Deciden si se permite una nueva entrada a la memoria, si se elimina la información o si se deja que afecte a la salida del instante de tiempo actual. Estas puertas podemos codificarlas mediante una función de activación sigmoide, lo que hace posible incluirlas en la \textit{Backpropagation} solucionando el problema de \textit{Vanishing Gradients}. Toda esta explicación está representada en la figura .


\subsubsection{Arquitectura Encoder-Decoder}
Considerando los diferentes tipos de redes neuronales descritas en los apartados anteriores, se da paso a la explicación propia del modelo Seq2Seq. Desde un punto de vista muy general, podríamos representar este modelo como un sistema que toma una secuencia de elementos como entrada (input) y genera otra secuencia de elementos de salida (output). Como se muestra en la figura \ref{fig:basicseq2seq}, la arquitectura de este sistema sigue una arquitectura \textit{Encoder-Decoder}, compuesta internamente por dichos componentes, un \textit{encoder} y un \textit{decoder} que implementan redes neuronales recurrentes, concretamente LSTM o en menor número de casos GRU (\textit{Gated Recurrent Units}).


\figura{Bitmap/03EstadoDeLaCuestion/basicseq2seq}{width=1\textwidth}{fig:basicseq2seq}%
{Arquitectura de un sistema Seq2Seq}

La tarea del \textit{encoder} consiste en resumir la información de la secuencia que se introdujo como entrada en forma de un vector de estado oculto o \textit{context} y enviar los datos resultantes al \textit{decoder}. El objetivo principal de este vector es encapsular la información de todos los elementos de entrada para ayudar al \textit{decoder} a realizar predicciones precisas. Para calcular el estado oculto t-ésimo de la secuencia se utiliza la fórmula representada en la ecuación \ref{eq:encoder}, donde  $x_t$ corresponde a la secuencia de entrada en el instante de tiempo \textit{t} y \textit{W} representa la matriz de pesos a aplicar sobre los datos de entrada $W^{hx}$ y sobre la salida de la celda del instante anterior $W^{hh}$. Para cada una de las celdas del \textit{encoder} se calcula su vector de estado oculto, generando la última celda (en el instante de tiempo \textit{t}) el vector de estados finales.

\begin{equation}
	\label{eq:encoder}
	\Large
	h_t = f(W^{(hx)}x_t+W^{(hh)}h_{t-1})
\end{equation}


Por su parte, el \textit{decoder} utiliza como estado inicial la salida del \textit{encoder} correspondiente al vector de estados finales, calculando cada celda su estado oculto con la fórmula \ref{eq:decoder}. Una vez que se obtiene el estado oculto $h_{t}$, puede generarse la secuencia de palabras final aplicando al dataset de palabras junto con $h_{t}$ la función \textit{softmax}.

\begin{equation}
	\label{eq:decoder}
	\Large
	h_t = f(W^{(hh)}h_{t-1})
\end{equation}

Aunque esta aproximación parece solucionar muchos de los problemas de modelos anteriores, añade o mantiene limitaciones. Una de ellas es el cuello de botella que se genera en el último estado oculto del codificador ya que toda la información de la entrada debe atravesar el \textit{encoder} hasta este último punto para poder pasarle toda la información junta al \textit{decoder}. Además, ya que se intenta mapear una secuencia de longitud variable en una memoria de longitud fija y en el caso de textos largos podría perderse parte de la información. 


\subsubsection{Mecanismos de atención}

Ante los problemas mencionados anteriormente, se plantea la utilización de mecanismos de atención que permiten que el \textit{decoder} no tenga que recibir toda la información del \textit{encoder}, sino que se fija en aquellas palabras más importante que producen los estados ocultos de codificador en cada uno de sus pasos. Estos mecanismos de atención fueron introducidos inicialmente por \cite{Bahdanau} para la traducción automática aunque posteriormente se ha aplicado a una multitud de áreas. 


Para conseguir estos beneficios del mecanismo de atención, se modifica ligeramente la arquitectura del sistema añadiendo una capa intermedia entre el codificador y decodificador que recibe los estados ocultos que se van generando en el \textit{encoder}. Sin embargo, no se espera a que todos los estados ocultos estén calculados sino solo los más importantes a los que se les establece un mayor peso. Para que el almacenamiento de los estados ocultos no sea ineficiente, los estados ocultos recibidos se combinan en un vector llamado \textit{vector de contexto} que contendrá más o menos información de las palabras dependiendo de su peso (figura ~\ref{fig:basicseq2seq_attention2}). Estos pesos se calculan comparando el último estado oculto del \textit{decoder} con cada uno de los estados del codificador determinando así las palabras más importantes.


\figura{Bitmap/03EstadoDeLaCuestion/basicseq2seq_attention2}{width=1\textwidth}{fig:basicseq2seq_attention2}%
{Arquitectura de un sistema Seq2Seq con mecanismo de atención}


\subsection{Modelos pre-entrenados: Transformers}
\label{sec:transformers}
Los modelos pre-entrenados son modelos de aprendizaje profundo o \textit{Deep Learning} que surgieron como una evolución de los modelos Seq2Seq. Estos modelos de lenguaje son entrenados bajo grandes conjuntos de datos para realizar diversas tareas de Procesamiento de Lenguaje. Como parten de un conocimiento base, pueden ajustarse a tareas específicas sin requerir un entrenamiento desde cero. Este pre-entrenamiento es la clave de porque son tan valiosos, ya que permiten sin una gran esfuerzo computacional (normalmente tardan en entrenarse semanas o meses con los mejores computadores) construir un sistema de generación de lenguaje adaptándose al objetivo buscado. Otra ventaja de la existencia de este tipo de modelos es la posibilidad de elección de una pequeña \textit{dataset} para realizar el entrenamiento ya que los patrones lingüísticos generales ya se han aprendido durante el entrenamiento previo.



Dentro de este tipo de modelos pre-entrenados, destacan los \textit{Transformers} \citep{vaswani2017attention}. Estos modelos revolucionaron el Procesamiento de Lenguaje desde el momento en que se presentaron. Se basan en modelos Seq2Seq con mecanismos de atención y tratan de remediar los problemas de generación de este tipo de sistema. Recapitulando, la arquitectura neuronal recurrente propia del Secuencia a Secuencia implicaba un procesamiento secuencial para codificar la entrada en el \textit{encoder}. Posteriormente, se procesaba la información procedente del último estado oculto de codificador en el \textit{decoder} de la misma manera. Este procedimiento secuencial dificulta aplicar este tipo de modelos a la generación de textos largos, ya que tomaría mucho tiempo procesar todas las palabras de entrada a través de las distintas partes de la arquitectura. Ante esta problemática surgieron los mecanismos de atención que lograban paliar el cuello de botella producido en el último estado oculto del \textit{encoder}. Esta arquitectura mantenía las Redes Neuronales Recurrentes en codificador y decodificador como las LSTMs, sin embargo los \textit{Transformers} las sustituyeron por otras funciones lineales y no lineales que permitían un procesamiento mucho más rápido de la información.



El modelo \textit{Transformer} sustituye la capa de atención del modelo Seq2Seq implementada como un producto de matrices (\textit{Scaled Dot-Product Attention}), una función bilineal \citep{luong2015effective} o un perceptrón multicapa (\textit{Multi-layer Perceptron}), dependiendo del modelo, mejorando su rendimiento. El núcleo del modelo \textit{Transformers} reside en el mantenimiento de los productos escalares de matrices (\textit{Scaled Dot-Product Attention}) que posibilitan una gran eficiencia en tiempo y memoria ya que consiste únicamente en unas multiplicaciones básicas de matrices. Sin embargo, este mecanismo formará parte de la capa de atención multi-cabeza (\textit{Multi-Head Attention}) que viene sustituir la función completa de la capa de atención del Secuencia a Secuencia. 
El \textit{Multi-Head Attention} es un procedimiento consistente en varias capas en paralelo del anterior \textit{Scaled Dot-Product Attention}. Esta opción permite el procesamiento simultáneo de las diferentes entradas necesarias para la generación de texto que en el modelo con atención de Secuencia a Secuencia se realizaba de manera secuencial, lo que permite un procesamiento más eficiente, especialmente de grandes corpus de texto.


\figura{Bitmap/03EstadoDeLaCuestion/left-Scaled-Dot-Product-Attention-right-Multi-Head-Attention}{width=0.7\textwidth}{fig:comparationattention}%
{Capa de atención de \textit{Transformers} \citep{atencion_capas}}



La arquitectura externa del modelo \textit{Transformer} no dista demasiado de las explicadas anteriormente ya que se trata de una evolución de los modelos Seq2Seq, manteniendo la existencia del \textit{encoder} y del \textit{decoder}. Las modificaciones se realizan en la estructura interna de ambos componentes. 

El \textit{encoder} o codificador comienza con un módulo \textit{Multi-Head Attention} que realiza \textit{Scaled Dot-Product Attention} sobre la secuencia de entrada. A este procedimiento le siguen varias capas de normalización y conexión residual \footnote{Más información en https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55} para terminar con una capa de prealimentación (\textit{Feed-Forward Layer}) y otra capa de normalización y conexión residual. 

El \textit{decoder} o decodificador está compuesto de una estructura similar aunque algo más complicada. Comienza con un módulo compuesto por \textit{Multi-Head Attention} enmascarado para que posteriormente cada una de las palabras dependa únicamente de las palabras previas en el corpus. A continuación, una estructura semejante al \textit{encoder} recibe como entrada la salida del módulo anterior y la salida del codificador. Para finalizar, aplica una serie de funciones lineales y la función de activación \textit{Softmax}. De esta manera así obtiene diferentes probabilidades que generan la salida del sistema (figura ~\ref{fig:arquitectura_transformers}).



\figura{Bitmap/03EstadoDeLaCuestion/arquitectura_transformers}{width=0.7\textwidth}{fig:arquitectura_transformers}%
{Arquitectura del modelo Transformer}



Hay que destacar la existencia de una herramienta en Python denominada \textit{transformers} que proporciona una serie de sistemas de propósito general para Natural Language Understading (NLU) y Natural Language Generation (NLG). Ofrece más de 32 modelos preentrenados en más de 100 idiomas, entre los que se encuentra el español. Entre los modelos más utilizados encontramos los famosos GPT-2 y BERT junto con un gran número de variaciones de ellos dependiendo de los datos utilizados para su entrenamiento.

\subsubsection{GPT-2}

GPT-2 (\textit{Generative Pretrained Transformer}) es un modelo GLN presentado por OpenAI en el año 2019 basado en redes neuronales para secuencias, basadas en la autoatención enmascarada(\textit{masked self-attention}), y que ha sido construido sobre una arquitectura Transformer. El objetivo de este sistema es construir una distribución de probabilidad en la que para cada palabra posible a generar se le asigna una probabilidad en función del contexto anterior. Se trata de un modelo que ha sido preentrenado con un conjunto de datos correspondiente a las 8 millones de páginas web mejor valoradas en Reddit, lo que resulta en una gran base de conocimiento para generar textos automáticamente de manera muy correcta.

La potencia de este modelo es tal que sus creadores no quisieron en un primer momento publicar la versión completa por miedo de que se pudiera utilizar de manera ilícita. Según fueron pasando los años, se fueron liberando progresivamente diferentes versiones del modelo original ya que comenzaban a surgir otros proyectos con potencias igualmente competitivas. Estas diferentes versiones se diferenciaban en el número de parámetros que admitía la arquitectura y de esta manera se conseguía limitar su funcionamiento. La primera versión contaba con 117 miles de millones de parámetros mientras que la última versión, publicada en 2020, posee 1,5 billones.

GPT-2 únicamente está disponible en inglés aunque puede hacer uso de GoogleTranslate API para generar textos en otros idiomas. Es importante resaltar que al depender del traductor se puede ver disminuida la calidad de generación de lenguaje.

\subsubsection{BERT}
BERT (Bidirectional Encoder Representations from Transformers) es un modelo NLP desarrollado por Google y publicado a finales de 2018 \citep{Devlin2019BERTPO}. Está basado redes neuronales bidireccionales que tratan de predecir las palabras perdidas (enmascaradas) en una oración y determinar si dos oraciones consecutivas son continuación lógica entre sí para determinar si están conectadas por su significado. Aunque originalmente no estaba destinado a la generación de textos, \cite{wang-cho-2019-bert} publicaron un método de utilización de este sistema para conseguir la generación de lenguaje que parece dar muy buenos resultados. De hecho, consiguió mejorar los resultados de la versión publicada en aquellos tiempos por GPT-2. 

De este modelo han surgido numerosas variaciones que se han publicado a lo largo de estos años. Como se puede apreciar en la figura \ref{fig:modelos_bert}, encontramos distintas ramificaciones que podemos dividir en dos grupos: modelos preentrenados con un corpus específico perteneciente a un dominio y modelos \textit{fine-tuned} que se ajustan a una tarea específica utilizando un modelo previamente entrenado \citep{rajasekharan_2019}.
Otras variaciones de BERT corresponden a los modelos construidos a partir de él pero entrenados en otros lenguajes para generar textos en otra lengua distinta al inglés, que es la original. Beto es la versión en Español de BERT \citep{CaneteCFP2020} y ha sido entrenado con una gran corpus en dicho idioma.

\figura{Bitmap/03EstadoDeLaCuestion/modelos_bert}{width=0.9\textwidth}{fig:modelos_bert}%
{Modelos surgidos a partir de BERT}


\subsection{SimpleNLG}
SimpleNLG es una API de Java que proporciona interfaces que ofrecen un control directo sobre la tarea de realización. Define un conjunto de tipos léxicos, correspondientes a las principales categorías gramaticales, así como formas de combinarlos y establecer valores de características. 

Esta orientado a la generación de oraciones gramaticalmente correctas en sistemas \textit{data-to-text}. Aunque originalmente solo estaba disponible para textos de lengua inglesa, actualmente se encuentra versionado para muchos idiomas, entre ellos el español. La versión española de esta herramienta se llama SimpleNLG-ES y realmente se trata de una adaptación bilingüe de la versión original en inglés.

Esta herramienta se basa en la flexibilidad a la hora de generar textos mediante la utilización de manera combinada de sistemas basados en esquemas y otros sistemas más avanzados; robustez generando salidas (aunque en ocasiones incorrectas) cuando las entradas estén erróneas o incompletas; e independencia entre las operaciones de decisión de morfología y sintácticas.

\section{Proyectos relacionados}
Muchos son los enfoques que se han estudiado para tratar de perfeccionar la generación de lenguaje natural. Los más tradicionales seguían la metodología presentada en el apartado \ref{sec:arquitectura_tradicional} de este documento. En ella dividían el problema principal en varios subproblemas o tareas. Entre ellas se incluía la selección de contenido, estructuración del texto, agregación, lexicalización, generación de expresiones de referencia y finalmente, la realización \citep{reiter1997building}. Sin embargo, en los últimos años ha crecido el interés por mirar más allá de aquella arquitectura. Los sistemas presentados en la sección \ref{sec:modelos} surgieron para romper con ella.

Todos estos sistemas presentados anteriormente tienen muy poca capacidad o poca calidad a la hora de generar textos de manera controlada. Algunos de ellos como GPT-2 generan textos a partir de una oración inicial, resultando su salida un texto de tamaño variable que da continuidad a la oración de entrada. Otros como BERT son capaces de generar palabras perdidas dentro de una oración. Sin embargo, pocos de ellos son capaces por sí mismos de generar contenido de manera controlada a partir de una información dada en todos los puntos de su generación.


Por todo esto, han surgido varios sistemas como DICE \citep{yang2020creative} que utilizarán estos modelos ajustándolos con el objetivo de controlar la generación del texto resultante. Como se muestra en la figura \ref{fig:DICE}, este sistema está compuesto por dos capas. La primera capa toma como entrada unas palabras clave introducidas por el usuario y forma un Grafo de Conocimiento. A continuación y para conectar ambas capas, utiliza tripletas como interfaz. Estas tripletas se pueden construir a partir del grafo o extraerse de un corpus de historias denominado ROCStory. Estas tripletas sirven como entrada a un sistema de generación de texto que utiliza GPT-2 para generar pequeñas historias.

\figura{Bitmap/03EstadoDeLaCuestion/DICE}{width=0.9\textwidth}{fig:DICE}%
{Arquitectura del sistema DICE}

Otro enfoque parecido utiliza una conocida dataset llamada WebNLG para entrenar un Modelo de Lenguaje. Esta base de datos contiene correspondencias entre textos y tripletas y es utilizada para ajustar el Modelo de Lenguaje T5. De esta manera, es capaz de generar textos a partir de tripletas introducidas como entrada al sistema. En la figura \ref{fig:T5}, podemos ver el formato de entrada.

\figura{Bitmap/03EstadoDeLaCuestion/T5}{width=0.9\textwidth}{fig:T5}%
{Entrada y salida del sistema T5}

Con respecto a propuestas dentro del ámbito de generación de lenguaje en terapias de reminiscencia, no son muchos los sistemas que encontramos. Por una parte, \citep{MoralesdeJess2020ACM} proponen desarrollar e implementar un modelo conversacional que pueda ayudar a los cuidadores y a sus propios pacientes con Alzheimer a realizar un mayor número de terapias de reminiscencia periódicas para así potenciar los beneficios de estas. Se centra en generar conversaciones personalizadas entre el prototipo del sistema conversacional y el paciente con el fin de recoger información relacionada con sus gustos, historial y estilo de vida. Su arquitectura, como se muestra en la figura \ref{fig:ConversationalModelArchitecture}, esta integrada por varios módulos: módulo de Reconocimiento Automático de Voz, Comprensión del Lenguaje Natural, Gestión de Diálogos, Modelo de Diálogos, Generación de Lenguaje Natural y Text-to-Speech.


\figura{Bitmap/03EstadoDeLaCuestion/ConversationalModelArchitecture}{width=.8\textwidth}{fig:ConversationalModelArchitecture}%
{Arquitectura modelo conversacional \citep{deconversational}.}

En otra investigación (realizada por \cite{shi2012user}), se propone el desarrollo de un sistema computarizado llamado Life Story Book (LSB), que facilita el acceso y la recuperación de recuerdos almacenados que se utilizan como base para interacciones positivas entre ancianos y jóvenes, y especialmente entre personas con deterioro cognitivo y miembros de su familia o cuidadores. Para facilitar la gestión de la información y la generación dinámica de contenido, este artículo presenta un modelo semántico de LSB que se basa en el uso de ontologías y algoritmos avanzados para la selección de características y la reducción de dimensiones. Para terminar, propone un algoritmo llamado Onto-SVD que combina la selección de características semánticas y la ontología orientada al usuario con  la utilización de SVD como método de reducción de dimensiones para lograr la identificación de temas basada en la similitud semántica.

